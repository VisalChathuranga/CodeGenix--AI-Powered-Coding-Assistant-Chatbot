
=== Page 1 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Lesson 4 Yolov5 Model Training

1. Yolo Model Series Introduction

1.1 Yolo

YOLO (You Only Look Once) is an one-stage regression algorithm based on

deep learning.

R-CNN series algorithm dominates target detection domain before YOLOV1 is
released. It has higher detection accuracy, but cannot achieve real-time
detection due to its limited detection speed engendered by its two-stage

network structure.

To tackle this problem, YOLO is released. Its core idea is to redefine target
detection as a regression problem, use the entire image as network input, and
directly return position and category of Bounding Box at output layer.
Compared with traditional methods for target detection, it distinguishes itself in

high detection speed and high average accuracy.

1.2 Yolov5

YOLOv5 is an optimized version based on previous YOLO models, whose

detection speed and accuracy is greatly improved.

In general, a target detection algorithm is divided into 4 modules, namely input
end, reference network, Neck network and Head output end. The following

analysis of improvements in YOLOv5 rests on these four modules.

1) Input end: YOLOv5 employs Mosaic data enhancement method to
increase model training speed and network accuracy at the stage of model

training. Meanwhile, adaptive anchor box calculation and adaptive image
1



=== Page 2 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

scaling methods are proposed.

2) Reference network: Focus structure and CPS structure are introduced in
YOLOv5.

3) Neck network: same as YOLOV4, Neck network of YOLOv5 adopts
FPN+PAN structure, but they differ in implementation details.

4) Head output layer: YOLOv5 inherits anchor box mechanism of output layer
from YOLOv4. The main improvement is that loss function GIOU_Loss, and

DIOU_nms for prediction box screening are adopted.

2. Yolov5 Model Structure

2.1 Component

1) Convolution layer: extract features of the image

Convolution refers to the effect of a phenomenon, action or process that
occurs repeatedly over time, impacting the current state of things. Convolution
can be divided into two components: "volume" and "accumulation". "Volume"
involves data flipping, while "accumulation" refers to the accumulation of the
influence of past data on current data. Flipping the data helps to establish the
relationships between data points, providing a reference for calculating the

influence of past data on the current data.

In YOLOv5S, the data being processed is typically an image, which is
two-dimensional in computer vision. Therefore, the convolution applied is also
a two-dimensional convolution, with the aim of extracting features from the
image. The convolution kernel is an unit area used for each calculation,
typically in pixels. The kernel slides over the image, with the size of the kernel

being manually set.



=== Page 3 ===
bf IVW/E) im | = t Shenzhen Hiwonder Technology Co,Ltd

During convolution, the periphery of the image may remain unchanged or be
expanded as needed, and the convolution result is then placed back into the
corresponding position in the image. For instance, if an image has a resolution
of 6x6, it may be first expanded to a 7x7 image, and then substituted into the
convolution kernel for calculation. The resulting data is then refilled into a blank

image with a resolution of 6x6.

Calculate
and fill in

Expand >

Original image New image

Expanded image



=== Page 4 ===
| IWE) M | = t Shenzhen Hiwonder Technology Co,Ltd

Eid -
: New image

Expanded image New image

2

New image

Expanded image

2) Pooling layer: enlarge the features of image

The pooling layer is an essential part of a convolutional neural network and is
commonly used for downsampling image features. It is typically used in
combination with the convolutional layer. The purpose of the pooling layer is to
reduce the spatial dimension of the feature map and extract the most important

features.

There are different types of pooling techniques available, including global
pooling, average pooling, maximum pooling, and more. Each technique has its

unique effect on the features extracted from the image.

4



=== Page 5 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Let's take maximum pooling as an example to explain how it works. Before
understanding the maximum pooling, we need to understand the filter. Similar
to the convolution kernel, the filter requires us to manually set the area. During
the calculation, the filter is slid across the image, and the pixels in the area are
compared. The maximum value of the pixels in the area is selected and
becomes the output for that region. This process results in a reduced feature
map size, which helps in reducing computational complexity and prevents

overfitting.

In summary, the pooling layer plays a crucial role in feature extraction and
downsampling. It helps to extract the most important features from an image
and reduce the computational complexity of the neural network. Different types

of pooling techniques can be used depending on the application requirements.

pooling >

Original image Filter

New image

Maximum pooling can extract the most distinctive features from an image,
while discarding the remaining ones. For example, if we take an image with a
resolution of 6 <6 pixels, we can use a 2X 2 filter to downsample the image

and obtain a new image with reduced dimensions.



=== Page 6 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

-E=*

Filtered data

ae

Filtered data

Original image

lise

Original image

Filtered data

Original image
3) Upsampling layer: restore the size of an image

This process is sometimes referred to as "anti-pooling". While upsampling
restores the size of the image, it does not fully recover the features that were
lost during pooling. Instead, it tries to interpolate the missing information based

on the available information.

For example, let's consider an image with a resolution of 6x6 pixels. Before
upsampling, use 3X3 filter to calculate the original image so as to get the new

image.



=== Page 7 ===
| IWEM Oo er Shenzhen Hiwonder Technology Co,Ltd

=i

Upsampled image

-= i

Upsampled image

-=

Original image Upsampled image

4) Batch normalization layer: organize data

It aims to reduce the computational complexity of the model and to ensure that

the data is better mapped to the activation function.

Batch normalization works by standardizing the data within each mini-batch,
which reduces the loss of information during the calculation process. By
retaining more features in each calculation, batch normalization can improve

the sensitivity of the model to the data.



=== Page 8 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Changes:

/ 1. reduce the

| | — [ Organized \ / pomatation

| Data Batch normalization =§=}———* ae 2. reduce the loss of

\ / \ / information

h : A 3. improve the

T_ sensitivity of the model
to the data

5) RELU layer: activate function

The activation function is a crucial component in the process of building a
neural network, as it helps to increase the nonlinearity of the model. Without an
activation function, each layer of the network would be equivalent to a matrix
multiplication, and the output of each layer would be a linear function of the
input from the layer above. This would result in a neural network that is unable

to learn complex relationships between the input and output.

There are many different types of activation functions. Some of the most
common activation functions include the ReLU, Tanh, and Sigmoid. For
example, ReLU is a piecewise function that replaces all values less than zero

with zero, while leaving positive values unchanged.

10 5

-10.0 —-7.5 -50 -—25 0.0 25 5.0 7.5 10.0
6) ADD layer: add tensor

In a typical neural network, the features can be divided into two categories:
8



=== Page 9 ===
| IWEDM Oo er Shenzhen Hiwonder Technology Co,Ltd

salient features and inconspicuous features.

The ADD layer works by adding the tensors of salient features, which can help

to amplify their importance and improve the overall performance of the model.

Salient features

Shallowed

Inconspicuous features

Darken

Original image New image

7) Concat layer: splice tensor

It is used to splice together tensors of features, allowing for the combination of
features that have been extracted in different ways. This can help to increase

the richness and complexity of the feature set.



=== Page 10 ===
| IWE) M Oo = t Shenzhen Hiwonder Technology Co,Ltd

Original image New image

Original image New image

2.2 Compound Element

When building a model, using only the layers mentioned above to construct
functions can lead to lengthy, disorganized, and poorly structured code. By
assembling basic elements into various units and calling them accordingly, the

efficiency of writing the model can be effectively improved.

1) Convolutional unit:
A convolutional unit consists of a convolutional layer, a batch normalization
layer, and an activation function. The convolution is performed first, followed by

batch normalization, and finally activated using an activation function.



=== Page 11 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

Batch :
convolution eS a +S Activate
Z/ | normalization | function

Y
f \\

siya ee

unit

2) Focus module

The Focus module for interleaved sampling and concatenation first divides the
input image into multiple large regions and then concatenates the small
images at the same position within each region to break down the input image
into several smaller images. Finally, the images are preliminarily sampled
using convolutional units.

As shown in the figure below, taking an image with a resolution of 6x6 as an
example, if we set a large region as 2x2, then the image can be divided into 9
large regions, each containing 4 small images.

By concatenating the small images at position 1 in each large region, a 3x3
image can be obtained. The small images at other positions are similarly
concatenated, and the original 6x6 image will be broken down into four 3x3

images.

Big area

2x2

Join image based
on position

9x9

Original image Alternating row sampling

3) Residual unit
11



=== Page 12 ===
bf IVW/EDM oer Shenzhen Hiwonder Technology Co,Ltd

The function of the residual unit is to enable the model to learn small changes
in the image. Its structure is relatively simple and is achieved by combining
data from two paths.

The first path uses two convolutional units to sample the image, while the
second path does not use convolutional units for sampling but directly uses the

original image. Finally, the data from the first path is added to the second path.

fe

The second path without The first path

After residual unit

sampling after being sampled
Convolutional Convolutional data
ater
Data | ini ut [| Add ate
Bsc
( )
Residual
unit

4) Composite Convolution Unit

In YOLOv5, the composite convolution unit is characterized by the ability to
customize the convolution unit according to requirements. The composite
convolution unit is also realized by superimposing data obtained from two
paths.

The first path only has one convolutional layer for sampling, while the second
path has 2x+1 convolutional units and one convolutional layer for sampling.
After sampling and splicing, the data is organized through batch normalization
and then activated by an activation function. Finally, a convolutional layer is

used for sampling.’



=== Page 13 ===
| IWE) | Oo = t Shenzhen Hiwonder Technology Co,Ltd

onvolutional

unit

2x
convolutional
units

convolutional
layer

composite
onvolutional
unit

Batch

normalization

Activate
function

onvolutiona
unit

Data after
rocessing
J

Piya

5) Compound Residual Convolutional Unit
The compound residual convolutional unit replaces the 2x convolutional layers
in the compound convolutional unit with x residual units. In YOLOv5, the

feature of the compound residual unit is mainly that the residual units can be

13



=== Page 14 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

customized according to the needs.

onvolutional
unit
x
convolutional
units
convolutional
layer

convolutional

layer

composite
onvolutional
unit

Batch

normalizatio

Activate
function

onvolutiona
unit

Data after
processing

6) Composite Pooling Unit

The output data of the convolutional unit is fed into three max pooling layers



=== Page 15 ===
a IWE) M Oo = t Shenzhen Hiwonder Technology Co,Ltd

and an additional copy is kept without processing. Then, the data from the four
paths are concatenated and input into a convolutional unit. Using the
composite pooling unit to process the data can significantly enhance the

features of the original data.’

Maximum Maximum Maximum
pooling pooling pooling

composite

rad convolutional
unit

Convolutional
unit

( Data after |
rocessin
\P ‘ fl

2.3 Structure
Composed of three parts, YOLOv5 can output three sizes of data. Data of
each size is processed in different way. The below picture is the output

structure of YOLOv5.



=== Page 16 ===
| WE) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

alternate sampling
concatenation unit
convolutional unit
composite residual

maximum pooling convolutional unit x=1
convolutional unit

composite residual
Maximum pooling convolutional unit x=3

convolutional unit

composite residual
-onvolutional unit x=3

convolutional unit

maximum pooling

composite pooling
unit

composite convolutional
unit x=1

convolutional unit

upsampling layer

concatenate

composite
convolutional unit x=

tH

convolutional unit

upsampling layer

concatenate

composite convolutional layer Data
convolutional unit x=1

76*76"255

convolutional unit

composite i. data
onvolutional unit x=1 Convolutional layer 38*38"255

concatenate

composite . data
onvolutional unit x=1 convolutional layer 19*19*255

concatenate

Below is the output structures of data of three sizes.

16



=== Page 17 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

alternate samplin
concatenation unit

convolutional unit

composite residual
convolutional unit x=1

maximum pooling

convolutional unit

composite residual
convolutional unit x=3

convolutional unit
maximum pooling

maximum pooling

composite residual
convolutional unit x=3

convolutional unit

composite pooling
unit

composite residual
convolutional unit x=

convolutional unit
upsampling layer

Composite convolutional
unit x=1

unit x=1

Data
76*76*255



=== Page 18 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

interleaved sampling
concatenation unit
convolutional unit

composite residual
convolutional unit x=1

convolutional unit

maximum pooling

composite residual
convolutional unit x=3
convolutional unit

18

composite residual
convolutional unit x=3

convolutional unit
‘omposite pooling unit}

composite
convolutional unit x=1

convolutional unit
upsampling layer

composite
convolutional unit x=1

convolutional unit
upsampling layer

composite
convolutional unit x=1

convolutional unit

composite
convolutional unit x=1
convolutional layer

Data
38*38*255



=== Page 19 ===
| IW) | Oo er Shenzhen Hiwonder Technology Co,Ltd

interleaved sampling
concatenation unit
convolutional unit
< . composite residua
maximum pooling convolutional unit x=1

convolutional unit

composite residual
onvolutional unit x=3

convolutional unit

maximum pooling

composite residual
convolutional unit x=3

convolutional unit
maximum pooling

composite
pooling unit

composite
convolutional unit x=1

convolutional unit
upsampling layer

composite
convolutional unit x=1

convolutional unit
upsampling layer

composite
convolutional unit x=1

convolutional unit

composite
onvolutional unit x=1

convolutional unit
convolutional unit x=1
convolutional layer

data

19*19*255

19


