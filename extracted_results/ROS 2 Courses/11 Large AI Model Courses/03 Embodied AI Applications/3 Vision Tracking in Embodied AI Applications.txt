
=== Page 1 ===
4 iwennder Shenzhen Hiwonder Technology Co,Ltd

Vision Tracking in Embodied Al
Applications
The large model used in this lesson operates online, requiring a stable

network connection for the main controller in use.

1. Brief Overview of Operation

Once the program starts running, the Six-Microphone Circular Array will
announce “I’m Ready,” hereafter called the voice device. To activate the voice
device, speak the designated wake words: “Hello Hiwonder.” Upon
successful activation, the voice device will respond with “I’m Here.” Once
activated, voice commands such as “Follow the person in white in front” can
be issued. The terminal will display the recognized command, and the voice
device will respond with a generated reply after processing and execute

corresponding actions.
2. Getting Started

2.1 Version Confirmation

Before starting this feature, verify that the correct microphone

configuration is set in the system.

1) Log in to the machine remotely via NoMachine. Then click the desktop

icon to access the configuration interface.

2) First, verify the system language setting.



=== Page 2 ===
4 iwennder Shenzhen Hiwonder Technology Co,Ltd

3) For the Six-Microphone Circular Array, select xf as the microphone

type as shown in the figure.

AP Name HW-24347055

Version V1.1.1 2025-05-20

whens EE
ROS2 Humble

4) After making the appropriate selection, click Save.

5) A “Save Success” message will confirm that the configuration has

been stored in the system environment.

Save Success



=== Page 3 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

6) Then, click Quit to close the interface.

2.2 Configuring the Large Model API-KEY

Open a new command-line terminal and enter the following command to

access the configuration file.

vim /home/ubuntu/ros2_ws/src/large_models/large_models/large_models/config.py

Refer to the tutorial under “Al Large Language Model Course / 1 Large
Language Model Basic Courses/1.1 Large Language Model Courses/1.1.2
Large Language Model Deployment” to obtain the API keys for vision language
model. For the vision language model, acquire the API key from the
OpenRouter official website and assign it to the vllm_api_key variable. For
the large language model, obtain the API key from the OpenAl official website
and assign it to the 11m _api_key variable. Ensure that the keys are inserted
in the designated positions, as indicated by the red boxes in the reference

image.

25, vllm_apt_key =

26 vllm_base url =
27 villm_model =

9) LLm api key =

© Llm_base_url =

1 Llm_model =

2 openai_vllm_model =
3 openai_tts_model =

4 openai_asr_model =
35 openai_voice_model =

3. Enabling and Disabling the Feature

@ 1. Command input is case-sensitive and space-sensitive.

3



=== Page 4 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

2. The robot must be connected to the Internet, either in STA (LAN)

mode or AP (direct connection) mode via Ethernet.

1) Open the command line terminal from the left side of the system

interface Sal In the terminal window, enter the following command

and press Enter to stop the auto-start service

sudo systemctl stop start_app_ node.service

stop start app node.service

2) Enter the following command and press Enter to launch the feature.

ros2 launch large models examples vllm_track.launch.py

Launch lLarge_models_examples vllm_track.launch.py

3) When the terminal displays output shown in the figure and the Circular
Microphone Array announces "I’m Ready", the device has completed
initialization and yolov8 model will be initialized at the same time. Then, you

can say the wake words: "Hello Hiwonder".

[ component_container_isolated-21] [INFO] [1742389931.895892168] [bt_navigator]: Creating bond (bt_navigator) to lifecycle manager.
[agent_process-6] [INFO] [1742389933.301530290] [agent_process]:
[tts_node-7] [INFO] [1742389933.680274215] [tts_node]:

[agent_process-6] [INFO] [1742389935.057819613] [agent_process
[agent_process-6] [INFO] [1742389935.273320881] [agent_proces

ye
132

aS

4) When the terminal displays the corresponding output shown in the
figure and the device responds with "I’m Here", it indicates successful

activation. The system will begin recording the user's voice command.

The system will begin recording the user’s voice command, and you can
say the command “Follow the person in white in front’ and wait for the large

model to process the input.

5) Upon successful recognition by the speech recognition service of

cloud-based large speech model, the parsed command will be displayed under

4



=== Page 5 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

the publish asr_ result output in the terminal.

[vocal_detect-3] [INFO] [1745641127.567506851] [vocal_detect]:
[vocal_detect-3] [INFO] [1745641131.038258861] [vocal_detect]:
[vocal_detect-3] [INFO] [1745641133.543467960] [vocal_detect]: asr time: 2.51

[vocal_detect-3] [INFO] [1745641134.699970578] [vocal_detect]: Follow the person in
white in front.

6) Upon receiving user input shown in the figure, the terminal will display
output indicating that the cloud-based large language model has been
successfully invoked. The model will interpret the command, generate a
language response, and execute a corresponding action based on the

meaning of the command.

@ tre response is automatically generated by the model. While the
semantic content is accurate, the wording and structure may vary due to

randomness in language generation.

7) When the terminal shows the output shown in the figure indicating the
end of one interaction cycle, the system is ready for the next round. To initiate

another interaction, repeat step 4 by speaking the wake words again.

: synthestzer ts opened.
Speech synthesizer is completed.

[tts_node-7] [INFO] [1742524778.951139572] [tts_node]: Speech synthesizer is closed.
[vocal_detect-5] [INFO] [1742524778.958064206] [vocal_detect]:

8) To exit the feature, press Ctrl+C in the terminal. If the feature does not

exit immediately, press Ctrl+C multiple times.

4. Project Outcome

Once the feature is activated, natural language commands such as
“Follow the person in white in front.” can be used. The robot will identify the
person wearing white in the camera view and begin following them. It will

automatically stop once a certain distance has been reached.



=== Page 6 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

5. Program Brief Analysis

5.1 Launch File Analysis

File Path:

/home/ubuntu/ros2_ws/src/large_models_examples/large_models_e

xamples/vilm_track.launch.py

@ Import Libraries

1 import os

2 from ament_index_python.packages import get_package_share_directory

3

4 from launch_ros.actions import Node

5 from launch.substitutions import LaunchConfiguration

6 from Launch import LaunchDescription, LaunchService

7 from Launch. launch_description_sources import PythonLaunchDescriptionSource

8 from Launch.actions import IncludeLaunchDescription, DeclareLaunchArgument, OpaqueFunction

@ os: used for handling file paths and operating system-related

functions.

@ ament index python.packages.get package share direc

tory: retrieves the share directory path of ROS 2 package.

(6) launch ros.actions.Node: used to define ROS 2 nodes.

@ launch.substitutions.LaunchConfiguration: retrieves

parameter values defined in the Launch file.



=== Page 7 ===
Hivwonder Shenzhen Hiwonder Technology Co,Ltd

© LaunchDescription, LaunchService: used to define and start

the Launch file.

© launch description sources

PythonLaunchDescriptionSource: enables the inclusion of other

Launch files.

@ launch.actions.IncludeLaunchDescription,
DeclareLaunchArgument, OpaqueFunction: used to define actions and

arguments within the Launch file.

@ Definition of the launch _setup Function

10 def Launch_setup(context):

11 mode = LaunchConfiguration( '™ , default=1)

12 mode_arg = DeclareLaunchArgument( ‘+ , default_value=mode)
13

14 slam_package_path = get_package _share_directory( )
15 large_models_package_path = get_package_share_directory( ge_ )
16

17 base_lLaunch = IncludeLaunchDescription(

18 PythonLaunchDescripttonSource(

19 os.path. join(slam_package_path, J ))5
20 Launch_arguments={

21 : -

22 t : os.environ[ ER'],

23 e': os.environ[ ‘| ]

24 }.items(),

25 )

26

27 large_models_launch = IncludeLaunchDescription(

28 PythonLaunchDescriptionSource(

29 os.path. join(large_models_package_path, t J Ws
30 Launch_arguments={ : mode}.items(),

31 )

32

33 vllm_track_node = Node(

34 package=

35 executable=

36 output='

37 )

38

39 # rqt

40 calibrate_rqt_reconfigure_node = Node(

41 package= t_re f

42 executable=

43 name=

44 )

45

46 return [mode_arg,

47 base_launch,

48 Large_models_launch,

49 vllm_track_node,

50 # calibrate_rqt_reconfigure_node,

51 ]

@ The launch setup function includes the file robot.launch.py

from the slam package, which is responsible for launching the robot's chassis
control and initializing related peripheral connections with corresponding

parameters.

@ It also includes the file start. launch.py from the large models

7



=== Page 8 ===
4 iwennder Shenzhen Hiwonder Technology Co,Ltd

package, which launches the intelligent agent equipped with the capabilities of

“seeing, listening, thinking, and speaking.”

@ Additionally, it includes the vllm_ track executable file from the

large models examples package, which serves as the main node for the

visual tracking functionality.

@ Definition of the generate launch description Function

53 def generate_lLaunch_descripttion():

54 return LaunchDescription([
55 OpaqueFunction(function = Launch_setup)
56 ])

7

@ This function is responsible for generating the complete Launch

description.

@ The launch setup function is incorporated using
OpaqueFunction.
@ Main Program Entry

79Af  name_ == H

80 # create a LaunchDescription object
81 ld = generate_lLaunch_description()
82

83 ls = LaunchService()

84 ls.include_Launch_description(1ld)
85 ls.run()

@ 1d= generate launch description() generates the Launch

description object.

@ 1s = LaunchService() creates the Launch service object.

@ 1s.include launch description(ld) adds the Launch

description to the service.

@ 1s.run() starts the service and execute all Launch actions.



=== Page 9 ===
4 IV/EM Oo er Shenzhen Hiwonder Technology Co,Ltd

5.2 Python File Analysis

The source code for this program is located at:

/home/ubuntu/ros2_ws/src/large_models_examples/large_models_e

xamples/vilm_track.py

@ Import the necessary libraries

5 import cv2

6 import json

7 import time

8 import queue

9 import rclpy

10 import threading

11 import PIL.Image

12 import numpy as np

13 import sdk.fps as fps

14 import message filters

15 from sdk import common

16 from rclpy.node import Node

17 from sensor_msgs.msg import Image

18 from geometry_msgs.msg import Twist

19 from std_msgs.msg import String, Float32, Bool

20 from std_srvs.srv import Trigger, SetBool, Empty

21 from rcl_interfaces.msg import SetParametersResult

22 from rclpy.executors import MultiThreadedExecutor

23 from rclpy.callback_groups import ReentrantCallbackGroup

24

25 from speech import speech

26 from Large_models.config import *

27 from Large_models_msgs.srv import SetString, SetModel, SetInt32
28 from Large_models_examples.track_anything import ObjectTracker
29 from servo_controller.bus_servo_control import set_servo_position
30 from servo_controller_msgs.msg import ServosPosition, ServoPosition

@. Image Processing

cv2: provides core functions for image processing and display using

OpenCV.

PIL.Image and numpy: used for image data manipulation and numerical

computations.
@ ROS Communication and Synchronization

rclpy: core library for creating ROS 2 nodes and handling

communication.

message filters: ensures time synchronization between multi-sensor

data streams, such as RGB and depth images.

sensor msgs, geometry msgs, std_msgs, std_srvs,



=== Page 10 ===
4 IV/EM | er Shenzhen Hiwonder Technology Co,Ltd

rcl_interfaces: core message and service types used for ROS data

and service exchange.

MultiThreadedExecutor and ReentrantCallbackGroup: enable
concurrent callbacks and multithreaded scheduling to maintain system

responsiveness and real-time performance.

@ PROMPT String

It defines a prompt string (PROMPT) used to guide how user commands and
image data are processed. It specifies the recognition task logic and expected

output format.

10



=== Page 11 ===
HIVVEXMCOECT Shenzhen Hiwonder Technology Co,Ltd

@ VLLMTrack Class

84 display_size = [int(640*6/4), int(480*6/4))
85 class VLLMTrack(Node):
def init__(self, name):
rclpy.init()
super().__init__(name)
self.fps = fps.FPS() # (frame rate counter)
self.image_queve = queve.Queve(maxsize=2)
self.vllm_result =
self.set_above =
self.running =
self.data = []
self.box = [J
self.stop =
self.start_track «=
self.actton_fintsh =
self.play_audto_fintsh =
self.track = ObjectTracker(use_mouse= », automatice= , logeself.get_logger())

timer_cb_group = ReentrantCallbackGroup()

self.cltent = speech.OpenAIAPI(apt_key, base_url)

self.jotnts_pub = self.create_publisher(ServosPositton s: 1)

self.mecanum_pub = self.create_publisher(Twist, t 5, a

self.tts_text_pub = self.create_publisher(String, tt ext’, 1)

self.create_subscriptton(Bool, ‘tt f » Sself.play_audto_fintsh_callback, 1, callback_group=timer_cb_group)

self.create_subscriptton(String, t t', self.vllm_result_callback, 1)
self.create_subscriptton(Bool, |v t k » Sself.wakeup_callback, 1)

self .awake_cltent = self.create_client(SetBool, tect t k )
self .awake_cllent.watt_for_service()

self.set_model_client = self.create_client(Setmodel,
self.set_model_client.watt_for_service()

self.set_mode_cltent = self.create_cltent(SetInt32,
self.set_mode_client.watt_for_service()

self.set_prompt_cllent = self.create_cllent(SetString,
self.set_prompt_cltent.watt_for_service()

image_sub = message_filters.Subscriber(self, Image,
depth_sub = message_filters.Subscriber(self, Image,

sync = message_filters.ApproximateTimesynchronizer([depth_sub, image_sub],
sync. registerCallback(self.multt_callback)

self.pid_params = {

}
@) display size: defines the size of the display window.
@ rclpy.init(): initializes the ROS 2 system.

@ super().init (name): calls the initializer of the parent class (Node)

to create a ROS 2 node.

@ fps.FPS(): creates an instance of a frame rate tracker.

© queue. Queue (maxsize=2): creates a queue with a maximum

capacity of 2.

© self.vllm result: stores the results processed by the model.

@ self.running: a boolean flag used to control the runtime state of

the program.

11



=== Page 12 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

self .data: an empty list used for storing intermediate data.

@ self.start track: a boolean flag used to control the start or stop

of target tracking.

self.track = ObjectTracker (): instantiates an object tracker.

11. timer cb group: creates a reentrant callback group to allow

concurrent callback execution.

12 speech.OpenAIAPI (): instantiates a speech interaction client.

13. self.joints pub: creates a publisher for sending servo motor

position commands.

14 self.tts_text pub: a publisher is created for sending text data to

the text-to-speech node.

15 self.create subscription: subscribes to image data, speech

playback completion signals, and image recognition results.

16 self.create client: creates service clients for tasks such as

wake-up control, model configuration, and motion control.

17 Each PID parameter in self.pid_params is declared as a ROS 2

dynamic parameter with a default value.

18 Current values are retrieved using self.get parameter() and

used to update self.pid params.

19 self.track.update pid/(): passes the updated PID parameters

to the object tracker to adjust its PID controller.

12



=== Page 13 ===
HIVVEXMCOECT Shenzhen Hiwonder Technology Co,Ltd

20 self.pid params: contains the PID controller parameters used in

the target tracking control logic.

21 self.add_on set parameters callback: registers a callback

function to handle parameter updates dynamically.

146
147
148
149
150
151
152
153
154
155

156
407

111
112

113
114
115
116
117

118
119

22 self.timer: creates a timer used to trigger the initialization process.

@ on_parameter_update Method

def on_parameter_update(self, params):

for param in params:
if param.name in self.pid_params.keys():
self.pid_params[param.name] = param.value
# self.get_logger().info(f'PID parameters updated: {self.pid_params}')

self.track.update_pid([self.pid_params['kpi'], self.pid_params[ ], self.pid_params[
[self.pid_params['kp2'], self.pid_params[ '} ], self.pid_params[ 'kd2']])

def on_parameter_update(self, params):

for param in params:
if param.name in self.pid_params.keys():
self.pid_params[param.name] = param.value
# self.get_logger().info(f'PID parameters updated: {self.pid_params}')
# BHipin Sw
self.track.update_pid([self.pid_params[ ], self.pid_params[ ], self.pid_params[ 33,
[self.pid_params[ ], self.pid_params[ ], self.pid_params[ 1))

@ self.pid params: a dictionary storing PID parameters.

return SetParametersResult(successful= e)

@ self.track.update pid: updates the PID parameters used by

the target tracking module.

@ create _update_callback Method

158 def create_update_callback(self, param_name):

159

160 def update_param(msg):

161 new_value = msg.data

162 self.pid_params[param_name] = new_value

163 self.set_parameters([Parameter(param_name, Parameter.Type.DOUBLE, new_value)])
164 self.get_logger().info(f'! ated ram_n ew e}')
165

123 def create_update_callback(self, param_name):

124 st

125 def update_param(msg):

126 new_value = msg.data

127 self.pid_params[param_name] = new_value

128 self.set_parameters([Parameter(param_name, Parameter .Type.DOUBLE, new_value)])

129 self.get_logger().info(f

130

# BRpio SH

13



=== Page 14 ===
HIVVEXMCOECT Shenzhen Hiwonder Technology Co,Ltd

It generates a dynamic update callback function for each PID parameter.

@ get_node state Method

134 def get_node_state(self, request, response):
135 return response

return response: returns a response object.
@ init_process Method

def init_process(self)
self.timer.cancel()

msg = SetModel.Request()
msg.model_type = ‘viim'
Uf language == ‘Chinese’:
msg.model = stepfun_vllm_model
msg.apit_key = stepfun_apt_key
msg.base_url = stepfun_base_url
else:
msg.api_key = vllm_apt_key
msg.base_url = vllm_base_url
msg.model vllm_model
self.send_request(self.set_model_client, msg)

9 SetString.Request()
g.data PROMPT
elf .send_request(self.set_prompt_client, msg)

m
m

set_servo_positton(self.joints_pub, 1.5, ((10, 300), (5, 500), (4, 100), (3, 100),

self .mecanum_pub.publish(Twist())

time.sleep(1.8)

speech.play_audto(start_audto_path)

threading. Thread(target=self.process, daemon=True).start()
threading. Thread(target=self.display_thread, daemon=True).start()
self .create_service(Empty, -/init_finish', self.get_node_state)
self .get_logger().info('\033[1;32m%s\033[0m' % ‘start’)

self .get_logger().info( '\033[1;32m%s\033[0m' % PROMPT)

@ self.timer.cancel(): cancels the timer.

(2, 750), (1, 500)))

@ SetModel.Request (): creates a request message to set the

model.

@ SetString.Request (): creates a request message to set the

prompt string.

@ set pose target: sets the robot's initial position.

© self.send request: sends a service request.

© set servo position: sets the position of servo joints.

@ speech.play audio: plays an audio file.

14



=== Page 15 ===
4 IV/EM | er Shenzhen Hiwonder Technology Co,Ltd

threading.Thread: starts a new thread for image processing and

tracking.

@ self.create service: creates a service to signal the completion

of initialization.
self.get logger () .info: prints log messages.

@ send_request Method

160 def send_request(self, client, msg):

161 future = client.call_async(msg)

162 while rclpy.ok():

163 if future.done() and future.result():
164 return future.result()

14c

@ client.call_ async (msg): makes an asynchronous service call.

@  future.done() and future.result (): check if the service call

is complete and retrieve the result.

@ vllm_result_callback Method

217 def vilm_result_callback(self, msg):
218 self.vllm_result = msg.data

aan

178 def vllm_result_callback(self, msg):

179 self.get_logger().info(f! WAIVLt y.data}')
180 self.vllm_result = msg.data

181

This callback receives results from the agent _process/result topic
and stores them in self.vllm_ result.
@ Play audio finish_callback Method

182 def play_audio_ finish_callback(self, msg):
183 self.play_audio_ finish = msg.data

ana

The play audio finish callback method sends a wake-up signal

after audio playback is completed.

@ process Method

15



=== Page 16 ===
HIVVEXMCOECT Shenzhen Hiwonder Technology Co,Ltd

def process(self):
box = ''
while self.running:
Af self.vllm_result:
try:
# self.get_logger().info('vllm_result: %s' % self.vllm_result)
Af self.vllm_result.startswith(" ") and self.vllm_result.endswith(” =)
self.vllm_result self.vllm_result.strip(” ").replace("json\n", “").strip()
self.vllm_result json. loads(self.vllm_result)
response = self.vllm_result[ ‘response’ ]
msg String()
msg.data = response
self.tts_text_pub.publish(msg)
box = self.vllm_result[ 'xyxy']
Af box:
if language == ‘Chinese’:
box = self.client.data_process(box, 640, 480)
self .get_logger().info('box: %s' % str(box))
else:
box = [int(box[0] * 640), int(box[1] * 480), int(box[2] * 640), int(box[3] * 480)]
# self.get_logger().info('box: %s' % str(box))
box [box[0], box[1], box[2] - box[0], box[3] box[1]]
box[0] int(box[0] / 640 * display_size[0])
box[1] int(box[{1] / 480 * display_size[1])
box[2] int(box[2] / 640 * display_size[0])
box[3] int(box[3] / 480 * display_size[1])
self .get_logger().info('box: %s' % str(box))
self .box box
except (ValueError, TypeError):
self. box 0)
msg = String()
msg.data = self.vllm_result
self.tts_text_pub.publish(msg)
self.vllm_result = °'
self.action_finish = True
else:
time.sleep(0.02)
if self.play_audio_finish and self.action_finish:
self.play_audio_finish = False
self.action_finish = False
msg SetBool.Request()
msg.data True
self.send_request(self.awake_client, msg)
# msg SetInt32.Request()
# msg.data = 1
# self.send_request(self.set_mode_client, msg)
self.stop = False

@ while self.running: ensures continuous data processing while

the node is active.

@ ifself.vllm_result: checks whether a new VLLM response has

been received.

@ Json.loads(self.vllm result): parses the received

JSON-formatted string into a dictionary for easier data extraction.
@ Extracting response and xyxy:

response = self.vllm_result['response']: retrieves the text

message for TTS (Text-to-Speech) playback.

box = self.vllm_result['xyxy']: retrieves the bounding box

coordinates returned from object detection.

© Processing Bounding Box Data:

16



=== Page 17 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

If a target is detected and the box is not empty, the method calls
self.client.data process to normalize the coordinates using a frame
size of 640 by 480. It then converts the coordinates into the format [xmin, ymin,
width, height], scales them based on the display window size, and stores the

result in self.box for tracking.

© Sending TTS Response:
A String message is created and the extracted response is sent to the

TTS node for speech playback.

@ Error Handling:
Ifa ValueError orf TypeError occurs during parsing or extraction, the
target info is cleared, and the original vllm_ result is sent to the TTS node

as plain text.

State Updates:

self.vllm_ result is reset to an empty string to indicate the response

has been processed.

self.action finish is set to True to mark that the action has been

completed.

Q) Loop Wait Mechanism:
If no new vllm_ result is received, the loop sleeps for 20 ms using

time.sleep (0.02) to avoid high CPU usage.

Subsequent Service Request: when both

self.play audio finishand self.action finish are True,

related flags are reset.

Aservice request is sent via

self.send_ request (self.set mode client, msg) to set the mode to

2, allowing target tracking to begin.

17



=== Page 18 ===
4 IV/EM Oo er Shenzhen Hiwonder Technology Co,Ltd

self.stop is updated to False, lifting the pause on target tracking.

@ track thread

def display_thread(self):
ae ESM OU —TIB cuva E FX
dev cuda.Device(0)
ctx dev.make_context()
try:
model_path os.path.split(os.path.realpath(_ file__))[0]

back_exam_engine_path os.path.join(model_path, “resources/models/nanotrack_backbone_exam.engine”)
back_temp_engine_path os.path.jotn(model_path, "“resources/models/nanotrack_backbone_temp.engine”)
head_engine_path os.path.jotn(model_path, "resources/models/nanotrack_head.engine”)
tracker Tracker(back_exam_engine_path, back_temp_engine_path, head_engine_path)
while self.running:
image, depth_image self.image_queve.get(block=True)
image cv2.restze(tmage, tuple(display_stze))
Af self.box:
self.track.set_track_target(tracker, self.box, image)
self .start_track True
self.box = []
Af self.start_track:
self.data = self.track.track(tracker, image, depth_image)
image self .data[-1]
twist Twist()
twist.linear.x, twist.angular.z self.data[0], self.data[1]
self .mecanum_pub.publish(twist)
self. fps.update()
self. fps.show_fps(image)
cv2.imshow(' image’, image)
key = cv2.waitKey(1)
if key ord('q') or key 27: # f&qatHesciki
self .mecanum_pub.publish(Twist())
self.running False
if not self.set_above:
cv2.moveWindow(' image’, 1920 - display_size[0], 0)
os.system("wmctrl -r image -b add,above")
self.set_above True

cv2.destroyAllWindows()
finally:
# GRE FREER

ctx.pop()

@® while self. running:

controls the thread's runtime, and the thread continuously processes

image data as long as self.running is True.

@ image, depth image = self.image queue.get (block=True):

fetches the latest RGB and depth images from the thread-safe queue to

ensure synchronized data processing.

@ ev2.resize:

resizes the retrieved images to the predefined display size for easier

processing and visualization.

@ if self.box:

checks if a new detection bounding box (box) is available. If available, the
method calls self.track.set track target (self.box, image) to set

the current tracking target in the image, sets self.start track = True,

18



=== Page 19 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

and clears self.box.

© ifself.start_track:

verifies if target tracking has been initiated. If True, it calls
self.track.track(image, depth image) to obtain the tracking result,

which is then stored in self.data.

@ Use of self.data:

the first two values in the tracking result usually represent control
commands, such as linear and angular velocity. The last element is the image

annotated with the tracking results, which is used for display.

@ Construct and Publish Twist Messages:
a Twist message is constructed using control commands extracted from
self.data and published via self.mecanum_ pub.publish (twist) to

control the movement of the robot base.

self.fps.update and self.fps.show_fps:

update the frame rate statistics and overlay the current FPS on the image

for real-time performance monitoring.

Q ecv2.imshow:

display the processed image in a window to provide a visual

representation of the target tracking status.

cv2.waitKey:
wait for keyboard input. If the user presses the q or ESC key, the tracking

thread is stopped and a stop-motion Twist message is sent to halt the robot.

11. Window on Top Handling:
The display window is repositioned using cv2 .moveWindow, and the

command os.system("wmctrl -r image -b add, above") sets the

19



=== Page 20 ===
Hivwoander Shenzhen Hiwonder Technology Co,Ltd

window to stay on top, ensuring it's always visible.
12 cv2.destroyAllWindows:

after the tracking thread exits, all OpenCV-created windows are closed to

complete the cleanup process.

@ main Method

260 def main():

261 node = VLLMTrack( 1 )

262 executor = MultiThreadedExecutor()

263 executor.add_node(node)

264 executor.spin()

265 node.destroy_node()
@ An instance of the VLLMTrack node is created.
@  Amultithreaded executor is used to handle the node’s tasks.
@ Callexecutor.spin() to start processing ROS events.
@ Upon shutdown, the node is properly destroyed using

node.destroy node().

20


