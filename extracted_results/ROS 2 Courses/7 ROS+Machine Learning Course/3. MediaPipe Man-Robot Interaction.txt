
=== Page 1 ===
ht IW/EM Oo er Shenzhen Hiwonder Technology Co,Ltd

MediaPipe Man-Robot Interaction




=== Page 2 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd
Catalog

1. MediaPipe Introduction ....... cc cc ccc ccc ee cece eee eee eee eees
1.1 MediaPipe Description ......... ce cece ce eect eee eee eee eee eens

1.2 MediaPipe Pros and Cons ......... ccc ccc cece cece eee eee eee eee
1.2.1 MediaPipe Pros ....... cc cc cc ee ccc ee eee eee eee eee teen eee

1.2.2 MediaPipe Cons ........ ccc cc ccc eee cece eee ee eees
1.3 How to use MediaPipe ....... cee ee ce cece ee eee eee eee wees
1.3.1 Dependency ........ ccc cece eee cee eee ee eee e eee teenies

1.3.2 MediaPipe Solutions ........ cece ec ccc eee eee eens

1.4 MediaPipe Learning Resources ......... cece eee eee eee ee eee eee

2. Image Background Segmentation .......... cece ccc eee eee eee teenies
2.1 Program LOGIC 2... . ec eee cece ee ee eee eee cette eee eee eees

2.2 Operation StepS 2... cece cee ec eee ee ee eee ee eee eee e ee ees

2.3 Program OutCOMe ...... cee cece eee eee cece eee eee eees 10

2.4 Program AnalySis ........ cece eee cece eee cere cece eee eee eeees 10
2.4.1 FUNCHION 2... cee cc ccc cee cece ee eee ee eee eee eee eee eens 10
2.4.2 CIASS 0. ccc ccc ccc ccc eee cece eee eee eee eee e eee ee eee 11
3. 3D Object Detection .. 2... cece ce ccc ec cece erect e ee eee teens 12
3.1 Program LOGIC 1... . cece cc ccc ee ee cece eee eee eee eect renee 12
3.2 Operation StepS 2... ccc ce ccc eee eee eee tree eee eee ees 12

3.3 Program OUtCOME ... 1... cece ec eee cee eee ee eee e ee eeee 13

3.4 Program AnalySiS ....... cc cece ccc eee cece eee eee eee cece eens 14



=== Page 3 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

3.4.1 FUNCHION 2... eee cece cee ee eee eee reece eee teenies 14

3.4.2 CIASS 2... ccc ccc ccc cece eee eee eee eee e eee scenes 15

4. Face Detection ....... ccc cc ccc cc ccc ce eee eee reece teen e econ 16
4.1 Program Logic 2... ccc ec eect ee eee ee eee ee eee eee eens 16

4.2 Operation StepS ..... cece cc eee eee eee eect eens 17

4.3 Program OutCOME ...... eect ee eee eee ee eee eee eee 18

4.4 Program AnalySis .. 1... .. ccc cece cece rece ee eee cece reece eens 18
4.4.1 FUNCHON 2... cece ccc eect ee ee eee etree eee e eee 19

4.4.2 ClASS 2. ccc cc ce eee eee eee eee ee eee eee eee e ees 19

5. 3D Face Detection ........ cece cc ccc ee ect ee eee eee reece teen eee 20
5.1 Program LOGIC .. 1... cece ee ccc ce ect ee eee eee eee eee e eee 21

5.2 Operation StepS 2... ccc cece cece eee eee eect eee eee eee e ees 22

5.3 Program OUtCOMe ...... cece cece eee cee eee eee ee eee eeee 23

5.4 Program AnalySiS ....... ccc ccc ce cee cece eee cece eee ee eee sees 23
5.4.1 FUNCHION 2... ec cece ee reece eee eee ee eee ee eees 23

5.4.2 CIASS 2. ccc ccc ccc cece eee eee eee eee ee eee e cess eens 24

6. Hand Key Point Detection ......... ccc ccc ce eect eee eect eee eens 25
6.1 Program LOGIC .. 1... cece ec ce cee ct eee ee eee eee eee eee eee 25

6.2 Operation StepS 2... cece ce ccc ec eee eect eect e eee eens 26

6.3 Program Outcome ...... cece cee cee eee eee ee eect eens 27

6.4 Program AnalySisS ........ cc cece cc eee cee eee ee eee eee eee eee 28
6.4.1 FUNCHION 2... ce cece e eee eter e eee eee eens 28



=== Page 4 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

6.4.2 ClaSS . oc. ec ccc eee eee eee eee ee eee eee e eens 28

7. Body Key Points Detection ... 2... .. ccc cc ccc ce cece ee eee eee teen nee 30
7.1 Program LOGIC ...... cece ee ccc ce ee eet eee ee eee eee eee eee eee 30

7.2 Operation StepS 2... ce ccc ccc eee eee eect eee eee eee ees 31

7.3 Program OUtCOMe ...... cece cece ec cee eee ee eee eee teens 32

7.4 Program AnalySisS 10... .. cc cece cece ee cece eee eee eee eee eee eee 33
7.4.1 FUNCHION 2... ec ce ec ccc ee ee eee eee eee erect eeees 33

TA.2 CIASS 2. ccc ccc ccc cece cc cee eee eee eee eee eee scenes 33

8. Fingertip Trajectory Recognition ........ ee cee ee cece eee eee ee eens 35
8.1 Program LOGIC 1.1... cece cece ee eect eee eee eee eee eee e ee eee 35

8.2 Operation StepS 2... cece ce cece eee eee eee teen eee teens 35

8.3 Program OUtCOME ... 1... ee ccc eee cee eee ee eee eee e eens 36

8.4 Program AnalySiS ........ cece cee eee cece eee eee eee eee eee eens 37
8.4.1 FUNCHION 2... ccc ce ccc cc eee cee ee eee eee eee eee eee eeeee 39
8.4.2 ClASS 2... ce cece eee ee eee eee eee ee eee eee eees 43

9. Posture Control ..... ccc ccc cc cc cece eee eee eee eee eee ee eees 44
9.1 Program LOGIC 1... . cece ec ee eect ee eee eee eee eee e eens 45

9.2 Operation StepS .... cee cece cee eee ee eee eee eee teens 46

9.3 Program OUtCOMe ...... cee cc cece cee eee eee eee e teense 46

9.4 Program AnalySisS 2... .... cee cece ce ee cet eee eee eee eee cere eens 47
9.4.1 FUNCHION 2... cece cc ec cc ee eee eee ee ee eee eee teenies 49

9.4.2 ClASS . oc. ccc ccc ccc ccc ec cee eee eee eee eee ee eee eeee 51



=== Page 5 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

10. Human Body Tracking ......... cee ccc ee cece e ee eee cece eee e eens 53
10.1 Program LOGic 2... .. cece ec ccc e ce eect e ee eee ee eee teenies 53
10.2 Operation Steps 2... ccc cc ccc cece ee eee eee eee e eens 54
10.3 Program Outcome ....... cece ccc cee cece eee eee e eee 55
10.4 Program Analysis ....... cece ec ee cece eee cece rete eees 55

10.4.1 FUNCTION 22... ccc ce eee cee ee eee erent e teen eee 58
10.4.2 ClaSS 2. ccc ccc ce ccc ee eee eee eee eee ee eees 58
10.5 Function Expansion ....... cece ec cee eee eect eee ee eees 62

11. Integration of Body Posture and RGB Control ............ 0c. eee eee eee 62
11.1 Program LOGIC... cece ccc cee eee eee e eee e eee eens 63
11.2 Operation StepS 2... ccc cece ee eee eee eee eee eees 63
11.3 Program Outcome ...... cece ccc cc eee eect eee eee ee eee 64
11.4 Program Analysis ......... cece ccc ce eee ee eee eee cere e eens 66

11.4.1 FUNCTION 2... cece ce eect ee eee eee reece eee e eee 69
11.4.2 ClASS 2. ccc ce eee eee eee eee e eee e eens 71

12. Pose Detection 0... ccc ccc ccc ce eect eee eee eee eee eee eeeee 73
12.1 Program LOGIC ... 1. . eee cee ccc ee eee eee e ee eee eees 74
12.2 Operation StepS 2... ccc ccc cee cece eee ee eee eeee 74
12.3 Program Outcome ... 2... cece cece eee eee ee eee ee eees 75
12.4 Program Analysis ......... cece cee cee eee ee eee eee eee ee eees 75

12.4.1 FUNCHION .. 0... ccc cc ce eect eee eect ee eee eee e eens 77
12.4.2 ClaSS 2. ccc ccc cece eee eee eee eter e eee 78



=== Page 6 ===
4 IVW/E) 1 Oo =) t Shenzhen Hiwonder Technology Co,Ltd

1. MediaPipe Introduction

1.1 MediaPipe Description

MediaPipe is an open-source framework of multi-media machine learning
models. Cross-platform MediaPipe can run on mobile devices, workspace and
servers, as well as support mobile GPU acceleration. It is also compatible with
TensorFlow and TF Lite Inference Engine, and all kinds of TensorFlow and TF
Lite models can be applied on it. Besides, MediaPipe supports GPU

acceleration of mobile and embedded platform.

This product has multiple features for an
affordable price. My experience has been
fantastic so far.

It has very responsive live chat support

Positive eeee te

Negative @

1.2 MediaPipe Pros and Cons

1.2.1 MediaPipe Pros

1) MediaPipe supports various platforms and languages, including iOS,
Android, C++, Python, JAVAScript, Coral, etc.

2) Swift running. Models can run in real-time.

3) Models and codes are with high reuse rate.



=== Page 7 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

1.2.2 MediaPipe Cons

1) For mobile devices, MediaPipe will occupy 10M or above.

2) As it greatly depends on Tensorflow, you need to alter large amount of
codes if you want to change it to other machine learning frameworks, which is
not friendly to machine learning developer.

3) It adopts static image which can improve efficiency, but make it difficult to

find out the errors.

1.3 How to use MediaPipe

The figure below shows how to use MediaPipe. The solid line represents the
part to coded, and the dotted line indicates the part not to coded. MediaPipe

can offer the result and the function realization framework quickly.

( Data ) |

Process | | Convert Build Output
> b = - =
Data | Data "| Model Data "Fompute

Result }

1.3.1 Dependency

MediaPipe utilizes OpenCV to process video, and uses FFMPEG to process
audio data. Furthermore, it incorporates other essential dependencies,

including OpenGL/Metal, Tensorflow, and Eigen.

For seamless usage of MediaPipe, we suggest gaining a basic understanding
of OpenCV. To delve into OpenCV, you can find detailed information in "2

Basic Development Courses/ 3. OpenCV Computer Vision Courses".

7



=== Page 8 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

1.3.2 MediaPipe Solutions

Solutions is based on the open-source pre-constructed sample of TensorFlow
or TFLite. MediaPipe Solutions is built upon a framework, which provides 16
Solutions, including face detection, Face Mesh, iris, hand, posture, human

body and so on.

1.4 MediaPipe Learning Resources

MediaPipe website: https://developers.google.com/mediapipe
MediaPipe Wiki: http://i.bnu.edu.cn/wiki/index.php?title=Mediapipe
MediaPipe github: https://github.com/google/mediapipe

Dlib website: http://dlib.net/

dlib github: https://github.com/davisking/dlib

2. Image Background Segmentation

This lesson provides instructions on utilizing MediaPipe's selfie segmentation
model to accurately segment trained models, such as human faces and hands,
from their backgrounds. Once separated, you can easily add virtual

backgrounds to these models.
2.1 Program Logic

To begin, import the selfie segmentation model from MediaPipe and subscribe

to the corresponding topic to access the live camera feed.

Next, flip the image and apply the segmentation to the background image. For

improved boundary segmentation, implement dual-border segmentation.



=== Page 9 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

Finally, complete the process by replacing the background with a virtual

background.

2.2 Operation Steps

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the command to enable the camera node.

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth _camera.launch.py

5) Enter the command and hit Enter key to run the game program.

cd ~/ros2_ws/src/example/example/mediapipe_example && python3

self_segmentation.py

~/ros2_ws/src/example/example/mediapipe example && self_segmentation.py

6) If you want to exit this game, please press ‘Esc’ key to exit the camera

interface. If the game cannot be closed, please retry.

7) Next, press "Ctrl+C" in the terminal. If it fails to close, please try again.



=== Page 10 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

2.3 Program Outcome

Once the game starts, the screen will transition to a gray virtual background.
As soon as hand appears, the program will automatically execute background

removal, effectively separating the human from the virtual background.

,
MediaPipe Selfie Segmentation > MediaPipe Selfie Segmentation - ™

2.4 Program Analysis
The source code of this program locates in:

— Function main Invoke node

self segmentation.py Background _—init__ Initialization
a segmentation Image callback

Class SegmentationNode class! image callback —fynetion

main Class main function

lros2_ws/src/example/example/mediapipe_example/self_segmentation.p

y

2.4.1 Function

main
node = SegmentationNode

ooo

rcLpy.spin(node
KeyboardiInterrupt:

node.destroy_node

rcLpy.shutdown

oOo oo

0
1
2
3
4
5
6
7
8

owmnm a

Be]

10



=== Page 11 ===
HIW/E9MOECT Shenzhen Hiwonder Technology Co,Ltd
Used to start the background control node.
2.4.2 Class

SegmentationNode:

SegmentationNode(Node):

yn = mp.soLlutions.selfie_segmentation
rawing = mp.soLutions.drawing_utils

= fps.FPS
e_queue = queue.Queue(maxsize=2
bscr ion(Image,
. Lmage_calLback
.get_Logger().info('\@33[1;32m%s\033[@ start
threading. Thread(target main, daemon 2). Start

Init:

— ARLE name):
reLpy.init
super().__init__(name
-running =
mp_selfie_segmentation mp.soLlutions.selfie_segmentation
mp_drawing = mp.solutions.drawing_utils
-fps = fps.FPS
image_queue = queue.Queue
COLOR = B2,. 452)" 9
. ge_sub = -create_s MW, ptt
. image_calLbac 1
get_Logger info('\@33[1;32m%s\633[@ start
threading. Thread(target= -main, daemon=Tr ).start

Initialize the parameters required for background segmentation, call the image

callback function, and start the model recognition function.

image_callback:

image_caLLback I ros_image):
rgb_image = np.ndarray(shape image.height, ros_imag 3), dtype=np.uints,
buffer= image.data RG TT
r.image_ queue

- im ge_queue.put rgb_image

Image callback function, used to read data from the camera node and

enqueue it.

Main:

11



=== Page 12 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

main

with mp_selfie_segmentation.SeLfieSegmentation
model_seLection=1 selfie_segmentation:
bg_image None|
y - r.running

image = image_queue.get(bLock=T , timeout=1
q
queue.Empty:
1 -running:

results = selfie_segmentation.process(image
image.flags iteable

image cv CoLlor(image

# ir W =) CLO

F bg_image :

bg_image np.zeros(image.shape, dtype=np.uint8&

bg_image[:] = eLf.BG_COLOR
output_image = np.where(condition, image, bg_image

fps.update

resuLt_image =
cv2.imshow('Medi
Rey cv2.wa

f.fps.show_fps(output_image
> Selfie Segmentation’

ip result

cv2.destroyALLWindows
rcLpy.shutdown

Load the model from MediaPipe, input the image, and display the output image

using OpenCV.

3. 3D Object Detection

3.1 Program Logic

To get started, import the 3D Objectron module from MediaPipe, and subscribe

to the topic message to receive the real-time camera image.
Next, flip the image to ensure proper alignment for 3D object detection.

Finally, draw the 3D boundary frame on the image.

3.2 Operation Steps
1) Start the robot, and enter the robot system desktop using NoMachine.

12



=== Page 13 ===
ht IV\/< ee Mm Oo er Shenzhen Hiwonder Technology Co,Ltd
2) Click-on to start the command-line terminal.

3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the following command to enable the camera node:

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth _camera.launch.py

5) In anew command line terminal, enter the command and press Enter to
run the gameplay program:
cd ~/ros2_ws/src/example/example/mediapipe_example && python3

objectron.py

~/ros2_ws/src/example/example/mediapipe example && objectron.pyi

6) Toclose this game, press the "Esc" key in the image interface to exit the
camera image interface.
7) Press "Ctrl+C" in the command line terminal interface. If the closure fails,

please try again repeatedly.
3.3 Program Outcome

Once the game starts, the 3D frame will be drawn around the boundary of the
recognized object. The system can identify several objects, including a cup

(with handle), shoe, chair, and camera.

13



=== Page 14 ===
4 IVW/E) 1 Oo — t Shenzhen Hiwonder Technology Co,Ltd

MediaPipe Objectron

3.4 Program Analysis

The program file corresponding to this section of the course documentation is

located at:

lros2_ws/src/example/example/mediapipe_example/objectron.py

__— Function main Invoke node

objectron.py __init__ Initialization
> 3D recognition / Image callback

~—— Class ObjectronNode class | image callback function

\ main Class main function

3.4.1 Function

Main:

Used to initiate 3D detection node.

14



=== Page 15 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

3.4.2 Class

ObjectronNode:

iNode (Node
; ‘i name
reLpy.init
super().__init name
- running
mp_objectron mp.soLutions.objectron
mp_drawing mp.solutions.drawing_utils
fps = fps.FPS
image_queue queue. Queue(maxsize=2)
image_sub = create_subscription(I
.image_calLback
.get_Logger().info

init__( , name
rceLpy.init
super -__init__ (name)

mp.soLlutions.objectron

-mp ing = mp.soLlutions.drawing_utils

.fps = fps.FPS

-lmage_queue = queue.Queue(maxsize=2
se image_sub = self.create_subscription(Image

image_callback, 1

get_Logger info [ 2 \ [

threading. Thread(target= ; n, daemon=

Initialize the parameters required for 3D recognition, call the image callback

function, and start the model recognition function.

image_callback:

image_caLLback I ros_image
rgb_image = np.ndarray(shape S_image.height, ros_image.width, 3), dtype=np.uint8,
f image .data 5 f 7]

Image callback function, used to read data from the camera node and

enqueue it.

Main:

15



=== Page 16 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

f.mp_objectron.Objectron(static_image_mode=
max_num_objects=1

ec
_tracking_confidence \
model_name='Cup') objectron:
running:

image = .image_queue.get(block=-True, timeout=1
queue.Empty:
r -running:

- 1O Lmp ove p

# pass |

image.flags.writeable f e
results = objectron.process(image

# Draw t
image .f L
image = lage )2.COLOR_RGB2BGR

.detected objects:
Rs

é .d r
image, detected _obje andmarks_2d mp_objectron.BOX_CONNECTIONS

mp_drawing.draw_a age, detected object.rotation,
detected_ob translation

eLf.fps.update
ma

cv2.destroyALLWindows
rcLpy.shutdown

Read the model inside MediaPipe, input the image, draw the edges of the

objects after obtaining the output image, and display using OpenCV.

4. Face Detection

Face detection is realized by face detection model of MediaPipe.

This model offers a swift and efficient solution for detecting faces, equipped
with 6 facial landmarks to provide detailed information. It is designed to detect
multiple human faces with accuracy and speed. Based on BlazeFace, it has
been optimized for mobile GPU inference, ensuring a lightweight and

high-performance face detection experience on mobile devices.

4.1 Program Logic

To begin, import the human face detection model from MediaPipe and

subscribe to the relevant topic message to obtain the live camera feed.

Next, utilize OpenCV to flip the image and convert the color space for further

16



=== Page 17 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

processing.

Then, using the face detection model's minimum confidence threshold,
determine whether a human face has been successfully detected. If a human
face is recognized, proceed to collect the necessary information about each
detected face, including the bounding frame and the 6 key points (right eye, left

eye, nose tip, right ear, and left ear).

Finally, frame the human face and mark the 6 key points on each detected face

for visual clarity and further analysis.

4.2 Operation Steps

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the command to enable the camera node:

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth _camera.launch.py

5) In anew command line terminal, enter the command and press Enter to

run the gameplay program:

cd ~/ros2_ws/src/example/example/mediapipe_example && python3

17



=== Page 18 ===
4 IVW/E) | Oo — t Shenzhen Hiwonder Technology Co,Ltd

face_detect.py

6) If you need to close this game, you need to press the "Esc" key in the

image interface to exit the camera image interface.

7) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

4.3 Program Outcome

After the game starts, depth camera will start detecting human face, and

human face will framed on the live camera feed.

face_detection - «x

4.4 Program Analysis

The source code of this program is saved in

lros2_ws/src/example/example/mediapipe_example/face_detect.py

18



=== Page 19 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

— Function main Invoke node

face_d etect. py Human face _ Jn __laleltention

; iti ; | Ilback
Class FaceDetectionNode dei | image callback aa

\ main Class main function

4.4.1 Function

main
node = FaceDetectionNode
rcLpy.spin(node
KeyboardInterrupt:
node.destroy_node
rcLpy.shutdown

WN Fe

IaNNN
f

Used to initiate face detection node.

4.4.2 Class

FaceDetectionNode:

Renee 29 5 ee name
reLpy.init
super().__init__(name
- running
1. join(os.path.abspath(os.path.split(os.path.realpath(__file__

) model_path

vision.Face ) s s=base_options

-detector = vi 2 z pate_ m_options (options
fps .FPS(

_init__(name
ning = T |
mo _pa os.path.join(os.path.abspath(os.path.spLlit(os.path.reaLlpath(_ file__

base_opt = python.BaseOptions(model_asset_path=model_path
options = vision.FaceDetectorOptions(base_options=base options
vision.FaceDetector.create_from_options(options

imag E Q e(maxsize=2
. imag ib _subscription(Image
.image_caLLback
-get_Lo
threading.T

Initialize the parameters required for face recognition, call the image callback

function, and start the model recognition function.

image_callback:

19



=== Page 20 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

image_caLLback I ros_image):
rgb_image = np.ndarray(shape=(ros_image.height, ros_image.width, 3), dtype=np.uinté8,

r=ros_image.data # [RR

Image callback function, used to read data from the camera node and

enqueue it.

Main:

f.running:

image = . image_queue.get(bLock=Tr timeout=1
e queue.Empty:
f S running:

image = cv2.flip(image
mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data-image
detection_result = .detector.detect(mp_image

annotated _image = visualize(image, detection_result)
fps .update
resuLlt_i r.fps.show_fps(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR

Read the model from MediaPipe, input the image, and after obtaining the
output image, use OpenCV to draw the facial keypoints and display the

feedback image.

5. 3D Face Detection

In this program, MediaPipe Face Mesh is utilized to detect human face within

the camera image.

MediaPipe Face Mesh is a powerful model capable of estimating 468 3D facial
features, even when deployed on a mobile device. It employs machine
learning to infer the 3D face contour accurately. Additionally, this model
ensures real-time detection by utilizing a lightweight model architecture and

GPU acceleration.

20



=== Page 21 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Furthermore, this model is integrated with a face conversion module that
compensates for any differences between face landmark estimation and AR
(Augmented Reality) applications. It establishes a metric 3D space and utilizes
the facial landmark screen positions to estimate facial conversion within this
space. The facial conversion data consists of common 3D primitives, including

facial gesture conversion matrices and triangle facial mesh information.

5.1 Program Logic

Firstly, you need to learn that machine learning pipeline is composed of two
real-time deep neural network models. The system consists of two
components: a face detector that processes the entire image and calculates
the locations of faces, and a 3D face landmark model that uses these locations

to predict an approximate 3D surface through regression.

To achieve 3D facial landmarks, we utilize transfer learning and train a network
with multiple objectives: predicting 3D landmark coordinates on synthetic
rendered data and 2D semantic contours on annotated real-world data
simultaneously. This approach yields plausible 3D landmark predictions not

only based on synthetic data but also on real-world data.

The 3D landmark network takes cropped video frames as input without
requiring additional depth input. The model outputs the location of a 3D point

and the probability that a face appears in the input and is properly aligned.

Once the face mesh model is imported, real-time images can be obtained from

the camera by subscribing to topic messages.

Next, image preprocessing techniques like flipping the image and converting
the color space are applied. The face detection model's minimum confidence is

then used to determine whether the face has been successfully detected.

21



=== Page 22 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Finally, the detected face on the screen is projected into a 3D grid for

visualization and display.

5.2 Operation Steps

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the command to enable the camera node:

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth _camera.launch.py

5) Enter the command in a new command-line terminal and press Enter to

run the game program:

cd ~/ros2_ws/src/example/example/mediapipe_example && python3

face_mesh.py
~/ros2_ws/src/example/example/mediapipe example && face mesh.py

6) If you need to close this game, you need to press the "Esc" key in the

image interface to exit the camera image interface.

7) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

22



=== Page 23 ===
| iIVWE) mM Oo [Shenzhen Hiwonder Technology Co,Ltd

5.3 Program Outcome

After starting the game, when the depth camera detects a face, it will outline

the face in the feedback image.

face_landmarker - x

5.4 Program Analysis

The source code of this program is located in

lros2_ws/src/example/example/mediapipe_example/face_mesh.py

_— Function \ main Invoke node

oe

face_detect.py Hunan face —init_ializatin
wa recognition f Image callback

/
~~ Class \_ FaceDetectionNode “node | image callback function

\_ main Class main function
5.4.1 Function

Main:

23



=== Page 24 ===
main
node = FaceMeshNode

te

rcLpy.spin(node

spt KeyboardInterrupt:

node.destroy_node

rcLpy.shutdown
nt('shutdown'

nt('shut in Fini

Used to initiate the 3D face detection node.
5.4.2 Class

FaceMeshNode:

FaceMeshNode (Node):
__init__(s fF, name
reLpy.init
super().__init__(name
‘ running t
ath = os.path.join(os.path.abspath(os.path.split(os.path.realpath(__file__))[@]
ce_lLa marker_ with_bLlends 2s.task'
base_options = ython.BaseOptions(model_asset_path=model_path
options = vision.FaceLandmarkerOptions (base options=base_ options,
output_face_blLendshapes=Tru

reLpy.init
super().__init name
elf.running =
model_path os.path. join(os.path.abspath(os.path.spLlit(os.path.realpath(_ file __
; 2 Landmarker with_bLlendsh e

base_options python.BaseOptions(model_asset_path=model_path

options = vision.FaceLandmarkerOptions(base_options=base_ options
output_face_bLendshapes= 2,
output_facial_transformation_matrixes=T
num_faces=1

-detector = vision.FaceLandmarker.create_from_options (options

self.fps = fps.FPS

image_queue queue. Queue (maxsize=2
.image_sub = self.create_subscription(Image
.image_callback, 1
Lf .get_Logger() .info
threading. Thread(target

Initialize the parameters required for 3D face detection, call the image callback

function, and start the model recognition function.

image_callback:

image_caLLback ros_image
rgb_image = np.ndarray(shape=(ros_image.height, ros_image.width, 3), dtype=np.uint8,
buffer=ros_image.data # RSS RGB BH

ca

# 6H &
r.image_queue.put(rgb_image

The image callback function is used to retrieve data from the camera node and

encapsulate it into a queue.

24



=== Page 25 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd
Main:

running:
image = f .image_queue.get(block=t timeout=1
queue.Empty:
f f.running:

image = cv2.flip(image, 1)

mp_image = m ge(image_format=mp.ImageFormat.SRGB, data=image
detection_re = detector .detect(mp_image

annotated

Lf.fps.show_fps(cv2.cvtCoLlor(annotated_image, cv2.COLOR_RGB2BGR

r key = 27: # HqmMBescisByw

yA
rcLpy.shutdown

Load the model from MediaPipe, input the image, and after obtaining the
output image, use OpenCV to draw the facial keypoints and display the

returned footage.

6. Hand Key Point Detection

MediaPipe's hand detection model is employed to showcase the key points of

the hand and the connecting lines of these key points on the live camera feed.

MediaPipe Hands is an advanced hand and finger detection model that
delivers high-fidelity results. Through the power of machine learning (ML), it

accurately infers 21 3D landmarks of the hand from a single frame.

6.1 Program Logic

Firstly, it's important to understand that MediaPipe's palm detection model
employs a machine learning pipeline consisting of multiple models (including a
linear model). This model processes the entire image and returns an oriented
hand bounding box. On the other hand, the hand landmark model operates on
cropped image regions defined by the palm detectors and provides

high-fidelity 3D hand keypoints.

25



=== Page 26 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

To begin, after importing the palm detection model, we subscribe to the topic

message to obtain real-time camera images.

Next, various image pre-processing steps, such as flipping the image and
converting the color space, are applied. These steps significantly reduce the

need for data augmentation for the hand landmark model.

Furthermore, our pipeline allows for generating crops based on hand
landmarks identified in the previous frame. The palm detection is invoked only
when the landmark model is unable to recognize the presence of the hand

accurately.

Afterward, by comparing the minimum confidence level of the hand detection

model, we determine whether the palm has been successfully detected.

Lastly, the hand keypoints are detected and drawn on the camera image to

visualize the detected hand in real-time.

6.2 Operation Steps

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on to open the command-line terminal.

3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the command to enable the camera node:

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth_camera.launch.py



=== Page 27 ===
4 IVW/E) | Oo — t Shenzhen Hiwonder Technology Co,Ltd

5) Enter the command in a new command-line terminal and press Enter to

run the game program:

cd ~/ros2_ws/src/example/example/mediapipe_example && python3

hand.py

src/example/example/mediapipe example &&

6) If you need to close this game, you need to press the "Esc" key in the

image interface to exit the camera image interface.

7) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

6.3 Program Outcome

Once the game starts, the depth camera will begin detecting the hand and
display the hand key points on the camera image, with the key points

connected.

27



=== Page 28 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

6.4 Program Analysis

_— Function main _ Invoke node

h a Nn e| e py ; _init_ Initialization

Image callback

cs oe Class HandNode image _callback function
Class main

main function

The program file corresponding to this section of the course documentation is

located at:

/ros2_ws/src/example/example/mediapipe_example/hand.py

6.4.1 Function
Main:

main
node = HandNode('|!

ann
UAH

rcLpy.spin(node
KeyboardiInterrupt:
node.destroy_node
rcLpy.shutdown
i hutdown:

6
6

Yu NNN OO
PrP Ow Ow

fi WN

Used to initiate the 3D face detection node.

6.4.2 Class

HandNode:

ath.split(os.path.realpath file

n ) ) ) path=model_pat

J od
vision.HandLé ero tions=base_options

ctor vision.HandLandmarker.create_from_options(options

= queue. Queue(maxsize=
create_subscription(Image, '
lage_calLback, 1
-get_Logger info('\

28



=== Page 29 ===
reLpy.init
super().__init__(name
running =
model_path os.path.join(os.path.abspath(os.path.split(os.path.realpath(_ file __
model/hand R'
base_options python.BaseOptions(model_asset_path=model_path
options = vision.HandLandmarkerOptions(base_options=base_options,
num_hands=
detector = vision.HandLandmarker.create_from_options (options
-fps = fps.FPS
image_queue queue . Queue (maxsize=2
€ - Lmage_sub Lf .create_subscription(Image,
.image_callback, 1
-get_Logger().info(' 3[1;32m 633[Om'
threading. Thread(target= -main, daemon=

image_callback(self, ros_image):
rgb_image np.ndarray(shape=(ros_image.height, ros_image.width
fer=ros_image.data) iE: RGB Bf

3 dtype=np.uint8,
Lf.image
# MRM
f.image_q

age_queue.put(rgb_image

Initialize the parameters required for hand keypoint detection, call the image

callback function, and start the model recognition function.

image_callback:

NOK

image_caLLback I ros_image
rgb_image = np.ndarray(shape=(ros_image.height, ros_image.. 3), dtype=np.uints,
buffer=r image.data # £

NoN

F.image_queue.full
# MRF , BF
5 . image_queue.get
# BHBRADA

.image_queue.put(rgb_image

Ww Ww

www

HRONrROUDAN

us

The image callback function is used to read data from the camera node and

enqueue it.

Main:

running:

image f.image_queue.get(bLock=} , timeout=1
queue.Empty:
F.running:

image cv2.flip(image, 1

mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image

detection_result Lf .detector.detect(mp_image

annotated_image = draw_face_Landmarks_on_image(image, detection_result
fps.update

resuLlt_image : 9s .show_fps(cv2.cvtCoLlor(annotated_image, cv2.COLOR_RGB2BGR

cv2.imshow('face ’ er', result_image

key = cv2.waitKe

key = ord('q r += # qm sAesci

cv2.destroyALLWindows
rcLpy.shutdown

Read the model from MediaPipe, input the image, and after obtaining the

output image, use OpenCV to draw the key points of the hand and display the

29



=== Page 30 ===
ht iIiweaonder Shenzhen Hiwonder Technology Co,Ltd

feedback image.

7. Body Key Points Detection

The MediaPipe body detection model is utilized to detect key points on the

human body, which are then displayed on the live camera feed. This

implementation incorporates MediaPipe Pose, a high-fidelity posture tracking

model that leverages BlazePose to infer 33 3D key points. Additionally, this

approach offers support for the ML Kit Pose Detection API.

0
1
2
3
4
5
6
7
8

ah oh at wh lt od: ot
Oumntrwon-- oOo wo

7.1 Program Logic

Firstly, import body detection model.

nose
left_eye_inner
left_eye

. left_eye_outer
. right_eye_inner
. right_eye

right_eye_outer
left_ear
right_ear
mouth_left

. mouth_right

. left_shoulder
. fight_shoulder
. left_elbow

right_elbow
left_wrist
right_wrist

. left_pinky

. right_pinky

. left_index

. right_index

. left_thumb

. right_thumb

. left_hip

. right_hip

. left_knee

. right_knee

. left_ankle

. right_ankle

. left_heel

. right_heel

. left_foot_index
. right_foot_index

Subsequently, flip over the image and convert the color space of the image.

Check whether the human body is successfully detected based on the

minimum confidence of the body detection model.

Next, define the tracked posture by comparing the minimum tracking

confidence. If the confidence does not meet the minimum threshold, perform

30



=== Page 31 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

automatic human detection on the next input image.

In the pipeline, a detector is employed to initially localize the region of interest
(ROI) corresponding to a person's pose within a frame. Subsequently, a
tracker utilizes the cropped ROI frame as input to predict pose landmarks and

segmentation masks within the ROI.

For video applications, the detector is invoked selectively, only when
necessary. Specifically, it is used for the first frame or when the tracker fails to
recognize the body pose in the preceding frame. In all other frames, the
pipeline derives ROIs based on the pose landmarks detected in the previous

frame.

After MediaPipe body detection model is imported, access the live camera

feed through subscribing the related topic message.

Lastly, draw the key points representing the human body.

7.2 Operation Steps

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the command to enable the camera node:

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth_camera.launch.py



=== Page 32 ===
| iIVWE) mM Oo [Shenzhen Hiwonder Technology Co,Ltd

5) Enter the command in a new command-line terminal and press Enter to

run the game program:

cd ~/ros2_ws/src/example/example/mediapipe_example && python3

pose.py

6) If you need to close this game, you need to press the "Esc" key in the

image interface to exit the camera image interface.

7) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

7.3 Program Outcome

After the game starts, depth camera will begin detecting human pose, and

body key points can be displayed and connected on the live camera feed.

pose_landmarker —- Xx

32



=== Page 33 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

7.4 Program Analysis

The program file is saved in:

Function main _ Invoke node
pose. py _init_ Initialization
a Face mesh /— Image callback
— Class - PoseNode class '  image_callback fynction

\ main Class main function

/ros2_ws/src/example/example/mediapipe_example/pose.py

7.4.1 Function

Main:

main
node = PoseNode

rcLpy.spin(node
KeyboardInterrupt:

node.destroy_node
rcLpy.shutdown

!

Used to initiate the 3D face detection node.

7.4.2 Class

PoseNode:

33



=== Page 34 ===
iat
oon)

reLpy.init
super init name
-running = True
model_path = os.path.join(os.path.abspath(os.path.spLlit(os.path.realpath(__file__))[6]
Landmarker. k'
base_options = python.BaseOptions(model_asset_path=model_path
options vision.PoseLandmarkerOptions
base_options=base_options,
output_segmentation_masks=Trt
.detector = vision.PoseLandmarker.create_from_options(options
fps = fps.FPS
.image_queue = queue.Queue(maxsize=
elf.image_sub = f.create_subscription(Image,
ge_callback, 1

NNN
NRO

WNNNNNNN
NOU BO

oUM

Fo init e , name
reLpy.init
super init__(name
F.running =
model_path os.path.join(os.path.abspath(os.path.split(os.path.reaLlpath
Landmarker k'
base_options python.BaseOptions(model_asset_path=model_path
options = vision.PoseLandmarkerOptions
base_options=base_options
output_segmentation_masks=Trt
-detector = vision.PoseLandmarker.create_from_options(options
-fps Fps.FPS
image_queue = queue. Queue(maxsize=2
elf.image_sub = F.create_subscription(Image
.image_callLback, 1
F.get_Logger info
eading.Thread(target=

WNNNNNNN

ooo

/rgb/imag

w wo
Ne

Initialize the parameters required for limb detection, call the image callback

function, and start the model recognition process.

image_callback:

image_callLback ros_image
rgb_image = np.ndarray ape=(ros_image.height, ros_i u 3), dtype=np.uints,
t f =ros_image.data

Image callback function, used to read data from the camera node and

enqueue it.

Main:

main :
I running:

image = f.image_queue.get(block=True, timeout=1
queue .Empty:
i seLf.running:

image = cv2.flip(image, 1
mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image
detection_resuLlt seLf.detector.detect(mp_image)
annotated image = draw_pose_Landmarks_on_image(image, detection_result
; .fps.update
resuLt_image = F.fps.show_fps(cv2.cvtColor(annotated_ image, cv2.COLOR_RGB2BGR
cv2.imshow(' marker', resuLlt_image
= cv2.waitKey(1
key = i('q' r key 27: # fRqmMBesci® wu

cv2.destroyALLWindows
rclpy.shutdown()l

34



=== Page 35 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Read the model inside MediaPipe, input the image, and after obtaining the
output image, use OpenCV to draw facial keypoints and display the feedback

image.

8. Fingertip Trajectory Recognition

Identify hand joints using MediaPipe's hand detection model. Once a specific
gesture is recognized, the robot will initiate fingertip locking on the screen,

track the fingertips, and generate their movement trajectory.
8.1 Program Logic

First, invoke the MediaPipe hand detection model to capture the camera image.
Next, flip and process the image to extract hand information. Utilizing the
connection lines between key points of the hand, calculate the finger angles to
determine the gesture. Upon recognition of a specific gesture, the robot will
proceed to identify and lock the fingertips on the screen, simultaneously

tracing the movement trajectory of the fingertips on the display.

8.2 Operation Steps

Note: the input command should be case sensitive, and the keyword can be

complemented by “Tab” key.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

35



=== Page 36 ===
e
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd
stop start_app_node.serviceff

4) Execute the command to enable the camera node:

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth_camera.launch.py

5) Enter the command in a new command-line terminal and press Enter to

run the game program:

cd ~/ros2_ws/src/example/example/mediapipe_example && python3

hand_gesture.py

~/ros2_ws/src/example/example/mediapipe example &&

6) The program will enable the camera automatically. The detailed

recognition process can be found in ‘8.3 Program Outcome’.

7) If you need to close this gameplay, you need to press the "Esc" key in the

image interface to exit the camera image interface.

8) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

8.3 Program Outcome

After starting the activity, position your hand within the camera's field of view.
Once the hand is detected, the system will highlight the keypoints on the

returned image.

If the system recognizes the gesture '1' (extending the index finger), the robot
will instantly enter recording mode, dynamically tracking and displaying the
fingertip's movement trajectory in real-time on the screen. This feature enables

users to freely draw in virtual space using simple gestures.

36



=== Page 37 ===
4 IVW/E) | Oo — t Shenzhen Hiwonder Technology Co,Ltd

On the other hand, if the system detects the gesture '5' (with the palm open,
resembling the shape of the number '5’), it will activate a clear command. In
this case, the recorded trajectory of the fingertip will be erased immediately,

and the display will revert to a state without any visible trajectory.

8.4 Program Analysis

The program file is saved in

lros2_ws/src/example/example/mediapipe_example/hand_gesture.py

main Run the main function of this file

[ get_hand landmarks Convert pixel coordinate
I
_—  Funetion _hand_angle Calculate the flex angle of the finger

\ h_gesture Determine gesture

|
\ ints Draw the knuck\
hand_gesture.py , draw_points Draw the knuckles

Enumeration class used to determine
State the current program state

| _init___ Initialization
~ Class < ——
\ HandGestureNode 2iR5I53s / main Gesture recognition
- \_ Image callback
\ image_callback function

Note: Prior to making any alterations to the program, ensure to create a backup of
the original factory program. Modify it only after creating the backup. Directly
editing the source code file is prohibited to prevent inadvertent parameter

modifications that could render the robot dysfunctional and irreparable!

Based on the game's impact, the process logic of this game is organized as

depicted in the figure below:

37



=== Page 38 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Image acquiring Image preprocessing
color space conversion

esture trajectory drawing Gesture key
(live camera feed display point retrieving

As depicted in the image above, the purpose of this game is to capture an
image using the camera, preprocess it by converting its color space for easier
identification, extract feature points corresponding to hand gestures from the
converted image, and determine different gestures (based on angles) through
logical analysis of key feature points. Finally, the trajectory of the recognized

gesture is drawn on the display screen.

The program's logic flowchart extracted from the program files is illustrated in

the figure below.

38



=== Page 39 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Object definition for drawing
images (self.drawing

4. Initialization Function Definition of key feature detectors for
identifying and detecting hands

Image Color Space Conversion

2. *Recognition Processing Save Key Points Recognition Result

Function
Drawing Object Parameter Settings
Logical Categorization of Finger Types
Hand Feature Detection

ingertips Feature Detection, Motion
Trajectory Drawing, Trajectory Cleaning

From the above diagram, it can be seen that the program's logical flow is
mainly divided into initialization functions and recognition processing
functions (with Xinghao being relatively important). The following document
content will be written according to the program logic flow chart mentioned

above.

8.4.1 Function
Main:
= ‘HandGestureNode(

rcLpy.spin(node
t KeyboardInterrupt:

node.destroy_node
rcLpy.shut

The main function is used to start the fingertip trajectory recognition node.

get_hand_landmarks:

39



=== Page 40 ===
get_hand_Landmarks(img, Landmarks

nun

4$ Landmarks \. medipipe gy [9 —th.#
:param Landmarks: iq@—4#

:return:

noun

h, w, _ = img.shape

Landmarks [CLm.x x Lm.y * Fo Landmarks |
np.array( Landmarks

Convert the normalized data from MediaPipe into pixel coordinates.

hand_angle:
f hand_angle Landmarks):

‘param Landmar
return: Sh Fi
angle_Llist = []
# thumb K#H
angle_ = vector_2d_angle(lLandmarks[3] - Landmarks[4], Landmarks[@] - Landmarks[2]
angle_List.append(angle_
# index ##%
angle_ = vector_2d_angle(Landmarks [6] Landmarks[6], Landmarks[7] Landmarks[8]
angle_List append angLle_
# middle me
angle_ = vector_2d_angle(Landmarks[@] - Landmarks[{1@], Landmarks[11] - Landmarks[12
angle_List.append(angle
# ring 3
angle_ = ctor_2d_angle(Landmarks[@] - Landmarks[14], Landmarks[15] - Landmarks
angle_List. gooane angLe_
# pink 4
angLle_ = vector 2d_angLe(Landmarks[@] - Landmarks[18], Landmarks[19] - Landmarks[26]
angLle_List. append ang Le
angle_list = [ a) yr a in angle_lList
rr angle_ list

After extracting the hand feature points into the 'results' variable, it is
necessary to logically process these points. By evaluating the angular
relationship between the feature points, specific finger types (thumb, index
finger) can be identified. The 'hand_angle' function accepts the landmark
feature point set (results) as input, and subsequently employs the
‘vector_2d_angle' function to compute the angles between the corresponding
feature points. The feature points corresponding to the elements of the

landmark set are depicted in the figure below:

40



=== Page 41 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Taking the thumb's angle calculation as an example: the vector_2d_angle
function is used to calculate the angle between joint points. landmarks[3],
landmarks[4], landmarks[0], and landmarks[2] correspond to feature points 3,
4, 0, and 2 in the hand feature extraction diagram. By calculating the angles
of these joint points, the thumb's posture characteristics can be determined.

Similarly, the processing logic for the other finger joints is analogous.

To ensure the accuracy of recognition, the parameters and basic logic
(addition and subtraction of angle calculations) in the hand_angle function

should remain at their default settings.

h_gesture:

41



=== Page 42 ===
fF h_gesture(angle_Lis

thr_angle

gesture_str one
(angle_List[@] > thr_angle_thumb) and (angle_list[1] > thr_angle) and (angle_list[2] >
thr_angle) and (
angle_List[3] > thr_angle) i (angle_list[4] > thr_angle)
gesture_str = Fist
(angle_List[@] < thr_angle_s) i (angle_list[1] < thr_angle_s) and (angle_list[2] >
thr_angle) (
] > thr_angle) i (angle_list[4] > thr_angle)
gestu st nd_heart"
(ang i @] < thr_angle_s) (angle_List[1] < thr_angle_s) and (angle_list[2] >
thr_angle)
> thr_angle) and (angle_list[4] < thr_angle_s)
LCO-NLCO-NL
if (angle 6] < thr_angle_s) i (angle_List[1] > thr_angle) (angle_list[2] >
thr_angle)
> thr_angle) i (angle_list[4] > thr_angle):
1 hance?

> 5) and (angle_list[1] < thr_angle_s) and (angle_list[2] > thr_angle) and (
3] > thr_angle) and (angle_list[4] > thr_angle)
gesture_str = "one

elif (angle_list[@] > thr_angle_thumb) and (angle_list[1] < thr_angle_s) and (angle_list[2] <
thr_angle_s) ¢
angle_list[3] > thr_angle) i (angle_list[4] > thr_angle)
esture_str = "two

le_list[@] > thr_angle_thumb) i (angle_list[1] < thr_angle_s) i (angle_list[2] <

< thr_angle_s) and (angle_list[4] > thr_angle)

if (ang g thr_angle_thumb) and (angle_List[1] > thr_angle) and (angle_lList[2] <
thr_angle_s)
angle_list[3] < thr_angle_s) and (angle_list[4] < thr_angle_s)
gesture_s OK"
if (angle ¢ > thr_angle_thumb) i (angle_list[1] < thr_angle_s) d (angle_list[2] <
hr_angle_s)
< thr_angle_s) and (angle_list[4] < thr_angle_s):

< thr_angle_s) and (angle_Llist[1] < thr_angle_s) and (angle_list[2] <

]
< thr_angle_s) and (angle_list[4] < thr_angle_s)

< thr_angle_s) (angle_List[1] > thr_angle) (angle_list[2] >

> thr_angle) and (angle_list[4] < thr_angle_s):

After identifying the different finger types of the hand and determining their
positions on the image, logical recognition processing of various gestures can

be performed by implementing the 'h_gesture’ function.

In the 'h_gesture' function depicted above, the parameters 'thr_angle’,
‘thr_angle_thenum’, and 'thr_angle_s' represent the angle threshold values
for corresponding gesture logic points. These values have been empirically
tested to ensure stable recognition effects. It is not recommended to alter
them unless the logic processing effect is unsatisfactory, in which case
adjustments within a range of +5 values are sufficient. The
‘angle_list[0,1,2,3,4]' corresponds to the five finger types associated with the

palm.

Here's an example using the gesture "one":

42



=== Page 43 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

elif (angle list[0] > 5) and (angle _list[1] < thr_angle_s) and (angle list[2] > thr_angle) and (

angle list[3] > thr_angle) and (angle list[4] > thr_angle):
gesture str = "one"

The code presented represents the logical angle evaluation of the fingers for
the "one" gesture. 'angle_list[0]>5' checks whether the angle value of the
thumb joint feature point in the image is greater than 5.
‘angle_list[1]<thr_angle_s' checks if the angle feature of the index finger joint
feature point is less than the predetermined value 'thr_angle_s'. Similarly,
‘angle_list[2]<thr_angle’ verifies if the angle feature of the middle finger feature
point is less than the predetermined value 'thr_angle’. The logical processing
for the other two fingers, ‘angle_list[3]' and 'angle_list[4]', follows a similar
method. When the above conditions are met, the current gesture feature is
recognized as "one", and the same principle applies to recognizing other

gesture features.

Different gesture recognitions involve distinct logical processing, but the
overall logical framework remains similar. For recognizing other gesture

features, refer to the previous paragraph.

draw_points:

draw_points(img, points, thickness=4, color=(2
points = np.arra -astype ype=np.int64
f points

t, p enumerate(po
Ltt E point

cv2.line(img, p, points[i + 1], color, thickness

Draw the currently recognized hand shape and each joint point.
8.4.2 Class

State:

> StateCenum.Enum):
START = 1

TRACKI
RUNNING = 3

An enumeration class used to set the current state of the program.

43



=== Page 44 ===
HandGestureNode:

HandGestureNode(Node
init el name):
reLpy.init
Super init name
running = P
elf .drawing = mp.soLlutions.drawing_utils

hand_detector = mp.solutions.hands.Hands
static. _image_mode= y
nds=

min_tracking_ confidence
min_detection_confidence-

f.fps = fps.FPS # fpsit Bs
state State .NULL
-points = []
count = 6

f.image_queue = queue.Queue(maxsize=2
.image_sub = -create_su iption(Image, '/dept

. image callback 1)
get_Logger info('\633[1;32 \933[6m'
ciresdtae Thread(target= F.main ‘daemon=

The HandGestureNode is a fingertip trajectory recognition node that contains
three functions: an initialization function, a main function, and an image

callback function.

Init:

( 2 name
running = Tr
‘drawing = = mp.soLutions.drawing_utils

-hand_detector = mp.soLutions.hands.Hands
static_image_mode= se,

max_num_hands=1

min_tracking_confidence=
min_detection_confidence=-

f.fps = fps.FPS # fps BH
*.State = State.NULL
points = |}
F.count = @
f.image queue = queue. Queue(maxsize=2)
.image_sub = Lf .create_subscription(Image, '/depth_cam/rgb/
- image_ callbac 4 BS
Lf.get_Logger().info(' Et;
threading. Thread(target=seLf.m

Initialize each component needed and call the camera node.

9. Posture Control

The human posture estimation model, trained using the MediaPipe machine
learning framework, detects the human body feature in the captured image,
identifies relevant joint positions, and subsequently recognizes a variety of

sequential actions. This process enables direct control of the robot through

44



=== Page 45 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

somatosensory input.

Viewed from the perspective of the robot, the following actions correspond to

specific movements:

@ lf the user lifts their left arm, the robot will move a certain distance to the

right.

@ lf the user lifts their right arm, the robot will move a certain distance to the

left.
@ lf the user lifts their left leg, the robot will move forward a certain distance.

e lf the user lifts their right leg, the robot will move backward a certain

distance.

9.1 Program Logic

First, import MediaPipe's human pose estimation model and subscribe to topic

messages to obtain real-time footage from the camera.

MediaPipe is an open-source multimedia machine learning model application
framework that runs cross-platform on mobile devices, workstations, and
servers. It supports mobile GPU acceleration and inference engines such as

TensorFlow and TF Lite.

Next, utilize the built model to detect key points of the human body in the
screen. Connect these key points to display the human body and determine

the human body posture.

Finally, if a specific action is detected in the human body posture, the robot will

respond accordingly.

45



=== Page 46 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

9.2 Operation Steps

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Execute the command to run the game program:

ros2 launch example body_control.launch.py

Launch example body control. launch.py

5) If you need to close this game, you need to press the "Esc" key in the

image interface to exit the camera image interface.

6) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.
9.3 Program Outcome

Once the game is initiated, stand within the camera's field of view. When a
person is detected, the screen will display key points of the body and lines

connecting them.

From the perspective of the robot, lifting the left arm will cause the robot to turn
left; lifting the right arm will make the robot turn right; lifting the left leg will
make the robot move forward a certain distance; lifting the right leg will make

the robot move backward a certain distance.

46



=== Page 47 ===
| iIVWE) mM Oo [Shenzhen Hiwonder Technology Co,Ltd

9.4 Program Analysis

The program file is saved in
ros2_ws/src/example/example/body_control/include/body_control.py

get joint landmarks Convert pixel coordinate
rr Function NN joint distance Calculate the distance to each recognition point
y

\ main Run the main function of this file

/ __init__ Initialization
get node state Node status

Close program
shutdown callback function

body control.py

iy
ra

k Image callback
function

image _callbac

Action function
after recognition

~~ Class \ BodyControlNode

. move

. buzzer warn Buzzer alarm

Body recognition
function

\ image_proc

. Class main
\ main function

Note: Prior to making any alterations to the program, ensure to create a backup of
the original factory program. Modify it only after creating the backup. Directly
editing the source code file is prohibited to prevent inadvertent parameter

modifications that could render the robot dysfunctional and irreparable!

The game process logic is outlined below:

47



=== Page 48 ===
ht IW/EM Oo er Shenzhen Hiwonder Technology Co,Ltd

Image retrieving (camera

Liftieft arm

Execute
Lift left leg action

ar moves

-_ Robot will execute corresponding action HaGRW GTM
based on human's pose .
Car turns left Car turns right

a. Capture an image through the camera.

b. After performing a demonstration action, the car will execute the

corresponding action.

c. From the car's perspective, lifting the left arm will cause the car to turn left;
lifting the right arm will cause the car to turn right in a circle; lifting the left
leg will make the car move forward a certain distance; lifting the right leg

will make the car move backward a certain distance.

The program's logic flowchart, obtained from the program files, is presented

below:

48



=== Page 49 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Definition of Drawing Tool Object
Definition for Human Body Detection

1. Definition for Basic Parameters
__ init__(self, name)

Save Feature Point Output Result
Calculation of Key Point
Distance
2. Recognition Logic Processing
image_proc
ge_P Save action result
Result Output

a. Initialization function (init(self.name)) defines relevant parameters,

including:
@ Definition of the image tool (self.drawing) object.
@ Points used to draw recognized features.
@ Definition of the limb detection object (self.body detector).

b. Identified feature points' output results undergo logical processing for

recognition.
c. Actions are determined and stored based on key point distance conditions.

d. Finally, the output results are generated, and the car executes

corresponding actions.
9.4.1 Function
Main:

main
node = BodyControLNode

rcLpy.spin(node)
node.destroy_node|

49



=== Page 50 ===
Used to start the body sensation control node.

get_joint_landmarks:

get_joint_Landmarks(img, Landmarks):

naw

+$ Landmarks \ medipipe i !9 — tk = f fm (Convert Landmarks from medipipe's normalized
output to pixel coordinates)
tparam img: RAH A9 AA Cpicture corresponding to pixel coordinate)

:param Landmarks: J = & (normalized keypoint)
treturn:

non

h, w, _ = img.shape

Landmarks = lm.x x w, Lma.y x h ) T Landmarks ]
r np.array(Landmarks

Used to convert the recognized information into pixel coordinates.

joint_distance:

joint_distance( Landmarks
distance_List

f

d1 = Landmarks[LEFT_HIP] Landmarks [LEFT_SHOULDER]
d2 = Landmarks[LEFT_HIP] - Landmarks[LEFT_WRIST]
dist = di[@]*x*2 + d1[1] 2

dis2 = d2[@]*x*x2 + d2[1]xx2
distance_List.append(round(dis1/dis2, 1)

Landmarks [RIGHT_HIP] Landmarks [RIGHT SHOULDER]
Landmarks[RIGHT_HIP] - Landmarks[RIGHT_WRIST ]
= d1[@]xx2 + d1[1]xx2Z
d2[@]*xx2 + d2[1]
distance_List.append(round(dis1/dis2, 1

di = Landmarks[LEFT_HIP] - Landmarks[LEFT_ANKLE]
d2 = Landmarks[LEFT_ANKLE] - Landmarks[LEFT_KNEE]
disi = d1[@]**x2 + d1[1]xx*2

dis2 = d2[@]*x*2 + d2[1]x*x2

distance_List.append und(disi/dis2, 14

WUNFOUDOYVAHRBWONKFODOaA UNH

NOU &

d1 Landmarks[RIGHT_HIP] - Landmarks[RIGHT_ANKLE]
d2 Landmarks[RIGHT_ANKLE] - Landmarks[RIGHT_KNEE]
dis1 = di1[@]*x*2 + d1[1]xx2

dis2 = d2[@]x*2 + d2[1]*x2
distance_List.append(round(dis1/dis2, 1)

ONF OW oO

distance_List

Used to calculate the distance between each joint point based on pixel

coordinates.

58



=== Page 51 ===
9.4.2 Class

BodyControLNode(Node):
jef __init_ ( F, name):
rclpy. init()
super().__init__(name, alLow_undecLlared_parameters=-True, automaticaLlly_decLare_parameters_
from_overrides=True)
F.name = name
F.drawing = mp.s ions .drawing_utils
F.body_detector = pose.Pose(
sStatic_image_mode
min_tracking_confid
min -detection_ contiden
Lf.running =
Lf. fps7:= fps .FPS()
etenal: signal(signal. SIGINT, shutdown)

f.move_finish =
stop_flag = False
Left_hand_count = []
right_hand_count = []
elf.left_leg_ count = []
F.right_lLeg count = []

F.detect status =

This class is the body control node.

Init:

, Name):

super ( it__(name, allow_undeclared_parameters=True, automatically_declare_parameters_
from_overrides €
Lf. name = name
-drawing = mp.solutions.drawing_utils
.body_detector = mp_pose.Pose(
NStAELS image_ mode=! Lse,
min _tracking_ confidence= ;
min_detection_confidence= )
running = Tr
fps = fps.FPS() # fps7S
a jenal(csion TGTN

Initialize the parameters required for body control, read the image callback
node from the camera, initialize nodes such as servos, chassis, buzzers,

motors, etc., and finally start the main function within the class.4
get_node_state:

F get_node_state(self, request, response):
response. success = Truel

rn response

Set the initialization state of the current node.

shutdown:

shutdown(self, signum, frame):

Lf.running =

Program exit callback function used to terminate recognition.

image_callback:

51



=== Page 52 ===
image_callback(seLlf, ros_image
rgb_image = np.ndarray(shape=(ros_image.height, ros_image.width, 3), dtype=np.uint8,
image .data $B ENA Bet

_queue. full ; i

f.image_queue.put rgb_image

Image node callback function used to process images and enqueue them.

Move:

joint

ti eep
elf .stop_flag
elf move fini

Movement strategy function that moves the vehicle according to the

recognized limb actions.

buzzer_warn:

buzzer_warn(seLf):

msg = BuzzerState

msg.freq = 1906

msg.on_time =

msg.off_time

msg.repeat = 1
F.buzzer_pub.pubLish(msg)

Buzzer control function used for buzzer alarms.

image_proc:

image_proc(se image):
image_flip cv2.flip(cv2.cvtCoLlor(image, cv2.COLOR_RGB2BGR
results = s F.body_detector.process(image
if results is not None and results.pose_Landmarks
f F.move_finish:
twist = Twist

Landmarks = get_joint_Landmarks(image, results.pose_lLandmarks.Landmark
distance_List = (joint_distance(Landmarks

distance_List[@ 4
self .detect_status([@]
distance_List[1] 4

Function for recognizing limbs, which invokes the model to draw key points of

52



=== Page 53 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

the human body based on the recognized information, and then performs

movements according to the recognized posture.

Main:

Lf .running:

image - . iLmage_queue. get (bLock-=
queue.Empty:
-running

resuLt_image = image_proc(np.copy(image
BaseException 5 et
get_Logger().info('\@

33[1;32m%s\033[0m' e
resuLt_image = cv2.flip(cv2.cvtColor(image, cv2.COLOR_RGB2BGR 1
f.fps.update

F.fps.show_fps(resuLt_image)
, result_image

F.mecanum_pub.pubLlish(Twist
-running =

The main function within the BodyControlNode class, used to input image

information into the recognition function and display the returned image.

10. Human Body Tracking

Note: This game is best suited for indoor environments. Outdoor settings may

significantly interfere with its effectiveness!

Utilize the yolov5 framework to import a pre-trained human pose model for
detecting human bodies. The center point of the detected human body will be
indicated in the returned image. When a human body approaches, the robot
will retreat; conversely, if the human body is distant, the robot will move
forward. This ensures that the distance between the human body and the robot

remains approximately 3 meters at all times.
10.1 Program Logic

First, import the human pose estimation model from yolov5, and subscribe to
topic messages to obtain real-time camera images. Next, utilize the trained

model to detect key points of the human body in the images, and calculate the

53



=== Page 54 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

coordinates of the human body's center point based on all detected key points.
Finally, update the PID controller based on the coordinates of the human
body's center point and the screen's center point to control the robot's

movement in sync with the human body's movement.

10.2 Operation Steps

Note: When entering commands, strict case sensitivity is required, and you can use

the "Tab" key to complete keywords.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to start the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.service

4) Enter the following command and press Enter:

ros2 launch example body_track.launch.py

Launch example body _track.Launch.py

5) To exit this activity, press the 'Esc' key in the image interface to exit the

camera view.

6) Press 'Ctri+C' in the command line terminal to close all windows. If you

encounter any issues closing them, please try again.



=== Page 55 ===
4 IVW/E) 1 Oo — t Shenzhen Hiwonder Technology Co,Ltd

10.3 Program Outcome

Upon initiating the game, the camera captures the human body within its field
of view. Once detected, the center point of the human body is highlighted in the

displayed image.

From the robot's perspective, if the human body is in close proximity, the robot
will retreat. Conversely, if the human body is distant, the robot will move
forward, ensuring that the distance between the human body and the robot

remains approximately 3 meters at all times.

10.4 Program Analysis

The program file is saved in

ros2_ws/src/example/example/body_control/include/body_track.py

55



=== Page 56 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Function main Run the main function of this file
_ init Initialization
| get_node state Node status

i Terminate program

wont k ; shutdown callback function
O rack. -

Je py ; Body control image_callback Image callback function
~—— Class BodyControl Node class

image proc Body recognition function

main Class main function

. Get the target
get_object_callback detection result

‘ Depth data
ME depth image callback callback function

Note: Prior to making any alterations to the program, ensure to create a backup of
the original factory program. Modify it only after creating the backup. Directly
editing the source code file is prohibited to prevent inadvertent parameter

modifications that could render the robot dysfunctional and irreparable!

Based on the game's effectiveness, the procedural logic is delineated as

follows:

Image obtaining (Camera)
Recognize human’s position

Determine the distance between human
and robot
Track human’s movement based on the
set distance

a. The car captures images through the camera.

56



=== Page 57 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

b. It identifies the human body's position in the image and calculates the

distance between the human body and the car.

c. Finally, the car adjusts its movement to follow within the preset distance
limit.

The program's logic flowchart, derived from the program files, is illustrated

below:

Linear Velocity and Angular Velocity
Settings
1. Initialization Function Definition of Chassis Publisher
__ init__(self, name)
yolovS Human Body Recognition Acquisitio

pid Parameter Initialization

Implement Security Measures for
Block Diagrams
“gs Logical Assessment for Determin
2. Human Body Recognition
and Tracking Logic Judgement

(image_callback)

Chassis Message
Publishing

Initialization Function of the BodyControlNode Class:

Configuration of Linear and Angular Speed: Adjusts the car's speed based

on the detected distance from the human body.

Definition of Chassis Publisher: Publishes the chassis's position and

orientation to control the car's forward and backward movement.

Yolov5 Human Body Detection: Utilizes the yolov5 single-target detection

57



=== Page 58 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

algorithm to detect human bodies and obtain their positions.

PID Parameter Initialization: Initializes parameters related to the PID
controller, governing sensitivity to the distance between the car and the human

body.

10.4.1 Function

Main:

main
node = BodyControLNode

rcLpy.spin(node
node .destroy_node

Used to start the human tracking node.

10.4.2 Class

BodyControLNode(Node
init__ , Name):
reLpy.init
3 super().__init__(name, allow_undeclared_parameters=Tr , automaticaLlly_decLlare_parameters f
rom_overrides=
name = name

.pid_d = pid.PID

=a
d_angular = pid.PID
pid_angula 1

—~ nid
= pid.

This class is the human tracking node.

Init:

rceLpy.init
super ( __init__(name, allow_undeclared_parameters=True, automatically_declare_parameters_
rom_overrides-Tr
f.name = name

f.pid_d = pid.PID a

#SeLT.p1d_c

: pid_angular = pid.PID

f.go_speed, turn_speed =

Initialize the parameters required for human tracking, read nodes such as the

camera's image callback, depth information, chassis, YOLOv5 recognition, etc.,

58



=== Page 59 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

and then synchronize the time to align depth information and image

information, finally start the main function within the class.

get_node_state:

" get_node_ state 1 request, response):
response.success =

response

Set the initialization state of the current node.

shutdown:

shutdown(self, signum,

.running =

Program exit callback function used to terminate recognition.

get_object_callback:

t_caLLback msg
i msg.objects:
class_name = i.class_name
class_name — 'person'
i.box[1 16:
F.center = [|

center =

Callback function for YOLOv5 recognition node, which converts recognition

information into pixel coordinates.

multi_callback:

muLti_callLback depth age, ros_image):
depth_frame np.ndarra e-(depth_image.height, depth_image.width), dtype=np.uint16,
r=depth_image.data
rgb_image = s e= _image.height, ros_image.width, 3 dtype=np.uints
ros_image.data - f (Cc vert the custo Lmage ssage into image)

+ ES tke a ]
.image_queue.put([depth_frame, rgb_image

Time synchronization callback function that requires input of depth information
and RGB image, synchronizes both pieces of information, and then enqueues
them.

image_proc:

59



=== Page 60 ===
2f image_proc(self, image):
twist = Twist()
depth_ Frame = image([@]
ber_ image = image[1]

f.center t N ;
h, w = bgr_image. chapel 7-1]
cv2.circle(bgr_image, tur elf.center),
HHHHHHH HHH RAPE EH
rol h; rot_w— 5; 5
w_1 = self.center([@] roi_w
w_2 = se .center[@] + roi_w

Function for tracking humans, which invokes the model to draw the position of
the person based on the recognized information, and uses PID to control the

movement according to the recognized person.

F.running:

image = seLlf.image_queue.get(bLlock=True, timeout=1
eee Empty:
Lf. running:

resuLlt_image = seLf.image_proc(image
BaseException e:
-get_ logger ( . info
.center = N
¥ eve: CunhiwCeake ane, resuLt_image)
Rey = cv2. sila 1
if key = ord('q' r key — 27: # Ram SesciBy
; .-mecanum_pub. publish Twist
running = Fals

\@33[1;32

The main function within the BodyControlNode class, used to input image

information into the recognition function and display the returned image.

10.5 Function Expansion

The default tracking speed in the program is set to a fixed value. If you
wish to alter the tracking speed of the robot, you can do so by adjusting

the PID parameters within the program.

60



=== Page 61 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

1) Open the terminal, enter the command to navigate to the directory where

the program is stored:

cd /home/ubuntu/ros2_ws/src/example/example/body_control/include/

home/ubuntu/ros2 ws/src/example/example/body control/include

2) Run the following command to open the program file:

vim body_track.py
body track.py a

3) Locate the "self.pid_d" and "self.pid_angular" functions, where the
values inside the parentheses are the PID-related parameters. One is for
the tracking linear velocity PID, and the other is for the tracking angular

velocity PID.

it__(name, allow_undeclared_parameters= ,» automatically _declare_parameters_from_overrides= )
= name

self.pid_d = pid.PID(

self.pid_angular = pid.PID(

, self.shutdown)

queue .Queve(maxsize=2)

The three PID parameters are proportional, integral, and derivative. The
proportional parameter adjusts the response level, the integral parameter
adjusts the smoothness, and the derivative parameter adjusts whether there is

overshoot.

4) Press "i" to enter edit mode. If you want to increase the robot's tracking
speed, you can correspondingly increase the value. For example, here we

set the tracking linear velocity PID to 0.05.

Note: It is recommended not to adjust the parameters too high. Excessive

parameters will cause the robot to track too quickly, affecting the experience.

61



=== Page 62 ===
rclpy.init()
super().__init__(name, allow_undeclared_parameters=True, automatically_declare_parameters_from_overrides=True)
self.name = name

self.pid_d = pid.PID( - si »)

self.pid_angular = pid.PID(
#self.pid_angular pid.PID(90

self.go_ speed, self.turn_speed =
self.linear self.angular =
self.runnin rue

self.fps = PS() # fpsitW
signal.signal gnal.SIGINT, s
self.image queve = queve.Queue(maxsize=2)

5) After completing the modifications, press "Esc" to exit edit mode, then

press ":wq" to save and exit.

low_undeclared_parameters=True, automatically_declare_parameters_from_overrides=True

scription(Image, , self.image_callback,

bscription(Image, % camera, self.depth_image callback,

Maer iit sme 22% “42/190— 49

6) Start the game according to the instructions provided in section 10.2.

11. Integration of Body Posture and RGB Control

The depth camera combines RGB capabilities, enabling both color recognition
and somatosensory control. This lesson will leverage color recognition, as
discussed in "9. Posture Control", to identify individuals wearing clothing of a
predefined color (which can be calibrated through operations). The robot's

movements are then controlled based on different body gestures.

If an individual wearing the specified color is not recognized, the robot remains
unresponsive, ensuring accurate identification and control over the individual

operating the robot.

62



=== Page 63 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

11.1 Program Logic

First, import MediaPipe's human pose estimation model and subscribe to topic

messages to obtain real-time camera footage.

Next, utilize the built model to detect key points of the human torso on the
screen. Connect these key points to display the human torso and determine
the human posture. Calibrate the center point of the human body based on all

key points.

Finally, if it is detected that the human body is standing with hands on hips,
calibrate the color of the clothes to identify the control object. The robot enters
control mode, and when the human body performs specific actions, the robot

responds with corresponding actions.

11.2 Operation Steps

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.serviceff

4) Run the following command and hit Enter key to initiate the game:

ros2 launch example body_and_rgb_control.launch.py

Launch example body _and_rgb_control.launch.py

63



=== Page 64 ===
4 IVW/E) 1 Oo — t Shenzhen Hiwonder Technology Co,Ltd

5) If you need to close this game, you need to press the "Esc" key in the

image interface to exit the camera image interface.

6) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

11.3 Program Outcome

After starting the gameplay, stand within the camera's field of view. When a
person is detected, the screen will display key points of the human torso, lines

connecting these points, and the center point of the human body.

First Step: Slightly adjust the camera to maintain a certain distance, ensuring

it can detect the entire human body.

Second Step: When the person to be controlled appears on the camera
screen, they can assume the posture of hands on hips. If the buzzer emits a
short beep, the robot completes calibration of the human body's center point

and clothing color, entering control mode.

64



=== Page 65 ===
4 IVW/E) | Oo — t Shenzhen Hiwonder Technology Co,Ltd

Third Step: At this point, with the robot as the primary viewpoint, raising the
left arm causes the robot to move to the right; raising the right arm causes the
robot to move to the left; raising the left leg causes the robot to move forward;

raising the right leg causes the robot to move backward.

Fourth Step: If a person wearing a different clothing color enters the camera's

field of view, they will not control the robot.

65



=== Page 66 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

11.4 Program Analysis

The program file is saved in:

ros2_ws/src/example/example/body_control/include/body_and_rgb_cont

rol.py

get joint landmarks Convert pixel coordinate

| joint_distance Calculate the distance to each recognition point

\/
_ | main Run the main function of this file
_-—Function } -

\ get_body_center Calculate the center point of the body
\ get_dif Compare the color of clothes

\__ joint_angle Calculate joint angle

_ init Initializa
body and _rgb control.py | get node state Node status

Terminate the program

shutdown callback function
2 Image callback
image callback funcion

— Class BodyControlNode S(Aye#ils

move Action function after recognition

\ buzzer warn — Buzzer alarm

\ image_proc Body recognition function

\ ™AIN Class main function

Note: Prior to making any alterations to the program, ensure to create a backup of
the original factory program. Modify it only after creating the backup. Directly
editing the source code file is prohibited to prevent inadvertent parameter

modifications that could render the robot dysfunctional and irreparable!

Based on the game's effectiveness, the procedural logic is delineated as

follows:

66



=== Page 67 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Image obtaining (Camera)

Human body key point
recognition

akimbo detection

Determine whether |
is the same person based
on color recognitio

Pose judgement

Robot performs
corresponding action

Retrieve the captured image from the camera, analyze the key characteristics
of the human body, initially detect and assess the "akimbo" posture using the
designated function. Subsequently, determine if it's the same individual in the
picture based on clothing color. If confirmed, identify specific body movements
(raising the left arm, right arm, left leg, and right leg) and instruct the car to

execute corresponding actions. Otherwise, reanalyze the key feature points.

The program's logic flowchart, derived from the program files, is illustrated

below:

67



=== Page 68 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Object Definition for Drawing Tool
1 Definition for Basic Parameters Object Definition for Body Detection
__ init__(self, name)

Pose Status and Counting Parameter Initialization

Human Body Key Point Recognition & Output

Upper Body Joint Angle Calculation
joint_angle
2 Recognition Logic Processing Logical Judgment of akimbo Posture
image_proc
Color Calibration
(Determine Whether It Is a Calibrated Person)

Storage and Judgement of Limb
Movement Types

Publish Chassis Motion Node

As shown in the above diagram, the program first uses the initialization
function of the BodyControlNode class to set the default values for the relevant
parameters. This primarily includes defining the drawing tool object, the limb
detection object, and initializing the posture state and count parameters. After
this setup, the program can logically process the incoming images. It begins by
identifying and outputting key body feature points, calculating joint angles from
the obtained (arm) parameters to mark the hands-on-hips posture. Then, it
matches colors based on the recognized key feature points to determine if the
person has been previously marked. Finally, the program controls the
movement of the vehicle by recognizing demonstration actions (raising an arm,

lifting a leg).

68



=== Page 69 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

11.4.1 Function

maint
node = BodyControLNode('body

rcLpy.spin(node)
node.destroy_node

Used to start the RGB body sensation control node.

get_body_center:

F get_body_center(h, w, Landmarks):
Landmarks = np.array Lm.x * w, Lma.y x h Lm Landmarks]
center = ((Landmarks[LEFT_HIP] + Landmarks[LEFT_SHOULDER] + Landmarks[RIGHT_HIP] + Landmarks[
[RIGHT_SHOULDER] )/4).astype
r n center.toLlist

Used to obtain the currently recognized body contours.
get_joint_landmarks:

get_joint_Landmarks img, Landmarks):

h, w = img.shape

landmarks = [(1m.x * w, lm.y * h 1 Landmarks]
r np.array(Landmarks

Used to convert the recognized information into pixel coordinates.
get_dif:

" get _dif(list1, List2):
if Len(list1) + len(list2):

d = np.absoLute(np.array(Llist1) - np.array(List2
ret sum(d

Used to compare the color of clothes on the body contours.

joint_angle:

69



=== Page 70 ===
fF joint_angle(lLandmarks):

angle_List = []

left_hand_angle1 = vector_2d_angle(Landmarks[LEFT_SHOULDER] - Landmarks[LEFT_ELBOW], Landmarks[
[LEFT_WRIST] - Landmarks[LEFT_ELBOW])

angle_List.append(int(Left_hand_angle1))

7 Lleft_hand_angle2 = vector_2d_angle(lLandmarks[LEFT_HIP] - Landmarks[LEFT_SHOULDER], Landmarks[
[LEFT_WRIST] — Landmarks[LEFT_SHOULDER])
angle_lList.append(int(Left_hand_angle2))

right_hand_angle1 = vector_2d_angle(Landmarks[RIGHT_SHOULDER] - Landmarks[RIGHT_ELBOW],
Landmarks[RIGHT_WRIST] - Landmarks[RIGHT_ELBOW] )
angle_List.append( Cright_hand_angle1))

8 right_hand_angle2 = vector_2d_angle(lLandmarks[RIGHT_HIP] - Landmarks[RIGHT_SHOULDER], Landmarks
[RIGHT_WRIST] - Landmarks [RIGHT_SHOULDER])
angle_List.append(int(right_hand_angle2))

nm angle_list

This function is used to calculate the recognition angles of various joints

between the body parts.

joint_distance:

joint_distance( Landmarks):
distance list = []

ono

Landmarks[LEFT_HIP] - Landmarks[LEFT_SHOULDER]

Landmarks[LEFT_HIP] - Landmarks[LEFT_WRIST]

= d1[@]xx2 + d1[1]*x2

= d2[@]*x2 + d2[1]x*x2
distance_List.append(round(dis1/dis2, 1))

nbhOWONrE OD

Landmarks[RIGHT_HIP] - Landmarks [RIGHT_SHOULDER]
Landmarks[RIGHT_HIP] - Landmarks[RIGHT_WRIST]
= d1[@]*x*2 + d1[1]*x2
dis2 = d2[@]xx2 + d2[1]xx2
distance_List.append(round(dis1/dis2, 1))

Landmarks[LEFT_HIP] - Landmarks[LEFT_ANKLE]

Landmarks[LEFT_ANKLE] - Landmarks[LEFT_KNEE]

= d1[@]x*x2 + d1[1]*x2

= d2[@]xx2 + d2[1]*xx2
distance_List.append(round(disi/dis2, 1))

WNrPOoOWON D

NOU &

Landmarks[RIGHT_HIP] - Landmarks[RIGHT_ANKLE]
Landmarks[RIGHT_ANKLE] - Landmarks[RIGHT_KNEE]
= di[@]x*2 + d1[1]*x2
dis2 = d2[@]xx2 + d2[1]xx2
distance_List.append(round(dis1/dis2, 1))

oO

NeFOw

to

urn distance_List

This function is used to calculate the distance between each joint point based

on pixel coordinates.

70



=== Page 71 ===
11.4.2 Class

Ss BodyControLNode (Node) :

__init__(self, name):

rcLlpy. init ()

super().__init__ (name)
f.name = name
F.drawing = mp.solutions.drawing_utils

.body_detector = mp_pose.Pose(

Static_image_ mode-F S
min _tracking_ confidence- ;
min_detection_confidence= )

OIA & &

2

28
29
30

-coLor_picker = ColorPicker (Point (), 2)
sd, high signal (signal. SIGINT, shutdown)
-fps = fps.FPS() # Fost

pare

F.running =
F.current color: = None
f.lock_color = No
F.calibrating
F.move_finish
F.stop_flag =
F.count_akimbo =
elf.count_no_akimbo = @
Lf.can_control =

This class is the body control node.

Init:

init_ _(se , Name):
rcLpy.init()
super() init__(name, allow_undeclared_parameters-True, automatically_decLlare_parameters_
from_overrides= e)
eLf.name = name
> pe ae a = mp.solutions.drawing_utils
.body_detector = mp_ ee Pose(
static image_mode=-False,
min _tracking_ confidence. ;
min_detection_confidence= )
F.running = Tr
F.fps = eRe: Fea # fp
4 3S TG

Initialize the parameters required for body control, read the camera's image
callback node, initialize nodes such as servos, chassis, buzzers, motors, etc.,

and finally start the main function within the class.

get_node_state:

F get_node state(s olf, request, response):
response .success = «Truel

rn response

Set the initialization state of the current node.

shutdown:

71



=== Page 72 ===
def shutdown(self, signum, frame

seLf.running = False

Program exit callback function used to terminate recognition.

image_callback:

image_callback(seLlf, ros_image
rgb_image = np.ndarray(shape=(ros_image.height, ros_image.width, 3), dtype=np.uint8,
r=ros_image.data) # BBE XA RIA

joint

joint

joint

nse)

joint

Movement strategy function that moves the vehicle according to the

recognized limb actions.

buzzer_warn:

buzzer_warn(seLf):
msg = BuzzerState
msg.freq = 1906
msg.on_time

msg.off_time
msg.repeat = 1
F.buzzer_pub.pubLish(msg)

Buzzer control function used for buzzer alarms.

image_proc:

72



=== Page 73 ===
image_proc(seLlf, image):
image_flip cv2.flip(cv2.cvtCoLlor(image, cv2.COLOR_RGB2BGR 1
results = seLf.body_detector.process(image

results N i results.pose_Landmarks

twist = Twist

Landmarks = get_joint_Landmarks(image, results.pose_Landmarks. Landmark

# REBRe
angle_List = joint_angle(Landmarks
#print(angle_List)
-15@ < angle_List[@] -36 d -36 angle_List[1] -10 $6 < angle_List[2]
1 10 angle_List[3] 30:
if.count_akimbo += 1 # +1(hands-on-hips detection+1)
-count_no_akimbo = @ # 4&4 ME i iGZ(clear no hands-on-hips detection)

f.count_akimbo = @ # MER iW! clear hands-on-hips detection)
ral foe seawel . a5 = fave be pl or a i o at.

Function for recognizing limbs, which invokes the model to draw key points of
the human body based on the recognized information. Then, it performs color
recognition based on the position of each limb's different contours, finally

determining and moving according to the recognized posture.

fF. running:

image = F.image_queue.get(bLock=True, timeout=1
pt queue.Empty:
Fon -running:

resuLt_image = image_proc(np.copy(Cimage)
BaseException as e:
F.get_Logger().info('\¢ [@m' e)
resuLlt_image = cv2.flip(cv2.cvtCoLor(image, cv2.COLOR_RGB2BGR), 1
elf.fps.update
result_image eLf.fps.show_fps(resuLt_image
cv2.imshow(seLf.name, result_image
key = cv2.waitKey(1
fF key = ord('q' r key = 27: # eqakBesciBy
-mecanum_pub.pubLish(Twist
é -running = ;
rceLpy.shutdown

The main function within the BodyControlNode class, used to input image

information into the recognition function and display the returned image.

12. Pose Detection

Through the human pose estimation model in the MediaPipe machine learning
framework, the human body posture is detected. When the robot detects a

person falling, it will sound an alarm and sway from side to side.

73



=== Page 74 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

12.1 Program Logic

First, import the human pose estimation model from MediaPipe and subscribe

to topic messages to obtain real-time footage from the camera.

Then, process the image, such as flipping, to detect human body information in
the image. Based on the lines connecting the key points of the human body,

calculate the limb height to determine the body movement.

Finally, if "falling" is detected, the robot will sound an alarm and move

forwards and backwards.

12.2 Operation Steps

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and enter the robot system desktop using NoMachine.

2) Click-on lo to open the command-line terminal.
3) Run the command to disable app auto-start app service.

sudo systemctl stop start_app_node.service

stop start_app_node.service

4) Click-on to start the ROS2 command-line terminal.

5) Run the following command and hit Enter key to initiate the game:

ros2 launch example fall_down_detect.launch.py

Launch example fall_down_detect.Launch.py

6) If you need to close this gameplay, you need to press the "Esc" key in the

74



=== Page 75 ===
ht IW/EM Oo er Shenzhen Hiwonder Technology Co,Ltd

image interface to exit the camera image interface.

7) Then press "Ctrl+C" in the command line terminal interface. If closing fails,

please try again.

12.3 Program Outcome

Once the game starts, ensure the human body remains as fully within the
camera's field of view as possible. Upon recognizing the human body, the key

points will be highlighted in the returned image.

At this point, the individual can sit down briefly. Upon detecting the "falling"
posture, the robot will continuously sound an alarm and make repeated

forward and backward movements as a reminder.

12.4 Program Analysis

The program file is saved in

75



=== Page 76 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

get _joint_landmarks Convert pixel coordinate

— Function a main Run the main function of this file

height_cal Calculate human height

init Initialization
| get node state Node status

/ Terminate th
fall down _detect.py [shutdown “mie orm
Image callback

/ image callback frelon

Action function
after recognition

- Class FallDownDetectNode
move

buzzer warn Buzzer alarm

image_proc Body recognition function

\ main Class main function

ros2_ws/src/example/example/body_control/include/fall_down_detect.py

Note: Prior to making any alterations to the program, ensure to create a backup of
the original factory program. Modify it only after creating the backup. Directly
editing the source code file is prohibited to prevent inadvertent parameter

modifications that could render the robot dysfunctional and irreparable!

Based on the game's effectiveness, the procedural logic is delineated as

follows:

Image obtaining
(Camera)

Recognize human body
key point

Detect whether Buzzer keeps making
human ‘falls’ ‘beeping’ sound

Car keeps making ‘beeping’
sound, and moves backward

The car captures images via the camera, identifies the key feature points of the

human body, and assesses whether the current posture indicates a "fall". If a

76



=== Page 77 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

fall is detected, the car's buzzer will emit a continuous "beep" sound while the
car moves backward. Otherwise, the buzzer will only emit a single "beep"

sound.

The program logic flow chart obtained from the program files is depicted in the

figure below.

Object Definition for
Drawing Tool

1. Definition for Basic
Parameters
__init__(self, name

Object Definition for
Body Detection

Status Parameter
Initialization

Save Feature Point Output Result

Fall Detection height_cal

2 Recognition Logic
Processing
image_proc
Execute Corresponding Action
Based on Human Pose

Draw Body Key Point

12.4.1 Function

Main:

main
node = FalLLDownDetectNode

rcLpy.spin(node
node.destroy_node

Used to start the body sensation control node.

get_joint_landmarks:

77



=== Page 78 ===
get_joint_Landmarks(img, Landmarks

nan

#§ Landmarks \\medipipe gy J — = fF (Convert Landmarks from medipipe's normalized
output to pixel coordinates)
tparam img: RH wiaAy AA Cpicture corresponding to pixel coordinate)
4

:param Landmarks: J9G—
treturn:

nun

(normalized keypoint)

h, w, _ = img.shape
Landmarks = Llm.x x w, Lm.y x ) T Landmarks ]
r np.array(Landmarks

Used to convert the recognized information into pixel coordinates.

height_cal:

eight_caL( Landmarks

For iin Landmarks:
y-append(i
height = sum(y

height

Calculates the height of the limbs based on the recognized information.

12.4.2 Class

Fal lLDownDetectNode(Node

__init__(self, name

reLpy.init

super().__init__(name, allow_undeclared_parameters=True, automaticalLy_decLare_parameters fr

om_overrides=True

f.name = name
f.drawing mp.soLutions.drawing_utils
f.body_detector mp_pose.Pose
Static_image_mode= ;
min_tracking_confidence=
min_detection_confidence=
F.running - 1
.fps = fps.FPS # fpsttB#

“I

st

wu

.fall_down_count =
fF.move_finish =
.Stop_flag = Fa
SignaLl.signal(signaL.SIGINT Lf.shutdown
-image_queue = queue. Queue(maxsize=2

“I

s

CeOOIMNH EON

on

This class is the fall detection node.

mame):

__init__(name, allow_undeclared_parameters=True, automatically_declare_parameters
om_overrides-True
name = name

F.drawing = mp.soLlutions.drawing_ utils

F.body_detector = mp_pose.Pose

static_image_mod : /

min_tracking_confidence

min_detection_confidence=

F.running = i
elf.fps = fps.FPS( # fps BR

f.faLllL_down_count =

Initialize the parameters required for body control, read the camera's image

78



=== Page 79 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

callback node, initialize nodes such as chassis, buzzers, and others, and

finally start the main function within the class.

get_node_state:

" get_node_ state | request, response):
response.success =

response

Set the initialization state of the current node.

shutdown:

shutdown(seLlf, signum, frame

Lf.running =

Program exit callback function used to terminate recognition.

image_callback:

image_caLllback(se
rgb_image = np.ndar e.he ,» ros_image.width, 3), dtype=np.uint8
r=ros_image = F a

Move:

twist = Twist

twist.Linear.x =
F.mecanum_pub.pubLish(twist

time.sleep

twist = Twist

twist.Linear.x = 2
‘ mecanum_pub.pubLlish(twist

time.
F.mecanum_pub.pubLish(Twist()
Lf. stop_flag = e
-move_finish =

Movement strategy function that moves the vehicle according to the

recognized limb height.

buzzer_warn:

79



=== Page 80 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

def buzzer _warn(selLf):
if not self.stop_flag:
while not self.stop_flag:
msg = BuzzerState
msg.freq = 1006
msg.on_time = @.14
msg.off_time = 6.1
msg.repeat = 1
seLf.buzzer_pub.pubLish(msg)
time.sleep(@.2)
else:
msg = BuzzerState
msg.freq = 1900
msg.on_time = 6.2
msg.off_time = @.61
msg.repeat = 1
seLf.buzzer_pub.pubLish(msg)

Buzzer control function used for buzzer alarms.
image_proc:

def image_proc(self, image):
image_flip = cv2.flip(cv2.cvtCoLlor(image, cv2.COLOR_RGB2BGR), 1
results = self.body_detector.process (image)
if results is not None and results.pose_lLandmarks:
if self.move_finish:
Landmarks = get_joint_Landmarks(image, results.pose_Landmarks. Landmark
h = height_cal(Landmarks
if h 240:
self .fall_down_count.append(1
else:
seLf.fall_down_count.append(@
if Len(self.fallL_down_count) = 3:
count = sum(self.fall_down_count)

Function for recognizing limbs, which invokes the model to draw key points of
the human body based on the recognized information, and moves according to

the recognized height.
Main:

def main(self
while seLlf.running:
try:
image = seLf.image_queue.get(block=True, timeout=1
except queue.Empty:
if not self.running:
break
else:
continue
try:
resuLlt_image = seLf.image_proc(np.copy(image

except BaseException as e:

seLf.get_Logger() .info('\0@33[1;32m%s\033[@m' % e

resuLt_image = cv2.flip(cv2.cvtCoLlor(image, cv2.COLOR_RGB2BGR
self .fps.update
result_image = self.fps.show_fps(resuLt_image
cv2.imshow(seLf.name, result_image)
key = cv2.waitKey(1)

tat

if key ord('q or Rey 27:
seLf.mecanum_pub.pubLlish(Twist
seLlf.running = False

80



=== Page 81 ===
4 IVW/E) 1 Oo — t Shenzhen Hiwonder Technology Co,Ltd

The main function within the FallDownDetectNode class, used to input image

information into the recognition function and display the returned image.

81


