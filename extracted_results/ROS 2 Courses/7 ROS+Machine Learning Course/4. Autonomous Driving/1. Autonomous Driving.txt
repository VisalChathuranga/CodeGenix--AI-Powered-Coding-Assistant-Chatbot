
=== Page 1 ===
ht IW/EM Oo er Shenzhen Hiwonder Technology Co,Ltd

Autonomous Driving




=== Page 2 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd
Catalog

AUtONOMOUS DIVING ...... cece cece cece cece sence cece cece eeeeececeees
1. Lane Keeping ...... ccc cece cece cc ccc cece eee e eee eeeeees
1.1 Preparation ....... cece ccc cee eect eee eee ee eee eee cere eeee
1.2 Program LOGiC 2... .. cece ccc ct ee eee eee eee eee eee eees
1.3 Operation StepS .... cece ccc cc cece eee eee tenet eee ee eens
1.4 Program Outcome ... 2... cece ccc cece eee eee eee e eee eeee
1.5 Program Analysis ........ cece ccc ec eee cere eee ee eee eee eee eee
2. Road Sign Detection ....... cc ccc cece ccc cece cece reece cess eeecees 11
2.1 Preparation ....... ccc ce ccc ccc eee eee eee eee eee eee eee eee eee 11
2.2 Program LOGIC .. 1... cece cee cece ee ee eee eee cette eee eee eees 11
2.3 Operation StepS 22... cece cc cee eee ee eee eee eee e ee eeee 12
2.4 Program Outcome ... 2... cc ccc ec cee cece eee eee eee eee eees 14
3. Traffic Light RECOGNITION 10... . cece ccc cee cece wc eee cee e eee ceeeees 14
3.1 Preparation ..... ccc ccc ce ce ec eee ee eee eee eee e eee eens 14
3.2 Program LOGIC .. 1... cc cee cece ee eect eee eee eee eect eee eee 15
3.3 Operation StepS 2... ccc ce cece ee eee eee ee ee eee teens 15
3.4 Program Outcome ...... cece ec cee cee eee eee teen e eens 17
4. Turning Decision Making ........... cece cece ccc ccc ccc cre vec cccceeee 18
4.1 Preparation ...... ccc cee cc ccc ee eect eee eee eee eee eee eee eee 18
4.2 Program LOGIC .. 1... cc cece cece ee ee ee eee eee ee eee e ee eees 19
4.3 Operation Steps .... cece ccc ce cece eee eee eee ee eee eeee 19



=== Page 3 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

4.4 Program OUtCOME 2.1... ec cece cee eee eee eee teen tees 21

5. AUTONOMOUS Parking ...... ccc ccc c ww ccc ccc ccc cece cece csc scccccces 22
5.1 Preparation 2... . cece ccc ce cece eee eee eee eee eee e eens 22

5.2 Program LOGIC .. 1... cece ee ccc ee ee eect ee eee eee eee eee eee eee 22

5.3 Operation StepS 2... ce cc ec ccc eee ee eee eee teen teens 23

5.4 Program Outcome ...... cece cece eee cee eee teeter ee eeee 25

5.5 Parameter Adjustment .......... cc cece cc ce eee eee ee eee cece eee 25

6. Integrated Application 2... .... ccc ccc ccc ccc ec ccc cece reece eens 27
6.1 Preparation ..... ccc ccc ce eee cee eee eee reece eee ee ee eee 27
6.1.1 Map Setup 2... ccc cc cc ccc cece eee eee eee eee eee nee 27

6.1.2 Color Threshold Adjustment ........... ccc eee eee eee ee eee eee 28

6.2 Program LOGIC .. 1... cece ce ccc ee eect eee eee eee eee eens 29

6.3 Operation StepS 2... cece ce cece eee ee eee teen tent e eens 30

6.4 Program Outcome ... 2... ccc eect e ee eee teen teens 31

6.5 Program AnalySiS ........ cece cece eee cece eee eee eee eee eee eee 32



=== Page 4 ===
Hiwander
1. Lane Keeping

This lesson focuses on controlling the car to move forward and maintain lane

alignment through instructions.




=== Page 5 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

1.1 Preparation

1)

2)

3)

4)

Before starting, ensure the map is laid out flat and free of wrinkles, with
smooth roads and no obstacles. For specific map laying instructions,
please refer to "0. Autonomous Driving Debugging Lesson" in the same
directory as this section for guidance. (In this lesson, we are only
experiencing the road driving-line patrol function, so there is no need to
place props such as traffic lights and signboards.)

When experiencing this game, ensure it is conducted in a well-lit
environment, but avoid direct light shining on the camera to prevent
misrecognition.

It is essential to adjust the color threshold beforehand and set the color
threshold of the yellow line to avoid misidentification during subsequent
recognition. For specific color threshold adjustment, please refer to the "1
Tutorials\ 3 ROS2 Courses\ 7. ROS+OpenCV Courses" for reference.
It is recommended to position the car in the middle of the road for easy

identification!

1.2 Program Logic

Lane keeping can be divided into three parts: obtaining real-time images,

image processing, and result comparison.

Firstly, real-time images are obtained by capturing images using the camera.

Next, image processing includes color recognition, converting recognized

images into different color spaces, erosion and dilation processing, and

binarization processing.

The result comparison part involves processing the images to select the region

of interest (ROI), outlining the processed images, and further comparing and

calculating.



=== Page 6 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Finally, based on the comparison results, the direction of advancement is

adjusted to keep the robot in the middle of the lane.
The source code for this program can be found at:

/home/ubuntu/ros2_ws/src/example/example/self_driving/lane_detect.py

1.3 Operation Steps

Note: The input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and access the robot system desktop using NoMachine.

Enable the model:

2) Click-on [= | to open the command-line terminal.

3) Execute the command and hit Enter to disable the app service.

sudo systemctl stop start_app_node.service

stop start_app node.service

4) Run the command to enable the autonomous driving service.

ros2 launch example self_driving.launch.py only_line_follow:=true

Launch example self_driving.launch.py only_line_follow:=true

5) If you need to terminate the game, press ‘Ctrl+C’. If the game cannot be

terminated, please retry.




=== Page 7 ===
| iIWEM Oo [Shenzhen Hiwonder Technology Co,Ltd

1.4 Program Outcome

After starting the game, place the robot on the road, and it will automatically
detect the yellow line at the edge of the road. The robot will then adjust its

position based on the detection results.

1.5 Program Analysis

The source code of this program is saved in

ros2_ws/src/example/example/self_driving/lane_detect.py

_-— Function .  jmage callback Image callback function

/ _init_ Initialization

setrol_— Setroi

,.. Get the contour with
get_area_max_contour the maximum area

lane detect.py

' add horizontal line Add horizontal line

~—— Class . LaneDetector j/
= _ add_vertical_line_far Add remote vertical line

| add vertical line near Add near vertical line
\

\ get binary Get binarized image

\ __call__—_ Class regression function

Function:



=== Page 8 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

image_callback:

image_callback(ros_image):
z Lob image
rgb_image np.ndarray(shape=-(ros_image.height, ros_image.width, dtype=np.uint8

=ros_image.data) # [R28 B Ef
image rgb_image

Image callback function is used to read the camera node.
Class:

LaneDetector:

r(Cobject):
, color):

olor = color

weight_sum =

Initialize the required parameters and set the ROI to lock the recognition

range.

set_roi:

Used to set the Region of Interest (ROI) for recognition.

get_area_max_contour:

Meco
get_area_max_contour(contours, threshold=16

contour_area = contours, ip Le( f >: math.fabs(cv2.contourArea(c contours
contour_area = tur ( Pp ) 5) : threshold, contour_area
contour_area

max_c_a = @ contour_area,
g max_c_a

Obtains the contour with the maximum area from the list of contours obtained

through OpenCV.



=== Page 9 ===
add_horizontal_line:

add_horizontalL_Line
# J... =---> |-
h, w image.shape
roi_w_min t(w/2
roi_w_max = Ww
roi_h_min = 6

roi max = h

roi image[roi_h_min:roi_h_max, roi_w_min:roi_w_max] # RRA+W
flip_binary = cv2.flip(roi, 6 # FKFRS

max_y cv2.minMaxLoc(flip_binary 4][1 # BRB EL, BRERA BAD5 HALLS

max_y

Adds a horizontal recognition line based on the width and height of the frame

and the ROI settings.

add_vertical_line_far:

yertical_Line_far

h, w = image.shape[:2]
roi_w_min = int(w/8
roi_w_max t(w/2
roi_h_min = 6
roi_h_max = h
roi = image[roi_h_min:roi_h_max,
Flip_binary = cv2.flip(roi, -1)
#cv2.imshow('1', flip_binary)

min_val, max_vaLl, min_Loc, max_lLoc = cv2.minMaxLoc(ret)

minVal: #hf

maxVaL:

minLoc:

maxLoc:

ie FG BY I TH, 47h BA, FM Jay
y_center = y_@ +
roi = flip_binary

= cv2.minMaxLoc [
roi_w_max See | y_ y_center

y_center = y_@ +

roi = flip_binary center:, :]
X_2, y_2) = cv2.minMaxLoc(roi)[-1

up_p = (roi_w_max - x_2, roi_h_max

up_point = (6,
down_poin (@, 6
up_p[1] down_p[1] + nd up_p[@] - down_p[@] + @:
up_point = t(-down_p[1] up_p[1] down_p[1] up_p[@] down_p[@ + down_p[@]), 6

down_point = t(Ch down_p[1 (Cup_p[1] down_p[1] up_p[@ down_p[@ down_p

up_point, down_point

Adds a recognition vertical line for the part of the frame farther from the robot

based on the ROI settings.
get_binary

if

olor(image, cv2.COLOR_RGB2LAB
img_blur = cv2.GaussianBLur(img_Lab 3 3); 3
mask = cv2.inRange(img_bLur, tup Lab_data['

lab_data['Lab']['St o'][self.target_color]['max'] # — (Bt
eroded = cv2.erode(mask, cv2.getStructuringELement(cv2.MORPH_RECT,
dilated = cv2.dilate(eroded, cv2.getStructuringELement (cv2.MORPH_RECT

dilated

Performs color recognition based on the color space and processes the

binarized image.



=== Page 10 ===
add_vertical_line_near:

up_point, down_point

add_vertical_Line_near(s
# —| E=
# | --->

h, w image.shape
roi_w_min = 6

roi_w_max ie (w/2
roi_h_min = int(h/2

roi_h_max = h

roi = image[roi_h_min:roi_h_max, roi_w_min:roi_w

flip_binary = cv2.flip(roi, -1

#cv2.imshow('1', flip_binary)

x_@, y_@) = cv2.minMaxLoc(flip_binary ] fe BY , 48 25589 a AB
down_p roi_w_max - x_@, roi_h_max - y_@

x_1, y_1) = cv2.minMaxLoc(roi)[-1]
y_center = t roi_h_max - roi_h_min - y_1
roi = flip_binary[y_center:

x, y cv2.minMaxLoc(roi) [-1
up_p = (roi_w_max X, roi_h_max y + y_center

up_point = (0, 6

down_point = (@, @
up_p[1] - down_p[1] 5 nd up_p[@] - down_p[@] + @
up_point = (Cint(-down_p[1] up_p[1] down_p[1] up_p[@] down_p[@])) + down_p[6@]
down_point = down_p

up_point, down_point, y_center

Adds a recognition vertical line for the part of the frame closer to the robot

based on the ROI and frame width and height settings.

image, result_image):
FR ty
centroid_sum 6
h, w image .shape[:2]
max_center_x -1
center_x = []
roi i F.rois:
blob image([r @]:roi[1], roi[2]:roi[3]] ww Hroi
contours = cv2.findContours(blob, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1

max_contour_area f.get_area_max_contour(contours, 386
maxX_contour_area is not None:
rect = cv2.minAreaRect(max_contour_area
box np.intp(cv2.boxPoints(rect # -
box 4] =.boxli, 1]: + rotfe
cv2.drawContours(resuLlt_image, [box],

# RRB

pti_x, pti_y box[@, @], box[

pt3 pt3_y = box[2, 6 box

# HP OR

Line_center_x, Line_center_y ptt_x + pt3s_x 2 pti_y + pt3_y

cv2.circle(resuLt_image int (Line_center_x t(Line_center_y
Btpo &
center_x.append(Line_center_x

center_x.append(-1
nr 5 en(center_x
center_x[i 4
center_x[i] max_center_x:
max_center_x center_x[i
centroid_s = center_x[i] * self.rois[i
centroid_sum —
r resuLt_image N , max_center_x
center_pos = centroid_sum F.weight_sum
angle = math.degrees(-math.atan((center_pos

resuLt_image, angle, max_center_x

Callback function for the entire class. Performs color recognition here, draws
the recognized yellow lines using OpenCV, and then outputs the image, angle,

and pixel coordinates X of each ROI recognition contour.

10



=== Page 11 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

2. Road Sign Detection

2.1 Preparation

1) Before starting, ensure the map is laid out flat and free of wrinkles, with
smooth roads and no obstacles. For specific map laying instructions,
please refer to "0. Autonomous Driving Debugging" in the same
directory as this section for guidance.

2) The model referenced in this section is trained using YOLOv5. For more
details on YOLOv5, please consult '1 Tutorials\3 ROS2 Course\7
ROS+Machine Learning Course\1. Machine Learning Fundamentals’

3) When experiencing this game, ensure it is conducted in a well-lit
environment, but avoid direct light shining on the camera to prevent

misrecognition.

2.2 Program Logic

Firstly, acquire the real-time image from the camera and apply operations such

as erosion and dilation.
Next, invoke the YOLOv5 model and compare it with the target screen image.

Finally, execute the appropriate landmark action based on the comparison

results.
You can find the source code for this program at:

/home/ubuntu/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.p

y

11



=== Page 12 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

2.3 Operation Steps

Note: The following steps exclusively activate road sign detection in the return
screen, without executing associated actions. Users seeking direct experience with
autonomous driving may bypass this lesson and proceed to "Integrated

Application" within the same file.

Please make sure to enter commands with strict attention to capitalization, spacing,

and you can use the "Tab" key to autocomplete keywords.

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on [= | to open the command-line terminal.

3) Execute the command, and hit Enter to disable the app service.

sudo systemctl stop start_app_node.service

stop start_app node.service

4) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/yolov5_detect

5) Type the command to open the program source code.

vim yolov5_trt.py

yolovS trt.py

6) Press the 'i' key to enter insert mode, locate the code highlighted in the red
box, uncomment line 403, and comment out line 404. Then, find line 415
and replace './yolov5s.engine’ with 'traffic_signs_640s_ 7 O.engine.' Once

finished, press the 'ESC' key, type ", and hit Enter to save and exit.



=== Page 13 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

yolov5_wrapper = YoLovSTRT( 5 , classes)
single_picture =
single picture:

7) Run the command to initiate game program.

python3 yolov5_trt.py

Poythons yolovstrt.py

8) Place the marker in front of the camera, and it will be recognized and
highlighted on the screen. To exit this mode, press "Ctrl+C" in the terminal.

If it does not close successfully, please try again.

Note: In the event that the model struggles to recognize traffic-related signs, it may
be necessary to lower the confidence level. Conversely, if the model consistently

misidentifies traffic-related signs, raising the confidence level might be advisable.

9) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/self_driving

10) Enter the command to access the game program.

vim self_driving.launch.py

The red box represents the confidence value, which can be adjusted to modify

the effectiveness of target detection.

13



=== Page 14 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

2.4 Program Outcome

After initiating the game, place the robot on the road within the map. Once the
robot identifies landmarks, it will highlight the detected landmarks and

annotate them based on the highest confidence level learned from the model.

3. Traffic Light Recognition

3.1 Preparation

1) Before starting, ensure the map is laid out flat and free of wrinkles, with
smooth roads and no obstacles. For specific map laying instructions,
please refer to "0. Autonomous Driving Debugging Lesson" in the same

directory as this section for guidance.

2) The model referenced in this section is trained using YOLOv5. For more

14



=== Page 15 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

details on YOLOv5, please consult '1 Tutorials\3 ROS2 Course\7

ROS+Machine Learning Course\1. Machine Learning Fundamentals’

3) When experiencing this game, ensure it is conducted in a well-lit
environment, but avoid direct light shining on the camera to prevent

misrecognition.

3.2 Program Logic

Firstly, capture a real-time image from the camera and apply operations such

as erosion and dilation.
Next, invoke the YOLOv5 model to compare it with the target screen image.

Finally, execute corresponding landmark actions based on the comparison

results.
The source code for this program can be found at:

/home/ubuntu/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.p

y

3.3 Operation Steps

1. Note: The following steps exclusively activate road sign detection in the return
screen, without executing associated actions. Users seeking direct experience
with autonomous driving may bypass this lesson and proceed to "Integrated

Application" within the same file.

2. Please make sure to enter commands with strict attention to capitalization,

spacing, and you can use the "Tab" key to autocomplete keywords.

1) Start the robot, and access the robot system desktop using NoMachine.

15



=== Page 16 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

2) Click-on [= | to open the command-line terminal.

3) Execute the command, and hit Enter to disable the app service.

sudo systemctl stop start_app_node.service

stop start_app node.service

4) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/yolov5_detect
{/home/ubuntu/ros2 ws/src/example/example/yolov5 detect

5) Type the command to open the program source code.

vim yolov5_trt.py

6) Press the 'i' key to enter insert mode, locate the code highlighted in the red
box, uncomment line 403, and comment out line 404. Then find line 415
and replace './yolov5s.engine’ with 'traffic_signs_640s_ 7 O.engine.' Once

you’ re done, press the 'ESC' key, type ", and hit Enter to save and exit.

yolov5_wrapper = YoLovSTRT( 5 , classes)
single_picture =

7) Run the command to initiate game program.
python3 yolov5_trt.py
yolovS trt.py i}
8) Position the road sign in front of the camera, and the robot will recognize

16



=== Page 17 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

the road sign automatically. If you need to terminate this game, press

‘Ctri+C’.

Note: In the event that the model struggles to recognize traffic-related signs, it may
be necessary to lower the confidence level. Conversely, if the model consistently

misidentifies traffic-related signs, raising the confidence level might be advisable.

1) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/self_driving

2) Enter the command to access the game program.

vim self_driving.launch.py

The red box represents the confidence value, which can be adjusted to modify

the effectiveness of target detection.

)

yolov5_node = Node(
package=
executable=
output= “
parameters=[{

)

self_driving_node = Node(
package=
executable=

output= ‘
parameters=[{ ¢ Start}, 4 : only_Line_follow}],

3.4 Program Outcome

After initiating the game, position the robot on the road depicted on the map.
Upon recognizing the traffic signal, the robot will assess the color of the signal
light and identify frames corresponding to red and green signal lights

accordingly.

17



=== Page 18 ===
4 IWS) | Oo i) w Shenzhen Hiwonder Technology Co,Ltd

4. Turning Decision Making

4.1 Preparation

1) Before starting, ensure the map is laid out flat and free of wrinkles, with
smooth roads and no obstacles. For specific map laying instructions,
please refer to "0. Autonomous Driving Debugging Lesson" in the same

directory as this section for guidance.

2) The model referenced in this section is trained using YOLOv5. For more
details on YOLOv5, please consult '1 Tutorials\3 ROS2 Course\7

ROS+Machine Learning Course\1. Machine Learning Fundamentals’

3) When experiencing this game, ensure it is conducted in a well-lit
environment, but avoid direct light shining on the camera to prevent

misrecognition.

18



=== Page 19 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd
4.2 Program Logic

Firstly, capture the real-time image from the camera and apply operations such

as erosion and dilation.

Next, invoke the YOLOv5 model to compare the obtained image with the target

screen image.

Finally, based on the comparison outcomes, recognize the steering sign and

direct the car accordingly.
The source code of this program is saved in

/home/ubuntu/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.p

y

4.3 Operation Steps

Notice:

1. The following steps exclusively activate road sign detection in the return
screen, without executing associated actions. Users seeking direct experience
with autonomous driving may bypass this lesson and proceed to "Integrated

Application" within the same file.

2. Please make sure to enter commands with strict attention to capitalization,

spacing, and you can use the "Tab" key to autocomplete keywords.

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on [=| to open the command-line terminal.

3) Execute the command, and hit Enter to disable the app service.

sudo systemctl stop start_app_node.service

19



=== Page 20 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

4) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/yolov5_detect

home /ubuntu/ros2 ws/src/example/example/yolovS detect

5) Type the command to open the program source code.

vim yolov5_trt.py

yolovS trt.py

6) Press the ‘i’ key to enter insert mode, locate the code highlighted in the red
box, uncomment line 403, and comment out line 404. Next, find line 415
and replace './yolov5s.engine’ with 'traffic_signs_640s_ 7 O.engine.' Once

done, press the 'ESC' key, type ", and then hit Enter to save and exit.

yolovS_wrapper = YoLovSTRT(

id = yolovS5_wrapper.infer(frame)

7) Run the command to initiate game program.

python3 yolov5_trt.py

Poythons yolovstrt.py

8) Place the marker in front of the camera, and it will be recognized and
highlighted on the screen. To exit this mode, press "Ctrl+C" in the terminal.

If it does not close successfully, please try again.

Note: In the event that the model struggles to recognize traffic-related signs, it may

be necessary to lower the confidence level. Conversely, if the model consistently
20



=== Page 21 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

misidentifies traffic-related signs, raising the confidence level might be advisable.
1) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/self_driving

home /ubuntu/ros2 ws/src/example/example/self drivin

2) Enter the command to access the game program.

vim self_driving.launch.py

The red box represents the confidence value, which can be adjusted to modify

the effectiveness of target detection.

)

yolov5_node = Node(
package=
executable=
output=
parameters=[{

)

self_driving_node = Node(
package=
executable=

output= ‘
parameters=[{ $ start}; £ : only_line_follow}],

4.4 Program Outcome

Once the game begins, position the robot onto the road within the map. As the
robot approaches a turning road sign, it will adjust its direction in accordance

with the instructions provided by the sign.

21



=== Page 22 ===
4 IVW/E) | Oo — t Shenzhen Hiwonder Technology Co,Ltd

5. Autonomous Parking

5.1 Preparation

1) Before starting, ensure the map is laid out flat and free of wrinkles, with
smooth roads and no obstacles. For specific map laying instructions,
please refer to "0. Autonomous Driving Debugging Lesson" in the same

directory as this section for guidance.

2) The model referenced in this section is trained using YOLOv5. For more
details on YOLOv5, please consult '1 Tutorials\3 ROS2 Course\7

ROS+Machine Learning Course\1. Machine Learning Fundamentals’

3) When experiencing this game, ensure it is conducted in a well-lit
environment, but avoid direct light shining on the camera to prevent

misrecognition.

5.2 Program Logic

Begin by capturing the real-time image from the camera and applying

operations such as erosion and dilation.

Next, invoke the YOLOv5 model to compare the obtained image with the target

22



=== Page 23 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

screen image.

Finally, based on the comparison results, identify the parking road sign and

autonomously guide the car to park in the designated parking space.
The source code of this program is saved in

/home/ubuntu/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.p

y

5.3 Operation Steps

Notice:

1. The following steps exclusively activate road sign detection in the return
screen, without executing associated actions. Users seeking direct experience
with autonomous driving may bypass this lesson and proceed to "Integrated

Application" within the same file.

2. Please make sure to enter commands with strict attention to capitalization,

spacing, and you can use the "Tab" key to autocomplete keywords.

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on [= | to open the command-line terminal.

3) Execute the command, and hit Enter to disable the app service.

sudo systemctl stop start_app_node.service

stop start_app node.service

4) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/yolov5_detect

5) Type the command to open the program source code.

23



=== Page 24 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

vim yolov5_trt.py

olovS trt.

6) Press the ‘i’ key to enter insert mode, locate the code highlighted in the red

box, uncomment line 403, and comment out line 404. Then, find line 415
and replace './yolov5s.engine’ with 'traffic_signs_640s_ 7 O.engine.' Once
you’ ve completed this, press the 'ESC' key, type '", and hit Enter to save

and exit.

yolov5_wrapper = YoLovSTRT( 5 , classes)
single_picture =
single picture:

7) Run the command to initiate game program.

python3 yolov5_trt.py

Poythons yolovstrt.pvM

8) Place the marker in front of the camera, and it will be recognized and
highlighted on the screen. To exit this mode, press "Ctrl+C" in the terminal.

If it does not close successfully, please try again.

Note: In the event that the model struggles to recognize traffic-related signs, it may
be necessary to lower the confidence level. Conversely, if the model consistently

misidentifies traffic-related signs, raising the confidence level might be advisable.

1) Run the command to navigate to the directory containing programs.

cd /home/ubuntu/ros2_ws/src/example/example/self_driving

2) Enter the command to access the game program.

24



=== Page 25 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

vim self_driving.launch.py

The red box represents the confidence value, which can be adjusted to modify

the effectiveness of target detection.

)

yolov5_node = Node(

parameters=[{

)

self_driving_node = Node(
package=
executable=

output= ‘
parameters=[{ ¢ Start}, 4 : only_Line_follow}],

5.4 Program Outcome

After initiating the game, position the robot on the road within the map. As the
robot progresses towards the parking sign, it will automatically park in the

designated parking space based on the instructions provided by the road sign.
5.5 Parameter Adjustment

If the robot stops upon recognizing the parking sign and the parking position is
not optimal, adjustments to the parameters in the program source code can be

made.

1) Click-on to open the ROS1 command-line terminal.

2) Execute the command to navigate to the directory containing game

programs.

cd ros2_ws/src/example/example/self_driving/

3) Run the command to access the source code.

vim self_driving.py

25



=== Page 26 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

4) Press the "i" key to enter insert mode and locate the code within the red

box. Adjusting the parameters within the red box allows you to control the
starting position for the robot to initiate the parking operation. Decreasing
the parameters will result in the robot stopping closer to the zebra crossing,
while increasing them will cause the robot to stop further away. Once
adjustments are made, press the "ESC" key, type ":wq", and press Enter to

save and exit.

< self.park_x < self.crosswalk_distance:
twist.linear.x = self.slow_down_speell
self.machine type !=
self.start_park j < self.crosswalk_distance:

self .mecanum_pub.publish(geo_msg.Twist())

self.start_park =

self.stop =

threading. Thread(target=self.park_action).start()
self.machine_type ==

self.start_park < self.crosswalk_distance:

se mecanum_pub.publish(geo_msg.Twist())

self.start_park

self.stop =

threading. Thread(target=self.park_action).start()

You can adjust the parking processing function to alter the parking position of
the robot. Initially, the parking action sets the linear speed in the negative
direction of the Y-axis (right of the robot) to 0.2 meters per second, with a
forward movement time of (0.38/2) seconds. To position the robot in the ideal
location on the left side of the parking space, modify the speed and time

accordingly.

(self):
self.machine type ==
twist = geo_msg.Twist()
twist.Llinear.y = -
self.mecanum_pub.publish( twist)

rospy.sleep( / )

- self.machine_type ==

twist = geo_msg.Twist()

twist.linear.x =

twist.angular.z = twist.linear.x*math.tan(-
self.mecanum_pub.publish( twist)
rospy.sleep(3)

26



=== Page 27 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd
6. Integrated Application

This lesson provides instructions for implementing comprehensive driverless
game on the robot, covering lane keeping, road sign detection, traffic light

recognition, steering decision-making, and self-parking.
6.1 Preparation

6.1.1 Map Setup

To ensure accurate navigation, place the map on a flat, smooth surface, free of
wrinkles and obstacles. Position all road signs and traffic lights at designated
locations on the map, facing clockwise. The starting point and locations of road

signs are indicated below:

Note: Tools required for autonomous driving are available for separate
purchase. If you are interested, kindly reach out to us at

support@hiwonder.com.

27



=== Page 28 ===
Shenzhen Hiwonder Technology Co,Ltd

SIS ah)
ZTERKT
A

NUNUUT seer pR ES VAAN UAT WT

be ache ana
a ATA
mniniti mn mn

Go straight
TTS reeves

soo =

6.1.2 Color Threshold Adjustment

Due to variations in light sources, it's essential to adjust the color thresholds for
‘black, white, red, green, blue, and yellow’ based on the guidelines provided in
the '6 ROS+OpenCV Course’ prior to starting. If the robot encounter
inaccurate recognition while moving forward, readjust the color threshold

specifically in the map area where recognition fails.

28



=== Page 29 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

6.2 Program Logic
Color rgb/image raw
Recognition |
go
self_driving 7

+ right hiwonder controller

yolov5/ /cmd_vel

object_detect park
YOLOvS = |}—_____,

red

green

— crosswalk

Actions implemented so far include:

1. Following the yellow line in the outermost circle of the patrol map.
2. Slowing down and passing if a zebra crossing is detected.

3. Making a turn upon detection of a turn sign.

4. Parking the vehicle and entering the parking lot upon detection of a stop

sign.
5. Halting when a red light is detected.
6. Slowing down when passing a detected street light.

First, load the model file trained by YOLOv5 and the required library files,
obtain real-time camera images, and perform operations such as erosion and
dilation on the images. Next, identify the target color line segment in the image
and gather information such as size and center point of the target image. Then,
invoke the model through YOLOv5 and compare it with the target screen
image. Finally, adjust the forward direction based on the offset comparison of
the target image's center point to keep the robot in the middle of the road.
Additionally, perform corresponding actions based on different recognized

landmark information during map traversal.
29



=== Page 30 ===
HIW/E2MOCT Shenzhen Hiwonder Technology Co,Ltd

The source code for this program can be found at:

/home/ubuntu/ros2_ws/src/example/example/self_driving/self_driving.py

6.3 Operation Steps

Note: the input command should be case sensitive, and keywords can be

implemented using Tab key.

As ROS2 is located within a container, it cannot directly access the GPU of the
mainboard. Thus, it's necessary to initiate the GPU of the mainboard through ROS1

and then transfer the recognition information for use within ROS2.

Start the robot, and access the robot system desktop using NoMachine.

Enable the model:

1) Click-on [=| to open the command-line terminal.

2) Execute the command, and hit Enter to disable the app service.

sudo systemctl stop start_app_node.service

stop start_app node.service

3) Run the command to enable the autonomous driving service.

ros2 launch example self_driving.launch.py

Launch example self drivin

4) If you need to close the program, simply click on the corresponding
terminal window of the program and use the shortcut "Ctrl+C" to exit the

program.

30



=== Page 31 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

6.4 Program Outcome

@ Lane Keeping

Upon initiating the game, the car will track the line and identify the yellow line
at the road's edge. It will execute forward and turning actions based on the

straightness or curvature of the yellow line to maintain lane position.

@ Traffic Light Recognition

When the car encounters a traffic light, it will halt if the light is red and proceed
if it's green. Upon approaching a zebra crossing, the car will automatically

decelerate and proceed cautiously.

@ Turn and Parking Signs

Upon detecting traffic signs while moving forward, the car will respond
accordingly. If it encounters a right turn sign, it will execute a right turn and
continue forward. In the case of a parking sign, it will execute a parking

maneuver.

Following these rules, the robot will continuously progress forward within the

map.

31



=== Page 32 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

6.5 Program Analysis

The source code of this program is saved in:

ros2_ws/src/example/example/self_driving/self_driving.py

Function main

_ init Initialization
init_process Initializa pose
param init — Initializa parameter

| get_node state Set the status of the current node

| send request Send request

eee / Initiate autonomous
self_d nving.py enter_srv_callback driving callback function
/ Exit autonomous

Class ~. SelfDrivingNode exit sry callback aback function

set_running srv callback
Start autonomous driving callback function

shutdown Terminate the callback function

image callback Image node callback function
park action Parking strategy

main Main program

Recognition callback

\ get object callback yolov5 function

Function:

main :
node = SeLfDrivingNode('self_driving
execu = MuLtiThreadedExecutor

executor.add_node(node
executor.spin
node.destroy_node

Initiate autonomous driving class.

SeLfDrivingNode(Node
__init__ , name
reLpy.init
super __init__(name, aLllow_undecLared_parameters-=Tr automaticaLLy_decLare_parametd

rom_overrides
-hame name

Nag

.is_running
pid = pid.PID
.param_init

© oO

32



=== Page 33 ===
super()._ t__(name, allow_undeclared_parameters=True, automatically_decLlare_parameters f
m_overrides=
€ name = name
fF.is_running =
F.pid = pid.PID
-param_init

.image_ queue = queue . Queue (maxsize=2
classes => ["go*, “right*; =park™,
F.dispaly =f
-bridge = CvBridge
. Lock = threading.RLock
colors = common.Colors
signal. SignaL(signaL.SIGINT, seLf.shutdown
machine_type = os.environ.get('MACHINE TYPE'

. Lane_detect = Lane_detect.LaneDetector("yeLl

F.mecanum_pub = create_pubLlisher(Twist, i)
.joints_pub = fF.create publisher | ServosPosition, “fs

-resuLlt_publisher = st .create_publisher (Image

.create_service(Trigger f : Lf.enter_srv_callback) #
-create_service(Trigger Se exit_srv_caLlback) # i8
.create_service(SetBooLl runnin elf .set_running_srv callback
F T eelt. heart = Heart(self.name + '/heartbeat', 5, Lambda _: self.exit_srv_callback(None))
timer_cb _group ReentrantCaLLbackGroup
.client = self.create_client Trigger,
-cLient.wait_for_service
F.client = self.create_client(Trigger,
-cLlient.wait_for_service
F.start _yolov5_ client = self.create_client Trigger fyo caLllLback_group=
imer _cb _group
start_yoLlov5 client.wait_for_service
-Stop_yoLov5_client = f.create_client(Trigger, '/yolovs , callback_group=
imer_cb_group
-Stop_yolov5_client .wait_for_service

'

'

.timer = seLlf.create_timer fF.init_process, callback_group=timer_cb_group

Initialize the required parameters, obtain the current robot category, set the
line-following color to yellow, start the chassis control, servo control, and image
reading. Set up three types of services: enter, exit, and start, and read the

YOLOv5 node.

init_process:

SI

init_process
.timer.canceL

sN
unrod

-mecanum _pub.publish(Twist
-get_parameter('only_Line_follow').value:
send_request(s start _yolovS client, Trigger.Request
machine_type ver Tz i
servo_ position 5 .joints_pub, ; 1@, 566), (5, 506
1, 500)) # 4

yu~

sy
Nau s

~
oo

set_ -Servo position(seLf.joints_pub, 1,
#
ime.sleep

SI

f.get_parameter t').vaLlue:
-dispaly =
f.enter_srv_callback(Trigger .Request Trigger .Response
request - SetBool. Request
request.data =
~set_running_srv_callback request, SetBool.Response

WNRro

ob

I

#seLf.park_action()
threading .Thread(target= : daemon=Tru start
f.create_service(Trigge '. it_finish get_node_state
.-get_Logger().info Q 033 y rt!

8
8
8
8
8
8
8
8
8
8
9

oo
Ne

Initialize the current robotic arm and start the main function.

param_init:

33



=== Page 34 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

def param_init (self
self.start = False
self.enter = False

seLf.have_turn_right = False
self.detect_turn_right = Falsel

seLf.detect_far_Lane
self.park_x = -1 4

+ +

seLlf.start_turn_time_stamp = @
self.count_turn = 6
self.start_turn = False #

Initialize parameters required for position recognition or usage.

get_node_state:

def get_node_state(self, request,
132 response.success = True
3 return response

response

Obtain the current state of the node.

send_request:

def send_request(self, client, msg
future = client.call_async(msg

while rclpy.ok():
if future.done and future.result
return future.resuLlt

Used to publish service requests.

enter_srv_callback:

def enter_srv_callback(self, request, response):
self.get_Logger() .info('\633[1;32m%s\633[6m'
with seLf.Lock
self.start = False
camera = 'depth_cam'#selLf.ge eter(' th_c a_name value
seLlf.create_subscription(Imag /*es/rgb/image_ n seLf.image_calLback, 1
self.create_subscription(ObjectsInfo, '/yolov5/ob ', self.get_object_callback

"

"self driving enter"

seLf.mecanum_pub.pubLish(Twist
self.enter = True
response.success = True
response.message "enter"
return response

Service for entering autonomous driving gameplay, start reading images and

YOLOv5 recognition content, initialize speed.

exit_srv_callback:

def exit_srv_callback(seLf, request, response
self.get_Logger() .info('\@33[1;32m%s\933[6m' "self driving exit”
with seLf.lock
try:
if self.image_sub is not None:
self .image_sub.unregister
if self.object_sub is not None:
seLf.object_sub.unregister

except Exception as e:
self.get_Logger().info('\@33[1;32m%s\933[@m' % str(e
seLf.mecanum_pub.pubLish(Twist
seLf.param_init
response.success = True
response.message = "exit"
return response

34



=== Page 35 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Service for exiting autonomous driving gameplay, stop reading images and

YOLOv5 recognition content, initialize speed, reset parameters.

set_running_srv_callback:

set_running_srv_calLback( f request, response):
get_Logger().info('\033[1;32m%s\@33[0m' % "
with F.lLock:
€ start = request.data

start:
mecanum_pub.pubLish(Twist
response.success = ie
response.message = "set_running
response

Start autonomous driving game, set the start parameter to True.

Shutdown:

shutdown(self, signum, frame
LF.is_running F

Callback function after closing the program, used to stop the currently running

program.

image_callback:

image_calLback( f s age): # Bt
rgb_image = np.nd ye=(ros_image , ros_image.width, 3 dtype=np.uints
ros_image.data) #

Lf image _queue.put rgb_image

Image callback function, enqueues images and discards expired ones.

park_action:

35



=== Page 36 ===
# HERE
. action DA
S .machine _type
twist = Twist
twist.Linear.y = -
F.mecanum_pub.pubLish(twist
time.sleep( /
) F.machine_type
twist = Twist
twist. Linear.x
twist.anguLlar.z = twist.Linear.x*xmath.tan(-é
F.mecanum_pub.pubLish(twist
time.sleep(3

twist = Twist

twist.Linear.x =

twist. angular.z = -twist.Linear.x*xmath.tan(-
-mecanum_pub.pubLish(twist

9

time. sleep(2

twist = Twist

twist.Linear.x -

twist.angular.z = twist.Linear.x*xmath.tan(-
F.mecanum_pub.pubLish(twist

time.sleep

set_servo_position(seLf.joints_pub,

twist = Twist
twist.angular.z = -1
F.mecanum_pub.pubLish(twist
time.sLeep
F.mecanum_pub.pubLlish(Twist
twist = Twist
twist.Linear.x =
eLf.mecanum_pub.pubLish(twist
time.sleep
$e .mecanum_pub.pubLlish(Twist
twist = Twist
twist.anguLlar.z = 1
.mecanum_pub.pubLish(twist
time.sLleep(1
F.mecanum_pub.pubLlish(Twist

Parking logic, runs three different parking strategies according to three

different chassis types.

get_object_callback:

get_ aback: callback self, msg):
elf.objects_info = msg.objects
.objects_info = > ST
".traffic_signs_status
-crosswaLk_distance =

min_distance = @
For i self .objects_ info:
class name = i.class_name
center = (int((i.box[@] + i. ([2])/2), int(Ci.box[1]

class_name — : v
if center[1] min_distance # RRR A Tey
min_distance = center[1
class_name — 'right'
Lf.count_right += 1
eLf.count_right_miss = @
f -count_right 2 186:
F.turn_right = Trt
F.count_right = 6
if class name — 'park':
.park_ x = center[@
class_name ‘'red' or class_name
traffic_signs_status = i

f.crosswaLlk_distance = min_distance

36



=== Page 37 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Callback function for reading YOLOv5, obtains the categories currently

recognized by YOLOv5.
Main:

main(s
wh .is_running
time_start = time.time
try:
image se Lf .image_queue.get(bLock=Tr , timeout=1
queue.Empty:
eLf.running:

image = image.copy
F.start: :
1, w = image.shape[:2]

tect.get_binary(image)

2%, Ft Je ® Py
crosswaLk_distance i t start_sLow_down:

F.count_crosswaLk += 1
F.count_crosswaLk =
.count_crosswaLk =
.Start_sLow_down
Ou SLOW do

The main function within the class, runs different line-following strategies

according to different chassis types.

FAQ

1. The robot exhibits inconsistent performance during line patrolling, often

veering off course.

Adjust the color threshold to better suit the lighting conditions of the actual
scene. For precise instructions on color threshold adjustment, please consult

"6 ROS+OpenCV Lesson" for detailed guidance.

2. The robot's turning radius appears to be either too large or too small.

1) Ensure correct adjustment of the robot arm deviation. For detailed
instructions on robot arm deviation adjustment, please refer to "3 ROS2
Courses\8 Robot Arm Control Course\Basic Control\2. Robot Arm
Deviation Adjustment (Optional)" for comprehensive learning.

37



=== Page 38 ===
HIW/E9MOECT Shenzhen Hiwonder Technology Co,Ltd

2) Modify the line patrol processing code

Navigate to the game program path by entering the command:

cd ros2_ws\src\example\example\self_driving

Open the game program by entering the command:

vim self_driving.py

The red box denotes the lane's center point, which can be adjusted to fine-tune
the turning effect. Decreasing the value will result in earlier turns, while

increasing it will cause later turns.

3. The parking location is suboptimal.

You can adjust the parking processing function or modify the starting position
of the parking operation. For detailed instructions, please consult "5.5

Parameter Adjustment" for reference and learning.

4. Inaccurate traffic sign recognition.

Adjust the target detection confidence. For detailed instructions, please refer to

"2.3 Operation Steps" for comprehensive learning.

38


