
=== Page 1 ===
| IWE) M Oo = t Shenzhen Hiwonder Technology Co,Ltd

Machine Learning Application




=== Page 2 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd
Catalog

oP © od On \0(0.-) (<1 6-11 (0) 8 ee 5
1.1 GPU Accelerated COMputing ......... ee eeeeeeeeeeeteetetttteeeeeeeeeeeeeeeees 5

1.2 Comparison between GPU and CPU .................cccceeeeeeeeeeeeeeeeeeeees 5

1.3 Advantage Of GPU ........ cece i teen eeteeeneeneeeeee 6

2. TENSOFRT Acceleration .............:..ccee eee eeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaeeeeeeeeeees 6
2.1 TensorRT Introduction .............ccccceeeeeeeeee eee e eee e eee eee eeeeeeeeneeeea 6

2.2 Optimization Methods .............cccccccccccceeeeeeeeeeeeeeeeeeeeeeaeaeeeeeeeeeeeteees 7

3. YOIOVS MOE] 2.2... cece cece cece eect eee eee tee eeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeees 9
3.1 Yolo Model Series Introduction ...................::e cece sees eee eeeeeeeeeeeeeeeeeee 9
3.1.1 YOLO Serie «0.2... cece e teeter eee eeeeeeeeneeeeeea 9

x ms A ©) Ek © )\ Lo 10

3.2 YOLOvS Model Structure .......... eee eeeeeeeeeeeteeeeeeeaaaaeeeeeeeees 11

x oud Ws @0) 00] 010) 0 -) 8 | See 11

3.2.2 Compound Element ..............::::::cccceeceeeeeeeeeeeeeeeeeeeeeeeeeeeees 17

Sano es) (0 (01 (| (eee 22

4. YOLOvS Running Procedure 1.0.0.0... eeeeeeeeeeeeteeeeeaaaaaaaaaaaaaeeeeeeeeeeees 27
4.1 Prior BOUNCING BOX... eee eee eeeeeeeeeeeaaaaaaaaaaaaaaaaaeeeeeeeeeeeeeeeeees 27

4.2 Prediction BOX 0.0.0... eeeeccceceeeeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegs 27

7. BG V8 (6: 81) il = 10), Genre 28

4.4 Realization Process ..............:eeeeee eect eee eee e eee eeeeeeeeeeeeeeeeeee 29

5. Image Collecting & Labeling «0.0.0... eee eee eeeeeeeeeeeeeeeeteeaeaaaaaeeeeeeeeeeeees 29



=== Page 3 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

5.1 Image Collecting ............ cece cece eee ee cece ee eeeeeeeeeaaaaaaaaaaaaaaaaaaaaeeeeeeeeees 29

5.2 IMage LAbDELING ..........eeeeeeeceeeeeeeeeeeeee eee eteet eee e eee e ete ttt teeeteeeeeeene 32

6. Data Format Conversion .............::cceeee cece eee eee eee eee e teeta aaeeeeeeeeeeee 36
6.1 Preparation ............:::::ccccceeeeeceeceeeeeeeeeeeeeeeeeeeeeeeeeeeecseeeeeeeeeeeeeseeaes 36

6.2 Format COnverSin ..........::ccccccceeeeeeeeeeeeee eee e eee e eee eeeeeeeeeeeeeeeeeeneneea 36

7. MOdel Training «0.0.0... eeeeeeeeceeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerereg® 38
7.1 Preparation ............:::::cccceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeaaeeceeeeeeeeeeeeeeseeees 39

7.2 MOdel Training ...........:ccccceeeeeceeeeeeeeeeeeeeeeeeeeeeeee eee t ee eeeeeeeeeeeeeeeeeneed 39

7.3 Import Training Result (Optional) ............... cece 40

8. TensorRT Road Sign Detection ............cccccccccceceeeeeeeeeeeeeeeeeeeeeeeeeeeteeeeees 42
8.1 Preparation ............:::::ccccceeeeeceeeceeeeeeeeeeeeeeeeeeeeeeeeaeeceeeeeeeeeeeeeeseeaes 42

8.2 Generate TensonRT Model Engine ........... ec: eeeeeeeeeeeeeeetetentteeeeees 42

8.3 Target Detection ........... cece cece cette eee ee ee eee eeeeeea 46
8.3.1 INStPUCTIONS «2... eee eee e eee eeeaaaaaaaaaaaaaaaaaaaaaaaeeeeeees 46

8.3.2 Detection ReSUIt ........ eee eeeeeeeeeeeeeeaeeaaaaaaaaaaaaeaanaas 47

9. Traffic Sign Model Training ............... ce cece eee ee ee eeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaeeees 48
9.1 Preparation .............::::ccccceeeeeceeeceeeeeeeeeeeeeeeeeeeeeeeeaeecseeeeeeeeeeeeeeeaees 49

9.2 Training INStrUCtiONS ...........ceceeee cece cece teeter eee eeeeeeeeeeeeea 49
9.2.1 Create Data Set Folder ..........ccccccccccccceceeeeeeeeeeeeeeeeeteeeeetes 49

9.2.2 Image Labeling... cece eee e eee e ee eee e ete eeteeeeeeeeeeee 52

9.2.3 Generate Related Files «2.0.0... ccccccccceeeceeeeeeeeeeeeeeeeeeeeeeeeees 55

9.2.4 Model Training ...........::ccccecceeecceceeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeeeeess 56

9.2.5 Generate TensorRT Model Engine ...............::cceeeeeeeeeeeeees 58

3



=== Page 4 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

9.3 Model USage ...........cccccccceeeeeeeee eee e eee e eee e eee e eee eeteeeeeeeeeeeea 62

10. Waste Card Model Training ...............:cceeee cece cece eee e eee e eee e eee eeeeeeeeeeeeeeeeeee 64
10.1 Preparation ............ccccccccccccceeeeeceeeeeeeeeeeeeeeaaeeeeeeeeeeeeeeeeeeseseaeaeeees 64
10.2 Training INStrUCTIONS 2000... ee eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeee® 64
10.2.1 Prepare Data Set ............ccccccccccccescceeeeeeeeeeeeeeeeeeeeeeteeeeeees 64

10.2.2 Image Collecting ............ cece eee eee eter eee e teens 65

10.2.3 Label Pictures ............ccccececceeeee cette etter tee ee eee eeeeeeeeeeee 67

10.2.4 Generate Related Files 0.0.0... eeeeeeeeeeeeeeeneetteeeeeeeeeeees 71

10.2.5 Start Training ........... cee eee ee eee teen 73

10.2.6 Format Conversion ............ ccc eeeeeeeeeeeeeeeeeeeeeaaaaeaaaaaaaeeeeees 74

10.3 Model USAC 2000... eee eeeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeg® 78

11. Physical Model Training ...............0.0 cece eee eee eeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaeeeeees 80
11.1 Preparation ...........cccccccccceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeesaaeeeaaeneeeeereees 80
11.2 Training INStrUCTIONS 200... eee eeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeees 80
11.2.1 Image Collecting ...... ee eeeeeeeeeeeeeeeenteeeeeeeeeeeeeeeeeeeeees 80

11.2.2 Image Labeling .............. cece eee eee eee eee ee ee teen 83

11.2.3 Generate Related Files «0.0.0.0... eeeeeeeeeeeetteeeeeeeeeee 87

11.2.4 Start Training ....... ce eeeeeceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeees 88

11.2.5 Format Conversion ...........00c cece eeeeeeeeeeeeeeeeeteeeeeteeetnaeeeeees 90

11.3 Model USaGe 1.0.22... eee cece cece eect eee eeeeeeeeeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaaaeees 94

5 2a AO cee 96



=== Page 5 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

1.GPU Acceleration

1.1 GPU Accelerated Computing

A graphics processing unit (GPU) is a specialized micro processor used to
process image in personal computers, workstations, game consoles and
mobile devices (phone and tablet). Similar to CPU, but CPU is designed to
implement complex mathematical and geometric calculations which are

essential for graphics rendering.

GPU-accelerated computing is the employment of a graphics processing unit
(GPU) along with a computer processing unit (CPU) in order to accelerate
science, analytics, engineering, consumer and cooperation applications.
Moreover, GPU can facilitate the applications on various platforms, including

vehicles, phones, tablets, drones and robots.
1.2 Comparison between GPU and CPU

The main difference between CPU and GPU is how they handle the tasks.
CPU consists of several cores optimized for sequential processing. While GPU
owns a large parallel computing architecture composed of thousands of

smaller and more effective cores tailored for multitasking simultaneously.

GPU stands out for thousands of cores and large amount of high-speed
memory, and is initially intended for processing game and computer image. It
is adept at parallel computing which is ideal for image processing, because the
pixels are relatively independent. And the GPU provides a large number of
cores to perform parallel processing on multiple pixels at the same time, but it
only improves throughput without alleviating the delay. For example, when
receives one message, it will use only one core to tackle this message
although it has thousands of cores. GPU cores are usually employed to
complete operations related to image processing, which is not universal as

CPU.



=== Page 6 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd
1.3 Advantage of GPU

GPU is excellent in massive parallel operations, hence it has an important role
in deep learning. Deep learning relies on neural network that is utilized to

analyze massive data at high speed.

For example, if you want to let this network recognize the cat, you need to
show it lots of the pictures of cats. And that is the forte of GPU. Besides, GPU

consumes less resources than CPU.

2. TensorRT Acceleration

2.1 TensorRT Introduction

TensorRT is a high-performance deep learning inference, includes a deep
learning inference optimizer and runtime that delivers low latency and high
throughput for inference applications. It is deployed to hyperscale data centers,
embedded platforms, or automotive product platforms to accelerate the

inference.

TensoRT supports almost all deep learning frameworks, such as TensorFlow,
Caffe, Mxnet and Pytorch. Combing with new NVIDIA GPU, TensorRT can

realize swift and effective deployment and inference on almost all frameworks.

To accelerate deployment inference, multiple methods to optimize the models
are proposed, such as model compression, pruning, quantization and
knowledge distillation. And we can use the above methods to optimize the
models during training, however TensorRT optimize the trained models. It
improves the model efficiency through optimizing the network computation

graph.



=== Page 7 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

After the network is trained, you can directly put the model training file into

tensorRT without relying on deep learning framework.

2.2 Optimization Methods

Layer & Tensor Fusion

[++ @ \
4 Be:
Precision Calibration Va IN Kernel Auto-Tuning

bd -
) 7 °
: ! | °)
Trained Neural sos 88 haat Optimized
Network Inference
Dynamic Tensor Multi-Stream Engine
Memory Execution

TensorRT has the following optimization strategies:

1) Precision Calibration

2) Layer & Tensor Fusion
3) Kernel Auto-Tuning

4) Dynamic Tenser Memory

5) Multi-Stream Execution

@ Precision Calibration

In the training phase of neural networks across most deep learning
frameworks, network tensors commonly employ 32-bit floating-point precision
(FP32). Following training, since backward propagation is unnecessary during
deployment inference, there is an opportunity to judiciously decrease data
precision, for instance, by transitioning to FP16 or INT8. This reduction in data
precision has the potential to diminish memory usage and latency, leading to a

more compact model size.

The table below provides an overview of the dynamic range for different



=== Page 8 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

precision:
FP32 -3.4x1038 ~ +3.4«x1038
FP16 -65504 ~- +65504
INT8 -128 ~ +127

INT8 is limited to 256 distinct numerical values. When INT8 is employed to
represent values with FP32 precision, information loss is certain, resulting in a
decline in performance. Nevertheless, TensorRT provides a fully automated
calibration process to optimally align performance by converting FP32

precision data to INT8 precision, thereby minimizing performance loss.

@ Layer & Tensor Fusion

While CUDA cores efficiently compute tensor operations, a significant amount
of time is still spent on the initialization of CUDA cores and read/write
operations for each layer's input/output tensors. This results in GPU resource

wastage and creates a bottleneck in memory bandwidth.

TensorRT optimizes the model structure by horizontally or vertically merging
layers, reducing the number of layers and consequently decreasing the

required CUDA core count, achieving structural optimization.

Horizontal merging combines convolution, bias, and activation layers into a
unified CBR structure, utilizing only one CUDA core. Vertical merging
consolidates layers with identical structures but different weights into a broader

layer, also using only one CUDA core.

Moreover, in cases of multi-branch merging, TensorRT can eliminate concat
layers by directing layer outputs to the correct memory address without

8



=== Page 9 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

copying, thereby reducing memory access frequency.

@ Kernel Auto-Tuning

During the inference calculation process, the neural network model utilizes the
GPU's CUDA cores for computation. TensorRT can adjust the CUDA cores
based on different algorithms, network models, and GPU platforms, ensuring
that the current model can perform computational operations with optimal

performance on specific platforms.

@ Dynamic Tenser Memory

During the utilization of each Tensor, TensorRT allocates dedicated GPU
memory to prevent redundant memory requests, thereby reducing memory

consumption and enhancing the efficiency of memory reuse.

@ Multi-Stream Execution

By leveraging CUDA Streams, parallel computation is achieved for multiple

branches of the same input, maximizing the potential for parallel operations.

3. Yolov5 Model

3.1 Yolo Model Series Introduction

3.1.1 YOLO Series

YOLO (You Only Look Once) is an one-stage regression algorithm based on

deep learning.

R-CNN series algorithm dominates target detection domain before YOLOV1 is

released. It has higher detection accuracy, but cannot achieve real-time



=== Page 10 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

detection due to its limited detection speed engendered by its two-stage

network structure.

To tackle this problem, YOLO is released. Its core idea is to redefine target
detection as a regression problem, use the entire image as network input, and
directly return position and category of Bounding Box at output layer.
Compared with traditional methods for target detection, it distinguishes itself in

high detection speed and high average accuracy.

3.1.2 YOLOv5

YOLOv5 is an optimized version based on previous YOLO models, whose

detection speed and accuracy is greatly improved.

In general, a target detection algorithm is divided into 4 modules, namely input
end, reference network, Neck network and Head output end. The following

analysis of improvements in YOLOv5 rests on these four modules.

1) Input end: YOLOv5 employs Mosaic data enhancement method to
increase model training speed and network accuracy at the stage of model
training. Meanwhile, adaptive anchor box calculation and adaptive image
scaling methods are proposed.

2) Reference network: Focus structure and CPS structure are introduced in
YOLOv5.

3) Neck network: same as YOLOV4, Neck network of YOLOv5 adopts
FPN+PAN structure, but they differ in implementation details.

4) Head output layer: YOLOv5 inherits anchor box mechanism of output layer
from YOLOv4. The main improvement is that loss function GIOU_Loss,

and DIOU_nms for prediction box screening are adopted.



=== Page 11 ===
HIVW/E9MCOECT Shenzhen Hiwonder Technology Co,Ltd
3.2 YOLOv5 Model Structure

3.2.1 Component
1) Convolution layer: extract features of the image

Convolution refers to the effect of a phenomenon, action or process that
occurs repeatedly over time, impacting the current state of things. Convolution
can be divided into two components: "volume" and "accumulation". "Volume"
involves data flipping, while "accumulation" refers to the accumulation of the
influence of past data on current data. Flipping the data helps to establish the
relationships between data points, providing a reference for calculating the

influence of past data on the current data.

In YOLOv5S, the data being processed is typically an image, which is
two-dimensional in computer vision. Therefore, the convolution applied is also
a two-dimensional convolution, with the aim of extracting features from the
image. The convolution kernel is an unit area used for each calculation,
typically in pixels. The kernel slides over the image, with the size of the kernel

being manually set.

During convolution, the periphery of the image may remain unchanged or be
expanded as needed, and the convolution result is then placed back into the
corresponding position in the image. For instance, if an image has a resolution
of 6x6, it may be first expanded to a 7x7 image, and then substituted into the
convolution kernel for calculation. The resulting data is then refilled into a blank

image with a resolution of 6x6.

Calculate
and fill in

Original image New image

Expanded image

11



=== Page 12 ===
| IVW/E) im | = t Shenzhen Hiwonder Technology Co,Ltd

New pixel

Expanded image New image
New pixel

Expanded image New image
New pixel

Expanded image New image

2) Pooling layer: enlarge the features of image

The pooling layer is an essential part of a convolutional neural network and is
commonly used for downsampling image features. It is typically used in
combination with the convolutional layer. The purpose of the pooling layer is to
reduce the spatial dimension of the feature map and extract the most important

features.

There are different types of pooling techniques available, including global
pooling, average pooling, maximum pooling, and more. Each technique has its

unique effect on the features extracted from the image.



=== Page 13 ===
| IWE) | Oo = t Shenzhen Hiwonder Technology Co,Ltd

ni New image

Original image | Filter

Maximum pooling can extract the most distinctive features from an image,
while discarding the remaining ones. For example, if we take an image with a
resolution of 6 X 6 pixels, we can use a 2X2 filter to downsample the image

and obtain a new image with reduced dimensions.

Filtered data New

Original image

Filtered data

Original image

Filtered data

Original image



=== Page 14 ===
| IW) ™ Oo © Shenzhen Hiwonder Technology Co,Ltd

3) Upsampling layer: restore the size of an image

This process is sometimes referred to as "anti-pooling". While upsampling
restores the size of the image, it does not fully recover the features that were
lost during pooling. Instead, it tries to interpolate the missing information based

on the available information.

For example, let's consider an image with a resolution of 6x6 pixels. Before

upsampling, use 3X3 filter to calculate the original image so as to get the new

-= i

Upsampled image

-= i

Upsampled image

-=

Original image Upsampled image

image.

4) Batch normalization layer: organize data

It aims to reduce the computational complexity of the model and to ensure that

the data is better mapped to the activation function.

14



=== Page 15 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Batch normalization works by standardizing the data within each mini-batch,
which reduces the loss of information during the calculation process. By
retaining more features in each calculation, batch normalization can improve

the sensitivity of the model to the data.

Changes:

; ~-* reduce the

f / ' \ / computation

Data coe Batch normalization ae — V 2. reduce the loss of

\ / \ a / information

l ie \ A 3. improve the

ft ar aT sensitivity of the model
to the data

5) RELU layer: activate function

The activation function is a crucial component in the process of building a
neural network, as it helps to increase the nonlinearity of the model. Without an
activation function, each layer of the network would be equivalent to a matrix
multiplication, and the output of each layer would be a linear function of the
input from the layer above. This would result in a neural network that is unable

to learn complex relationships between the input and output.

There are many different types of activation functions. Some of the most
common activation functions include the ReLU, Tanh, and Sigmoid. For
example, ReLU is a piecewise function that replaces all values less than zero

with zero, while leaving positive values unchanged.



=== Page 16 ===
| IWEDM Oo er Shenzhen Hiwonder Technology Co,Ltd

10

-10.0 -7.55 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0

6) ADD layer: add tensor

In a typical neural network, the features can be divided into two categories:

salient features and inconspicuous features.

Salient features

Shallowed

Inconspicuous features

Darken

Original image New image

7) Concat layer: splice tensor

It is used to splice together tensors of features, allowing for the combination of
features that have been extracted in different ways. This can help to increase

the richness and complexity of the feature set.



=== Page 17 ===
a IWE) M Oo ES t Shenzhen Hiwonder Technology Co,Ltd

convolution
-1

Original image New image

convolutio
=2

Original image New image

3.2.2 Compound Element

When building a model, using only the layers mentioned above to construct
functions can lead to lengthy, disorganized, and poorly structured code. By
assembling basic elements into various units and calling them accordingly, the

efficiency of writing the model can be effectively improved.

1) Convolutional unit:
A convolutional unit consists of a convolutional layer, a batch normalization
layer, and an activation function. The convolution is performed first, followed by

batch normalization, and finally activated using an activation function.

Batch Activate

convolution a
normalization function

\y

onvolutio
unit

2) Focus module



=== Page 18 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

The Focus module for interleaved sampling and concatenation first divides the
input image into multiple large regions and then concatenates the small
images at the same position within each region to break down the input image
into several smaller images. Finally, the images are preliminarily sampled
using convolutional units.

As shown in the figure below, taking an image with a resolution of 6x6 as an
example, if we set a large region as 2x2, then the image can be divided into 9
large regions, each containing 4 small images.

By concatenating the small images at position 1 in each large region, a 3x3
image can be obtained. The small images at other positions are similarly
concatenated, and the original 6x6 image will be broken down into four 3x3

images.

Big area

Join image based
on position

9x9

Original image Alternating row sampling

3) Residual unit

The function of the residual unit is to enable the model to learn small changes
in the image. Its structure is relatively simple and is achieved by combining
data from two paths.

The first path uses two convolutional units to sample the image, while the
second path does not use convolutional units for sampling but directly uses the

original image. Finally, the data from the first path is added to the second path.



=== Page 19 ===
bf IVW/E) im | = t Shenzhen Hiwonder Technology Co,Ltd

PA <

The second path without

After residual unit The first path

sampling after being sampled
C lutional C lutional, ie
onvoiutiona onvoiutiona
Data ‘ai Foot . Toor Add residual
nit unit processing
vv
Residual

unit

4) Composite Convolution Unit

In YOLOv5, the composite convolution unit is characterized by the ability to
customize the convolution unit according to requirements. The composite
convolution unit is also realized by superimposing data obtained from two
paths.

The first path only has one convolutional layer for sampling, while the second
path has 2x+1 convolutional units and one convolutional layer for sampling.
After sampling and splicing, the data is organized through batch normalization
and then activated by an activation function. Finally, a convolutional layer is

used for sampling.’



=== Page 20 ===
| IWEM Oo er Shenzhen Hiwonder Technology Co,Ltd

2x
convolutional
units
convolutional
layer

convolutional

layer

composite
onvolutional
unit

Batch

normalizatio

Activate
| function

onvolutiona
unit

20



=== Page 21 ===
a IWE) M Oo ES t Shenzhen Hiwonder Technology Co,Ltd

5) Compound Residual Convolutional Unit

The compound residual convolutional unit replaces the 2x convolutional layers
in the compound convolutional unit with x residual units. In YOLOv5, the
feature of the compound residual unit is mainly that the residual units can be

customized according to the needs.

Se

Data

SN

onvolutional
unit
x
convolutional
units

composite
onvolutional
unit

Batch

normalization

Activate
function

onvolutiona
unit

Data after

rocessing
rs
oe

21



=== Page 22 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

6) Composite Pooling Unit

The output data of the convolutional unit is fed into three max pooling layers
and an additional copy is kept without processing. Then, the data from the four
paths are concatenated and input into a convolutional unit. Using the
composite pooling unit to process the data can significantly enhance the

features of the original data.’

Maximum
pooling

Maximum
pooling

Maximum
pooling

composite
> convolutional
unit

Convolutional
unit

4 K:
i‘ Data ae
rocessin
¥ af

af
ne ~

3.2.3 Structure

Composed of three parts, YOLOv5 can output three sizes of data. Data of
each size is processed in different way. The below picture is the output

structure of YOLOv5.

22



=== Page 23 ===
| WE) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

alternate sampling
concatenation unit
convolutional unit
composite residual

maximum pooling convolutional unit x=1
convolutional unit

composite residual
Maximum pooling convolutional unit x=3

convolutional unit

composite residual
-onvolutional unit x=3

convolutional unit

maximum pooling

composite pooling
unit

composite convolutional
unit x=1

convolutional unit

upsampling layer

concatenate

composite
convolutional unit x=

tH

convolutional unit

upsampling layer

concatenate

composite convolutional layer Data
convolutional unit x=1

76*76"255

convolutional unit

composite i. data
onvolutional unit x=1 Convolutional layer 38*38"255

concatenate

composite . data
onvolutional unit x=1 convolutional layer 19*19*255

concatenate

Below is the output structures of data of three sizes.

23



=== Page 24 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

alternate samplin
concatenation unit

convolutional unit

composite residual

maximum pooling convolutional unit x=1

convolutional unit

composite residual
convolutional unit x=3

convolutional unit
maximum pooling

maximum pooling

composite residual
convolutional unit x=3

convolutional unit

composite pooling
unit

composite residual
convolutional unit x=

convolutional unit
upsampling layer

Composite convolutional
unit x=1

unit x=1

Data
76*76*255

24



=== Page 25 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

interleaved sampling
concatenation unit
convolutional unit

composite residual
convolutional unit x=1

convolutional unit

maximum pooling

composite residual
convolutional unit x=3
convolutional unit
convolutional unit x=3

composite residual

convolutional unit
‘omposite pooling unit}

composite
convolutional unit x=1

convolutional unit
upsampling layer

composite
convolutional unit x=1

convolutional unit
upsampling layer

composite
convolutional unit x=1

convolutional unit

composite
concatenate convolutional unit x=1

convolutional layer

Data
38*38*255



=== Page 26 ===
| IW) | Oo er Shenzhen Hiwonder Technology Co,Ltd

interleaved sampling
concatenation unit
convolutional unit
< . composite residua
maximum pooling convolutional unit x=1

convolutional unit

composite residual
onvolutional unit x=3

convolutional unit

maximum pooling

composite residual
convolutional unit x=3
convolutional unit
composite
pooling unit
convolutional unit x=1
convolutional unit
upsampling layer
composite
convolutional unit x=1
convolutional unit
upsampling layer
composite
convolutional unit x=1
convolutional unit

composite
onvolutional unit x=1

convolutional unit

concatenate

composite
convolutional unit x=1

convolutional layer

data

19*19*255

26



=== Page 27 ===
| IWEDM Oo er Shenzhen Hiwonder Technology Co,Ltd
4. YOLOv5 Running Procedure

In this section, we provide an explanation of the model workflow using the

anchor boxes, prediction boxes, and prior boxes employed in YOLOv5.

4.1 Prior Bounding Box

When an image is input into model, object detection area requires us to offer,
while prior bounding box is that box used to mark the object detection area on

image before detection.

4.2 Prediction Box

The prediction box is not required to set manually, which is the output result of
the model. When the first batch of training data is input into model, the
prediction box will be automatically generated with it. The position in which the
object of same type appear more frequently are set as the center of the

prediction box.

27



=== Page 28 ===
4 IVC) r) Co = t Shenzhen Hiwonder Technology Co,Ltd

4.3 Anchor Box

After the prediction box is generated, deviation may occur in its size and
position. At this time, the anchor box serves to calibrate the size and position of

the prediction box.

The generation position of anchor box is determined by prediction box. In order
to influence the position of the next generation of the prediction box, the

anchor box is generated at the relative center of the existing prediction box.

28



=== Page 29 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd
4.4 Realization Process

After the data is calibrated, a prior bounding box appears on image. Then, the
image data is input to the model, the model generates a prediction box based
on the position of the prior bounding box. Having generated the prediction box,
an anchor box will appear automatically. Lastly, the weights from this training

are updated into model.

Each newly generated prediction will be influenced by the last generated
anchor box. Repeating the operations above continuously, the deviation of the
size and position of the prediction box will be gradually erased until it coincides

with the priori box.

Yo ™~

/ \ Prior ,

\ F Prediction Anchor -Update
| Data / roms box box weight
\ / Calibrate Model

—— prediction ‘ =!
Adjust position

5. Image Collecting & Labeling

Given that training the Yolov5 model necessitates a substantial volume of data,
our initial step involves collecting and labeling the requisite data in preparation

for subsequent model training.

To begin, you can assemble the images that require collection. As an

illustration, let's consider the collection of traffic signs.
5.1 Image Collecting
1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.

3) Execute the command and hit enter to terminate the app auto-start

service.

29



=== Page 30 ===
| IWE) | Oo = t Shenzhen Hiwonder Technology Co,Ltd

sudo systemctl stop start_app_node.service

4) Run the command to initiate the depth camera service.

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth camera.launch.py

5) In anew command line terminal, enter the command and press Enter to

create a directory for storing the dataset:

mkdir -p ~/my_data

p ~/my_data

6) Execute the command to open the image collecting tool.

cd ~/software/collect_picture && python3 main.py

~/software/collect picture && main.py

collection 1.0 - *

x «© English Camera View

SGWES fAvinnioerse ©

e.1sunes ©

.%

Resolution Width | 640 | Height | 480 :
Save path /home/ubuntu/Pictures g

30



=== Page 31 ===
| IWS) mM a | =) wt Shenzhen Hiwonder Technology Co,Ltd

The "save number" displayed in the upper left corner represents the image ID,
indicating the order in which images are saved. The term "existing" denotes

the total number of images already saved.

7) Change the storage path to ‘my_data’ folder.

Resolution Width 640 Height 480 _
Stereo
Save path /home/ubuntu/Pictures

8) After confirming the correct catalog, click-on ‘Open’ button.

»
Cancel select directory Q
® Recent < | Gtubuntu | my data > 3
G} Home Name Size Type Modified
(© Desktop
(2) Documents

9) Place the target object within the camera's field of view, then click the
‘Save (space)' button or press the spacebar to save the current image from

the camera.

collection 1.0 - x

© xz) English Camera View

y fAvinnloen
yf

Resolution Width | 640 | Height 480 |

31

Save path | /home/ubuntu/my_data




=== Page 32 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

After clicking 'Save (space)' or pressing the spacebar, a folder named
'‘JPEGImages' will be automatically created in the directory

‘home/ubuntu/my_data' to store the images.

Note: To improve the model's reliability, please capture the target object from

different distances, angles, and tilts.

10) After completing the image collection, click the 'Exit' button to close the

tool.

Resolution Width 640 Height 480 ’
Save path /home/ubuntu/my_data elect _ east

11) Click CO on the system status bar to open the file manager, where you
can view the saved images.

1} Home my_data JPEGImages

© Recent
° a
Starred image_1. image_2.
jpg jpg
+} Home

=) Documents

& Downloads

Music

Next, use the key combination Ctrl+C to exit all open command line terminals.

This completes the steps for image collection and annotation.

5.2 Image Labeling

Labeling images is crucial for feature datasets as it enables the trained model
to understand the categories of significant elements within the image. This
understanding empowers the model to recognize these categories in new,
previously unseen images.

32



=== Page 33 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Open the command-line terminal, and execute the command to open the

image labeling software.
python3 ./software/labellmg/labellmg.py
/software/labelImg/labelImg.py

The table below outlines the function of each icon:

A Select the directory where the
Ctrl+U
Open Dir picture is saved.
‘A Select the directory where the
Ctrl+R
Change Save Dir calibration data is saved.
WwW Create annotation box
—
ae Ctri+S Save annotation
¢ A Switch to the previous image
Prev Image
» D Switch to the next image
Next Image

Open Dir

2) Click to open the directory for our image collection.

33



=== Page 34 ===
| IWS) mM a | =) wt Shenzhen Hiwonder Technology Co,Ltd

In this tutorial, the selected location is shown in the image below:

Cancel labelimg - Open Directory

® Recent < Gtubuntu = my_data  JPEGimages

3) After clicking [2 Loven J} the following window will appear:

rz

ne,

4) Click LShange Save Pir_| in the interface and select the corresponding

storage folder for the calibration data, which is the 'Annotations' directory

X:0;¥:21]

located in the same path as the image collection.

Cancel labelimg - Save annotations to the directory Q Open
® Recent |< | Grubuntu } data JPEGImages ER
at Home Name Size Type Modified
@ JPEGImages 09:48
D Desktop @® imageSets 09:47
{2) Documents
S.
5) Next, click to return to the image annotation interface.

6) Press the 'W' key on the keyboard to start creating the annotation box.

7) Move the mouse to the appropriate position, press and hold the left mouse

34



=== Page 35 ===
bol iIiWEaMn oer Shenzhen Hiwonder Technology Co,Ltd

button to drag, allowing the annotation box to cover the entire target object.

Release the left mouse button to complete the selection of the target

object.

8) Inthe pop-up window, name the category of the target object. Here, it is
named ‘left.’ After naming, click the 'OK’ button or press the 'Enter' key to

save this category.

labelimg x

“Acancet [| _voK |

dog ~
person
cat

tv

car
meatballs

marinara sauce
tomato soup
chicken noodle soup
french onion soup
chicken breast =

9) Use the shortcut 'Ctri+S' to save the annotation data for the current image.

35



=== Page 36 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

10) Then press the 'D' key to move on to the next image's annotation,

repeating steps 7) to 9) until all images are annotated. Click
in the upper right corner of the software to exit.

11) You can click CJ on the system status bar to open the file manager and
navigate to the directory '/home/ubuntu/my_data/' to view the annotation

files for the images.

r _
¢ Annotations ; Q | = = oO x
<) Recent
* Starred image_1. image_2. image_3. image_4. image_5. image_6. image_7.

xml xml xml xml xml xml xml
Gt Home
(1) Desktop

image_8. image_9. image_10. image_11. image_12. image_14. image_15.

(2) Documents xml xml xml xml xml xml xml
& Downloads
Jl Music image_16. image_17. image_18. image_19. image_20.

xml xml xml xml xml

6. Data Format Conversion

6.1 Preparation

Before proceeding with this section, ensure that image collection and
annotation have been completed. For detailed operational steps, please refer

to the documents located in the "5. Image Collecting and Labeling" directory.

Prior to feeding the data into the YOLOv5 model for training, it is essential to
assign categories to the images and convert the annotated data into the

required format.
6.2 Format Conversion

Please operate the below steps after collecting and labeling the pictures.

Note: the input command should be case sensitive, and keywords can be

36



=== Page 37 ===
HIVW/E9MCOECT Shenzhen Hiwonder Technology Co,Ltd

complemented using Tab key.

1) Open a new terminal and enter the command to view the file. Reminder: If

the file does not exist, you can create it by running:

gedit ~/my_data/classes.names

2) Write the category ‘left’ into the text file (if there are multiple different
categories, make sure to edit them on separate lines)

Open v eal classes.names

1 left|

Note: The class names added here must match the naming used in the image

annotation software ‘labellmg.'

3) After editing, use the shortcut 'Ctrl + S' to save and exit.

4) Then, return to the command line terminal, enter the command to convert

the data format, and press Enter:

python3 ~/software/xml2yolo.py --data ~/my_data --yaml
~/my_data/data.yaml

~/software/xml2yolo.py --data ~/my data --yaml ~/my_data/data.yaml

: /home/ubuntu/my_data

':; '/home/ubuntu/my_data/ImageSets/train.txt', ‘val': '/home/ubuntu/my_data/ImageSets/val.txt
‘: 1, 'names': ['left']}

Note: The paths for ~/software/xml2yolo.py and my_data must correspond to the

actual locations of the files!!!

The above command involves three parameters:

1. xml2yolo.py: This script converts the annotated label format into a format

compatible with YOLOv5 for model conversion. Ensure the path is correct!

2. my_data : This represents your annotated folder. Ensure the path is

37



=== Page 38 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

correct!

3. data.yaml: This indicates the format conversion of the entire folder after
the model is split. From the command, we can see that the saved directory

is within the my_data folder.

The following image shows the generated data.yaml file:

data.yaml

Open v

1 names:
2 e

3 nc:

4 train:
5 val:

The items following 'names' indicate the types of labels, 'nc' represents the
number of label types, and ‘train’ refers to the training set (a term used in deep
learning for the data used to train the model). The subsequent parameters
indicate the paths, while 'val' represents the validation set (used to validate
results during the training process). The following parameters also indicate
paths. Note that the paths should be filled in or changed according to their
actual locations. For instance, if you need to speed up the training process
later, you can move the dataset from the robot to your local PC or cloud server,
in which case you would need to replace the current train and val paths

accordingly.

Finally, an XML file will be generated in the ~/my_data folder to indicate the
path of the current dataset that has been segmented. You can also change the
last parameter in step 4, ~/my_data/data.yaml, to alter the save path.

Remember this file's path, as it will be used for model training later.

7. Model Training

Note: the input command should be case sensitive, and the keywords can be

complemented using Tab key.

38



=== Page 39 ===
HIVW/E9MCOECT Shenzhen Hiwonder Technology Co,Ltd
7.1 Preparation

After converting the model format, the next step is to proceed with model
training. However, before training, ensure that the dataset has been prepared
and converted into the required format. For detailed instructions, please refer

to "6. Data Format Conversion".

7.2 Model Training

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.

3) Execute the command and hit Enter key to navigate to the designated

directory.

cd ~/third_party_ros2/yolov5/

4) Run the command to train model.

python3 train.py --img 640 --batch 8 --epochs 300 --data
~/my_data/data.yaml --weights yolov5n.pt

train.py --img 640 --batch 64 --epochs 300 --data ~/my data/data.yaml --weights yolovsn. pti

In the command, "--img" specifies the image size; "--batch" determines the
number of single input images per batch; "--epochs" indicates the number of
training iterations; "--data" denotes the path to the dataset; "--weights"

represents the location of the initial training file.

Users can adjust these parameters according to their specific requirements.
For instance, to enhance model reliability, one can increase the number of

training iterations, though this will also extend the training duration.

39



=== Page 40 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

If the content shown in the picture below appears, it indicates that the training

is in progress.

Upon completion of the model training, the path to the generated file will be
displayed in the terminal and recorded. This path will be utilized in the

subsequent course.

Validating runs/train/exp/weights/best.pt...

Fusing layers...

Model summary: 157 layers, 1760518 parameters, © gradients, 4.1 GFLOPs
cl Images Instances R

Pp mAPSO mAPSO-95: 100%|MMMMNNN| 1/1 [00:00<00:00, 6.26it/s]
3 3 0.051 1 0.696 66

Results saved to runs/train/exp

Note: The path to the generated file may vary and can be located in the

corresponding folder within runs/train.

7.3 Import Training Result (Optional)

After training the model using an external computer or server, you can import
the trained model to the robot motherboard, such as Jetson Orin Nano, and

then proceed with model conversion.
Instructions:

1) Transfer the trained model file to the Jetson Orin Nano board using
NoMachine software by simply dragging and dropping it with the left
mouse button, as shown in the image below. The red box highlights the

trained .pt model, which should be dragged into the remote desktop.

40



=== Page 41 ===
Note: The image below depicts the remote desktop of our JetAuto robot.

Different robot models may present varying remote desktop interfaces, but this

does not affect the file transfer process.

After dragging the file into the interface, it will appear as shown below (the
content within the red box indicates that the model has been successfully

imported to the desktop).

Next, you will need to copy the trained model (for instance, let's consider the

best.pt model file) to the following path: "/third_party/yolov5".

41



=== Page 42 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

2) Right-click and select ‘Open Terminal’.

3) Enter the command in the terminal to complete copying the model to the

yolov5 folder:
mv ~/Desktop/best.pt ~/third_party_ros2/yolov5/

/Desktop/best.pt

Next, you can proceed with model conversion and detection based on the

content of "TensorRT Road-Sign Card Detection.’

8. TensorRT Road Sign Detection

8.1 Preparation

After extensive training, we obtain a new model. To enhance its performance,

we need to convert this new model into a TensorRT-accelerated version.

8.2 Generate TensonRT Model Engine

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

42



=== Page 43 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.

3) Execute the command, and hit Enter to navigate to the designated

directory.

cd ~/third_party_ros2/yolov5/

4) Enter the command to convert the .pt file to a .wts file:

python3 gen_wts.py -t detect -w best.pt -o best.wts

gen_wts.py -t detect -w best.pt -o best.wts

The ‘best.pt’ refers to the trained model, as shown in the image below. The
trained model named "best.pt" has been placed in the path
*~/third_party/yolov5’. If you're using your own trained model, simply place it in
the same directory and change the name in the command accordingly. If the

following image appears, it indicates that the conversion was successful.

gen _wts.py -t detect -w best.pt -o
Generating .wts for detect model
Loading best.pt

YOLOvVS @ 2024-5-31 Python-3.10.12 torch-2.4.0a0+07cecf4168.nv24.05 CPU

Writing into best.wts

5) Execute the command to navigate to the specified directory:
cd ~/third_party_ros2/tensorrtx/yolov5/
~/third party ros2/tensorrtx/yolovs
6) Enter the command to open the specified file:
gedit src/config.h
src/config.h

7) Locate the parameter ‘kNumClass’ in the code shown below. This
43



=== Page 44 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

parameter represents the number of categories for the target detection.
Modify the value according to your actual situation, changing it to match

your annotated classes.

Open ~ fea

1 #pragma once
2

4 * These configs are related to tensorrt model, if these are changed,
5 * please re-compile and re-serialize the tensorrt model.

rg

8 // For INT8, you need prepare the calibration dataset, please refer to
9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs#int&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

11

42 // These are used to define input/output tensor names,

13 // you can set them to whatever you want.

14 const static char* kInputTensorName = ;

45 const static char* kOutputTensorName = 3

16

17 // Detection model and Segmentation model' number of classes
18 constexpr static int kNumClass =[I}|

19

20 // Classfication model's number of classes

21 constexpr static int kClsNumClass = :

22

23 constexpr static int kBatchSize = 1;

24

25 // Yolo's input width and height must by divisible by 32

26 constexpr static int kInputH ‘

27 constexpr static int kInputW 3

72

Saving File “/home/ubuntu/third_party_ros2/tensorrtx/y.. C/ObjC Header - Tab Width:8 v Ln 18, Col 36 vy

8) After making the modifications, press ‘Ctrl + S° to save and exit the editor.
9) Next, create a new build directory and enter it:

mkdir build && cd build

-p build && cd build
10) Enter the command to compile the build folder:

cmake ..

The C compiler identification is GNU 11.4.0
The CXX compiler identification is GNU 11.4.0
Detecting C compiler ABI info

Detecting C compiler ABI info - done

Check for working C compiler: /usr/bin/cc - skipped
Detecting C compile features

Detecting C compile features - done

Detecting CXX compiler ABI info

11) Enter the command to compile the configuration files:

make

44



=== Page 45 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd
Be

If the following message appears, it indicates that the build was completed

successfully.

{ 90%]
{ 95%]

[100%]
[100%] Built target yolov5_seg

12) Copy the generated ‘best.wts’ file to the current directory:

cp ~/third_party_ros2/yolov5/best.wts ./

13) Enter the command to generate the TensorRT model engine file:

sudo ./yolov5_det -s best.wts best.engine n

-S best.wts best.engine n

Since you are already in the directory where the .wts file is located, you can
simply enter the name of the .wts file here. "yolov5n.engine" is the name of
the engine file. If the message "Build engine successfully!" appears, it

indicates that the engine file was generated successfully.

Build engine successfully!

In the same path, a file named "libmyplugins.so" will also be generated for
later detection processes. Remember this for future use; when you need to
use the libmyplugins.so’ file in the current path, make sure to specify its

correct path.

14) If the generated ‘best.engine’ file has permission issues and cannot be

read (locked), as shown in the image below:

best.engine

45



=== Page 46 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Open a terminal in the current directory and enter the command to add

permissions:

sudo chmod +x+w+r best.engine

+X+W+l best. engine

8.3 Target Detection

8.3.1 Instructions
1) Copy the TensonRT model engine file to the specified folder by entering

the command, and then press Enter.

cp best.engine libmyplugins.so

~/ros2_ws/src/example/example/yolov5_detect/

best.engine Libmyplugins.so ~/ros2 ws/src/example/example/yolov5 detect/§J

2) Execute the command and hit Enter to terminate the app service.

sudo systemctl stop start_app_node.service

stop start_app_node.servicelf

3) Click-on Fl to enter file manager, then locate the designated folder.

Starred garbage_ libmyplugin reset.sh traffic yo!
classificati s.so

si
Home on_640s_... 7_0.engine

Rartimante

4) Right click the file ‘yolo5v_trt.py’, and select ‘Open With Text Editor’.

46



=== Page 47 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

e

yolovSs. yolovs trt
engine

[4] Open With Text Editor

Open With Other Application

5) In the first red box, enter the names of the objects to be recognized (delete
the original content). In the first parameter position of the second red box,
enter the name of the trained model, and in the second parameter position,
specify the ‘libmyplugins.so’ file. Here, you can rename it to
“libmyplugins_640.so,” or users can enter their own file names

accordingly.

3991f _ name_

400 import os

401 import sdk.fps as fps
i

single_picture = Fa
417 if single_picture:

418 frame = cv2.imread( )
419 boxes, scores, classid = yolovS_wrapper.infer(frame)

420 for box, cls_conf, cls_id in zip(boxes, scores, classid):

421 color = colors(cls_id, )

6) Click-on ‘Save’ button.

SeSsees
SESEBS 6
$
*

eoe0
s
ee
fT

7) Run the command, and hit Enter to initiate target detection.

python3 ~/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.py

8.3.2 Detection Result

Position the road sign within the camera's field of view. Upon recognition, the
road sign will be outlined with a box in the displayed image, accompanied by

its category name and the confidence level of detection and recognition.

47



=== Page 48 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

Furthermore, relevant detection information will be printed on the terminal

interface.

class_name: "park"
box: [448, 178, 482, 211]
score: 0.875512599945

"class_name" refers to the category name of the recognized target; "box"
denotes the starting coordinates (upper left corner) and ending coordinates
(lower right corner) of the identification box; "score" indicates the confidence

level of detection and recognition.

9. Traffic Sign Model Training

@ Please use the actual product names and reference paths as they appear in the

document.

For large datasets, it is not recommended to use Jetson Nano for training due to its slower
training speed caused by I/O port speed and memory limitations. It is advisable to use a
computer equipped with a dedicated graphics card for faster training. The training process
remains the same, and you only need to configure the relevant program running

environment.

48



=== Page 49 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

If the performance of traffic sign recognition in the driverless gameplay is poor and you

need to train your own model, you can refer to this section for training the traffic sign

model.

The screenshots in the following instructions may display different robot host names
(though the environment configurations of different robots are roughly the same). Please

input them according to the document content; however, it will not affect the execution.

9.1 Preparation

1) Prepare a laptop, or a PC with wireless network card and mouse.
2) Access the robot system desktop using NoMachine.

9.2 Training Instructions

9.2.1 Create Data Set Folder

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.
3) Execute the command to terminate the app service.

sudo systemctl stop start_app_node.service

stop start _app_node.serviceff

4) Run the command to initiate the camera service.

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth camera. Launch. pvil

5) Open a new terminal window. Navigate to the directory where the
collection tool is stored, then enter the command below to launch the
image collection tool.

cd software/collect_picture && python3 main.py

49



=== Page 50 ===
| IW) | Oo © Shenzhen Hiwonder Technology Co,Ltd

collection 1.0 - *

‘Ax’ English Camera View

( AU AAloxet!
existing ©

Resolution Width 640 | Height | 480 |

“save number" represents the picture ID, indicating which picture has been

Save path /home/ubuntu/Pictures I Select |

saved. "existing" denotes the total number of saved images.

6) Change the storage path to ‘Ihome/ubuntu/my_data’.

7) Position the target recognition content within the camera's field of view,

Resolution Width 640 Height 480

e path /home/ubuntu/my_data

then click the 'Save (space)' button or press the space bar to capture and
save the current camera image. Upon saving, both the 'save number’ and
‘existing’ counts will increment by 1. These two parameters allow for
monitoring of the saved picture names displayed on the current camera

screen and the total number of pictures stored in the folder.

50



=== Page 51 ===
| IWEM Oo er Shenzhen Hiwonder Technology Co,Ltd

S MUnloer

|

Upon selecting the "Save (space)" option, a JPEGImages folder will
automatically be created within the directory "/home/ubuntu/my_data" to

store the images.

Notice:

1. For enhanced model reliability, capture target recognition content from various
distances, rotation angles, and tilt angles.

2. To guarantee recognition stability, it's advisable to increase the number of
training images. It is recommended that each image category comprises at least

200 images for effective model training.

8) After image collecting, click-on ‘Quit’ to close this software.

Resolution Width 640 Height 480 |
Save path /home/ubuntu/my_data Sy

9) Click-on besmmssd to open the file manager, and navigate to the folder as

pictured, then you can check the saved pictures.

Gt} Home / my_data / JPEGimages : Q

51



=== Page 52 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

9.2.2 Image Labeling

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.
3) Execute the command to terminate the app service.

sudo systemctl stop start_app_node.service

stop start_app node.servicell

4) Create a new terminal, and execute the command below.

python3 software/labellmg/labellmg.py

software/lLabelImg/ label Ing.pvil

5) Upon launching the image annotation tool, the table below outlines the key

functions

A Select the directory where the
Ctrl+U
Open Dir picture is saved.
‘A Select the directory where the
Ctrl+R
Change Save Dir calibration data is saved.
WwW Create annotation box
=)
ae Ctrl+S Save annotation
Save

52



=== Page 53 ===
| IWE) | Oo = t Shenzhen Hiwonder Technology Co,Ltd

¢ A Switch to the previous image
Prev Image

» D Switch to the next image
Next Image

6) Press "Ctrl+U" to designate the image storage directory as

"Ihome/ubuntu/my_data/JPEGImages", then click the "Open" button.

Home “= size Modified

¥ 4
a
| 3
3 |

Desktop

Documents
Downloads
Music

Pictures

Ta)
R

Videos

AllFiles v

7) Press "Ctrl+R" to specify the calibration data storage directory as

"/home/ubuntu/my_data/Annotations/". The "Annotations" folder is

automatically created during image collection. Finally, click the "Open" button.

Home size Modified
Desktop

Documents

Downloads

Music

AllFiles v

8) Press the "W" key to initiate label box creation. Position the mouse cursor

appropriately, then press and hold the left mouse button while dragging to
encompass the entire target recognition content within the label box. Release

the left mouse button to finalize the selection of the target recognition content.

53



=== Page 54 ===
| iIWEM a | er Shenzhen Hiwonder Technology Co,Ltd

9) Inthe pop-up window, assign a name to the category of the target
recognition content, such as "right". Once the naming is complete, either click

the "OK" button or press the "Enter" key to save this category.

dog =|
person

cat

tv

car
meatballs
marinara sauce

tomato soup
chicken noodle soup
french onion soup
chicken breast

10) Use short-cut ‘Ctrl+S’ to save the labeled data of the current pictures.

11) Label the remaining pictures in the same manner as step 9).

12) Click in the system status bar to open the file manager. Navigate to
the directory "/home/ubuntu/my_data/Annotations/" to view the picture

annotation file.



=== Page 55 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

9.2.3 Generate Related Files

1) Click-on lo to open the command-line terminal.
2) Execute the command, and hit Enter key.

vim ~/my_data/classes.names

~/my_data/classes.namesff

3) Press the "i" key to enter the editing mode and input the class name of the
target recognition content. If multiple class names are required, enter them
one per line.

/bin/zsh 80x24
1 go
2 right
3 park
4 red
> green
crosswalk|

classes.names

- INSERT --

Note: The class name entered here must match the naming convention used in the

image annotation software "labellmg" when applicable.

4) Having finish input, press ‘Esc’ key, and input ‘:wq’ to save the change and

close the file.

55



=== Page 56 ===
STIL ne n Hiw ma f inolog\ { o.Ltd

/bin/zsh 80x24

de

2 right
park

4 red
green

6 crosswalk

utf-8[unix]

5) Execute the command to convert the data format by entering:

python3 software/xml2yolo.py --data ~/my_date --yaml
~/my_data/data.yaml

software/xml2yolo.py --data ~/my_date --yaml ~/my_data/data.yaml

The xml2yolo.py script converts annotated files into XML format, organizes the

dataset into training and validation sets.

If you encounter the following prompt, the conversion is successful.

The output paths may vary on different robots based on their actual storage
locations. However, the generated data.yaml file corresponds to the calibrated

dataset.

9.2.4 Model Training
1) Click-on lo to open the command-line terminal.

56



=== Page 57 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

2) Execute the command, and hit Enter to navigate to the designated

directory.

cd ~/third_party_ros2/yolov5/

3) Run the command, and hit Enter to train the model.

python3 train.py --img 640 --batch 8 --epochs 300 --data
~/my_data/data.yaml --weights yolov5n.pt

train.py --img 640 --batch 64 --epochs 300 --data ~/my data/data.yaml --weights yolovsn. pti

In the command, "--img" specifies the image size, "--batch" determines the
number of images processed in each iteration, and "--epochs" denotes the
number of training iterations, which impacts the quality of the final model. For
quick testing, we've set the number of training iterations to 8, but for optimal
results, this value should be adjusted based on your specific requirements and

the capabilities of your computer system.

Furthermore, "--data" indicates the path to the calibrated dataset, while
"weights" refers to the path of the pretrained model. It's crucial to remember

whether you're using "yolov5n.pt" or "yolov5s.pt" as your pretrained model.

Users can customize these parameters according to their specific needs. If
model reliability needs to be improved, consider increasing the number of

training iterations, Keeping in mind that this will also extend the training time.

4) When the option displayed in the figure below appears, input "3" and press

Enter.

Matplotlib created a temporary config/cache directory at /tmp/matplotlib-1ufysni_ because t
he default path (/home/jetauto/.cache/matplotlib) is not a writable directory; it is highly

recommended to set the MPLCONFIGDIR environment variable to a writable directory, in parti
cular to speed up the import of Matplotlib and to better support multiprocessing.

: (1) Create a W&B account

: (2) Use an existing W&B account

: (3) Don't visualize my results

: Enter your choice: (30 second timeout) |3

If the content displayed in the image below appears, it indicates that the

57



=== Page 58 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

training is ongoing.

Starting training for 5 epochs...

Epoch gpu_mem box obj cls labels img_size
0% | | 0/35 [00:00<?, ?it/s]
0/4 ©.877G @.1251 0.03143 0.0532 31 640: 100% | | 35
Class Images Labels P R MAP@.5 mAP@.5:.95: 8
WARNING: NMS time Limit 0.780s exceeded
Class Images Labels P R MAP@.5 mAP@.5:.95: 100
all 53 146 0.175 0.0197 ®©.00285 ©.000911

Epoch gpu_mem box obj cls labels img_size
1/4 0.965G ©.1072 ©.02946 0©.02899 20 640: 100% | | 35
Class Images Labels P R MAP@.5 mAP@.5:.95: 100
all 53 146 0.853 0.0645 0.0137 ©.0033

After the model has been trained successfully, the terminal will print the path
containing the file, and you need to record the path which will be required for

‘9.2.5 Generate TensorRt Model Engine’.

Optimizer stripped from runs/train/exp5/weights/Llast.pt, 3.9MB

Optimizer stripped from runs/train/exp5/weights/best.pt, 3.9MB

Results saved to runs/train/exp5

Notice: If multiple trainings are conducted, the naming of the "exp5” folder
mentioned here will vary. For instance, it might be changed to "exp2", "exp3", and
so on. The following steps will be associated with the folder naming used in this

step.

9.2.5 Generate TensorRT Model Engine

1) Click-on lo to open the command-line terminal.

2) Execute the command, and hit Enter to navigate to the designated

directory.

cd third_party/yolov5

~/third party ros2/yolov5/

3) Copy the "best.pt" file generated after model training to the current

directory by executing the command, and then press Enter.

58



=== Page 59 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

cp ~/third_party/yolov5/runs/train/exp5/weights/best.pt ./

~/third_party/yolovS5/runs/train/exp5/weights/best.pt ./

In command, the path where the file best.pt is stored can be modified

according to the actual situation.
4) Enter command and press Enter to convert the .pt file into .wts file.

python3 gen_wts.py -t detect -w best.pt -o best.wts

gen_wts.py -t detect -w best.pt -o best.wtsff

If you want to use other models, you only need to replace “best.pt” in

command with the name of new model file.

5) Execute the command to navigate to the designated directory.

cd ~/third_party_ros2/tensorrtx/yolov5/

6) Run the command, and hit Enter to open the designated file.

vim src/config.h

src/config.h

7) Locate the code as shown in the figure below. This parameter represents
the number of categories of target recognition content. Simply modify the
value according to your specific situation, which is highlighted by the red
box in the figure. The "kNumClass" parameter signifies the total number of
calibrated categories. The current data category, indicated as 1, must be
adjusted based on the actual scenario. In this case, it should be changed
to 6, representing the six categories of traffic signs. After selecting the
number to be modified by clicking the left mouse button, press the "i" key

on the keyboard to enter the editing mode.

59



=== Page 60 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

config.h

/third_party_ros2/tensorrtx/yolovS/sre

Open - irl

1#pragma once
2

4 * These configs are related to tensorrt model, if these are changed,
* please re-compile and re-serialize the tensorrt model.

5
6
rs

8 // For INT8, you need prepare the calibration dataset, please refer to
9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs#int&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

11

12 // These are used to define input/output tensor names,

13 // you can set them to whatever you want.

114 const static char* kInputTensorName = “data";

45 const static char* kOutputTensorName = "prob";

16

17 // Detection model and Segmentation model' number of classes

48 constexpr static int kNumClass =[I}|

19

20 // Classfication model's number of classes

21 constexpr static int kClsNumClass = 1000;

static int kBatchSize = 1;

input width and height must by divisible by 32
static int kInputH =
static int kInputW = 640;

640;

Saving file “/home/ubuntu/third_party_ros2/tensorrtx/y... C/ObjCHeader ~ Tab Width: 8 v Ln 18, Col 36

The modification should be done as shown in the picture.

1 pragma once
2

3 /* --------------------------------------------------------
4 * These configs are related to tensorrt model, if these are changed,

5 * please re-compile and re-serialize the tensorrt model.

6 * --[----------------------------------------------------- */

z

8 // For INT8, you need prepare the calibration dataset, please refer to

9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs5#int8&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

pe

12 // These are used to define input/output tensor names,

13 // you can set them to whatever you want.

14 const static char* kInputTensorName = “data”;
15 const static char* kOutputTensorName = "prob";

16

17 // Detection model and Segmentation mo er of classes
18 constexpr static int kNumClass = 67

a9

20 // Classfication model's number of classes

21 constexpr static int kClsNumClass = 1000;

22

23 constexpr static int kBatchSize = 1;

24

25 // Yolo's input width and height must by divisible by 32
26 constexpr static int kInputH 405
27 constexpr static int kInputW
28

29 // Classfication model's input shape
30 constexpr static int kClsInputH = 224;
31 constexpr static int kClsInputW = 224;

The parameters highlighted in the red box in the figure below determine the
size of the input image. The default height and width are both set to 640 pixels,
consistent with the images cropped using the image collection tool earlier. It's
advisable to retain these default settings unless adjustments are necessary

based on specific requirements.

60



=== Page 61 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

24
25 // Yolo's input width and height must by divisible by 32
26 constexpr static int kInputH = -

27 constexpr static int kInputW =

28

8) After modification, press ‘ctrl +s’ to save the change and exit.

9) Run the command below to navigate to the ‘build’ directory newly built.

cd ~/third_party/tensorrtx/yolov5/ && mkdir build && cd build

~/third_party/tensorrtx/yolov5/ && build && buil

10) Execute the command, and hit Enter to compile build folder.

cmake ..

11) Input the command, and hit Enter key to compile the configuration file.

make

Notice:

—_

If an error occurs during compilation at this stage, you'll need to delete the

entire build folder, create a new one, and then repeat steps 10 to 11.

NS

The deleted build folder contains data related to the initial model training. It's

advisable to save this data before deletion.

12) Execute the command, and press Enter key to copy the generated

best.wts file to the current directory.

cp ~/third_party_ros2/yolov5/best.wts ./

13) Run the command, and hit Enter to generate TensorRT model engine file.

61



=== Page 62 ===
HIVW/E9MCOECT Shenzhen Hiwonder Technology Co,Ltd
sudo ./yolov5_det -s best.wts best.engine n

In the command, "best.wts" represents the path to the "best.wts" file. Since
it's currently in the same directory as the .wts file, simply fill in the .wts file

name here.

"best.engine" is the name of the engine file. You can define this parameter

yourself, but it should also be noted by the user.

The last parameter 'n' signifies the YOLOv5 model used in the previous
training (9.2.5 model training). If it's "yolov5s.pt", this command will change to

's'; if it's "yolov5n.pt", this command will change to 'n’.

Upon seeing the prompt "Build engine successfully!", it indicates that the

engine file has been generated successfully.

Build engine successfully!
9.3 Model Usage

1) Type the following command:

cp best.engine libmyplugins.so

~/ros2_ws/src/example/example/yolov5_detect/

best.engine Libmyplugins.so ~/ros2 ws/src/example/example/yolovs detect/fJ

2) Execute the following command and hit Enter key to disable the app

service.

sudo systemctl stop start_app_node.service

stop start_app_node. service

3) Click-on Fy) to navigate to the file manager and direct the designated

62



=== Page 63 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

folder.

Recent
| 4.2 @ Ld
Starred garbage_ tibmyplugin reset.sh traffic_ yolovs_ yolovss. yolovs_trt.
classificati s.so signs_640s_ node.py engine
Home on_640s_... 7_0.engine

Desktop

Rarumante

yolovSs.
engine

(4) Open With Text Editor Return

Open With Other Application

5) Inthe list at the first red box, enter the names of the objects to be
recognized (delete the original content). In the first parameter position of
the second red box, enter the name of the trained model. In the second
parameter position, enter the ‘libmyplugins.so' file, and rename it here to
‘libmyplugins_640.so'. Users should fill in the names according to their

own file names.

400 import os

401 import sdk.fps as fps
)

402 os.system(

classes = [

416 single picture = Fatse

417 if single_picture:

418 frame = cv2.imread( f g')
419 boxes, scores, classid = yolovS5_wrapper.infer(frame)

420 for box, cls_conf, cls_id in zip(boxes, scores, classid):

421 color = colors(cls_id, )

6) Click-on ‘Save’ button to save the modification.

7) Run the following command to initiate target detection.

python3 ~/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.py

63



=== Page 64 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

~/ros2 ws/src/example/example/yolov5 detect/yolovsS trt.

After making the modifications, follow the corresponding ‘Autonomous Driving’

course documentation to explore the features.

10.Waste Card Model Training

@ Please ensure that the product names and reference paths mentioned in the

document accurately reflect the actual ones.

It's not advisable to utilize Jetson Nano for training when dealing with large datasets. Due
to limitations in I/O port speed and memory, the training process may be slow. It's
recommended to use a computer equipped with a dedicated graphics card for faster
training. The training process remains the same; you only need to configure the relevant

program running environment accordingly.

If the recognition performance of the garbage classification gameplay is unsatisfactory
and you need to train your own model, you can refer to this section for guidance on

physical model training.

10.1 Preparation

1) Prepare a laptop or a PC equipped with a wireless network card and a

mouse.

2) Install and open the remote connection tool ‘NoMachine’.
10.2 Training Instructions

10.2.1 Prepare Data Set

@eroto materials can be sourced from the Internet. To reduce the
performance demands for subsequent annotation and training, you can adjust
the image resolution to 640*480. The default resolution for images captured

with the "Capture" tool is 640*480.

64



=== Page 65 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

For training purposes here, we utilize selfie picture materials. If you intend to

use Internet pictures for training, please refer to "Mask Image Training".

10.2.2 Image Collecting

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.

3) Execute the command to disable app service.

sudo systemctl stop start_app_node.service

stop start _app_node.serviceff

4) Run the command to enable the camera service.

ros2 launch peripherals depth_camera.launch.py

Launch peripherals depth camera. Launch. pvil

5) Open anew terminal. Navigate to the directory where the collection tool is

stored and enter the command to launch the image collection tool.

cd software/collect_picture && python3 main.py

software/collect picture &&

6) Position the card that requires training within the camera's field of view,
and click "Save" to capture the current image. Ensure that the number of
images for each type is consistent. For instance, if you capture 50 images of
the banana peel card, you should also capture 50 images of the toothbrush

card.

65



=== Page 66 ===
| IWS) mM Oo Ea wt Shenzhen Hiwonder Technology Co,Ltd

Camera

Upon clicking the "Save (space)" button, a folder named JPEGImages will be
automatically created to store the images in the directory

"/home/ubuntu/my_data".

. Stereo
Save path /home/ubuntu/my.data | EET "| a fo

Note:

1. For enhanced model reliability, it's important to capture the target recognition

content from various distances, rotation angles, and tilt angles.

2. To ensure stable recognition, increase the number of images used for model
training. It's recommended to have at least 200 pictures for each type during

training.

7) After image collecting, click-on ‘Quit’ button to close this software.
Resolution width 640 Height | 480
Stereo
Save path /home/ubuntu/my_ data

66



=== Page 67 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

8) Click-on CI to open the file manager, and navigate to the

designated storage path to check the stored pictures.

10.2.3 Label Pictures

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and access the robot system using the remote control
software ‘NoMachine’.

2) Click-on bo to open the command-line terminal.

3) Execute the command to disable the app service.

sudo systemctl stop start_app_node.service

stop start app node.servicell

4) Create a new terminal, and execute the following command to start the

annotation tool.
python3 ./software/labellmg/labellmg.py
/software/labelImg/labelImg.py

5) Upon launching the image annotation tool, the table below outlines the key

functions.

A Select the directory where the
Ctrl+U
Open Dir picture is saved.

67




=== Page 68 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

"4 Select the directory where the
Ctrl+R
Change Save Dir calibration data is saved.
Ww Create annotation box

Create RectBox

BS |

oe Ctri+S Save annotation

Save

¢ A Switch to the previous image
Prev Image

» D Switch to the next image
Next Image

Use short-cut "Ctrl+U" to select the image storage directory as

"/home/ubuntu/my_data/JPEGImages/", then click the "Open" button.

labelimg
Box Labels
\4
difficult

Use default label
Open Dir

Change Save Dir
Next Image

Prev Image

w File List

Verify Image

</>
Pascal VOC

¥

6) Begin by clicking "Open Dir" to access the folder where the pictures are
stored. Next, select "/nome/ubuntu/My_Data/JPEGlImage", then click
"Open" to proceed.

68



=== Page 69 ===
Hiweander

Shenzhen Hiwonder Technology Co,Ltd

labelimg

7

Box Labels
[4
Open

ad

| Use default label |
Open Dir

| difficult

Change Save Dir

»

Next Image

«

Prev Image

w
Verify Image

a
ir

File List

</>

PascalVOC
¥

7) Click-on ‘Create RectBox’ button to create a annotation box.

Open
Open Dir

Change Save Dir
>
Next Image
+
Prev Image
w
Verify Image
|
oe
-

PascalVOC

ce
Create RectBox

8) Position the mouse cursor appropriately, then press and hold the left

mouse button while dragging out a rectangular frame to select the training
content in the photo. Let's use bananas as an example.

69



=== Page 70 ===
| iIWEM oer Shenzhen Hiwonder Technology Co,Ltd

e

Open

8

Open Dir

Change Save Dir

»>

Next Image

ws

Prev Image

w

Verify Image

i)

<>
PascalVOC

¥

9) After releasing the mouse, enter the category name of this card in the
pop-up dialog box, then click "OK". For instance, you can enter "apple" for

apples and "potato" for potatoes. (Items of the same type must have the

same name)

Box Labels
A Bi edit
Open
Open Dir
Change Save Dir
»>
Next Image
os
Prev Image
w
Verify Image

= labelime

*
“Acaneel [VOR]

PascalVOC

difficult

~ | Use default label

| =

Box Labels | File List

¥
Width: 157, Height: 165 / X: 415; Y: 341

10) After labeling a picture, click-on ‘Save’ button, then click-on ‘Next Image’

to proceed with the image labeling.

70



=== Page 71 ===
| IWEM Oo er Shenzhen Hiwonder Technology Co,Ltd

Box Labels

A fe}
Open difficult

e

Open Dir

7

Change Save Dir

Next Image

|Usedefaultlabel | sid

Prev Image

w

Verify Image

Save
<>
PascalVOC

|
Create RectBox

¥ | Box Labels | File List

X: 97; Y: 58

Notice:

1. While annotating, you can use shortcut keys to expedite the process. For
instance, press "D" to switch to the next picture and press "W" to create a label
box.

2. You can also use "Ctrl+V" to paste the annotation box from the previous picture
here. However, note that this method is only applicable to annotations of the same
type of image. This is because when pasting the annotation box, it also pastes the

name information from the previous picture.

10.2.4 Generate Related Files

1) Click-on bo to open the command-line terminal.

2) Execute the command, and hit Enter.

3) Press the "i" key to enter editing mode and include the class name for the

target recognition content. When adding multiple class names, each class

71



=== Page 72 ===
name should be on a separate line. If you need to enter multiple classes,
press enter for a new line and then specify each class accordingly based

on the different types.

/bin/zsh 80x24
BananaPeel
BrokenBones
CigaretteEnd
DisposableChopsticks
Ketchup
Marker
OralLiquidBottle
Plate
PlasticBottle
StorageBattery
Toothbrush
2 Umbrella

Mtiaamee classes.names a utf-8[unix] 100% 12/12= %8

-- INSERT --

Note: The class name added here must match the naming convention used in the

image annotation software "labellmg."

4) Having finished the input, press ‘Esc’ key and type ‘:wq’ to save the

change and close the file.

rrr a classes.names {..} ON Y Tieee-e Leal 100% “12/12— S68
:wq

5) Enter the command to convert the data format and press Enter.

python3 software/xml2yolo.py --data ~/my_data --yaml

~/my_data/data.yaml

software/xml2yolo.py --data ~/my data --yaml ~/my data/data.yaml |

This output should reflect the actual storage location of the folder within the
robot system. The output paths may vary between different robots, but the

data.yaml file generated ultimately corresponds to the calibrated dataset.

72



=== Page 73 ===
HIVW/E9MCOECT Shenzhen Hiwonder Technology Co,Ltd

10.2.5 Start Training

1) Click-on bo to open the command-line terminal.

2) Run the command to navigate to the specific directory.

cd ~/third_party_ros2/yolov5/

3) Enter the command and press Enter to train the model.

python3 train.py --img 640 --batch 8 --epochs 300 --data
~/my_data/data.yaml --weights yolov5n.pt

train.py --img 640 --batch 64 --epochs 300 --data ~/my data/data.yaml --weights yolovsn. pti

In the command, "--img" denotes the image size; "--batch" indicates the
number of individual image inputs processed together; "--epochs" represents
the number of training iterations, reflecting the frequency of machine learning
cycles. This value should be determined based on the final desired model
performance. For expedited testing purposes, the number of training iterations
is initially set to 8. However, on more capable computer host systems,

increasing this value can lead to improved training outcomes.

".-data" specifies the path to the dataset folder that has been calibrated.
"weights" denotes the path to the pre-trained model file, which serves as the
starting point for your own training process. It's important to note whether

you're using "yolov5n.pt" or "yolov5s.pt" as input parameters.

Users have the flexibility to adjust these parameters according to their specific
requirements. To enhance the model's reliability, users may consider
increasing the number of training iterations, although it's essential to be aware

that this will also extend the training duration accordingly.

4) When the terminal prints that Enter your choice, enter ‘3’, and hit Enter.

73



=== Page 74 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

Matplotlib created a temporary config/cache directory at /tmp/matplotlib-1ufysni_ because t
he default path (/home/jetauto/.cache/matplotlib) is not a writable directory; it is highly

recommended to set the MPLCONFIGDIR environment variable to a writable directory, in parti
cular to speed up the import of Matplotlib and to better support multiprocessing.

: (1) Create a W&B account

: (2) Use an existing W&B account

: (3) Don't visualize my results

: Enter your choice: (30 second timeout) |3

If the content displayed in the image below appears, it indicates that the

training process is currently underway.

Starting training for 5 epochs...

Epoch gpu_mem box obj cls labels img_size
0% | | 0/35 [00:00<?, ?it/s]
0/4 ©.877G @.1251 0.03143 0.0532 31 640: 100% | | 35
Class Images Labels P R MAP@.5 mAP@.5:.95: 8
WARNING: NMS time Limit 0.780s exceeded
Class Images Labels P R MAP@.5 mAP@.5:.95: 100
all 53 146 0.175 0.0197 ®©.00285 ©.000911

Epoch gpu_mem box obj cls labels img_size
1/4 0.965G ©.1072 ©.02946 0©.02899 20 640: 100% | | 35
Class Images Labels P R MAP@.5 mAP@.5:.95: 100
all 53 146 0.853 0.0645 0.0137 ©.0033

Once the model training is completed, the path of the generated file will be
printed on the terminal and recorded. This path will be utilized in the
subsequent step of "Generating TensorRT Model Engine’.

Optimizer stripped from runs/train/exp5/weights/Llast.pt, 3.9MB
Optimizer stripped from runs/train/exp5/weights/best.pt, 3.9MB

Results saved to runs/train/exp5

Note: If multiple trainings are conducted, the naming of the "exp5" folder

mentioned here will vary. For instance, it might be adjusted to "exp2", "exp3", and

so forth. The subsequent steps will depend on the folder naming used in this step.

10.2.6 Format Conversion

1) Click-on lo to open the command line terminal.

2) Execute the command, and hit Enter to navigate to the specific directory.

cd third_party/yolov5

~/third party ros2/yolovs/

74



=== Page 75 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

3) Copy the "best.pt" file generated after model training to the current

directory by entering the following command, then press Enter.

cp ~/third_party/yolov5/runs/train/exp5/weights/best.pt ./

~/third_party/yolovS/runs/train/expS/weights/best.pt ./ a

Users can adjust the path of the "best.pt" file in the instructions based on their

specific circumstances.
4) Run the command, and hit Enter to convert pt file into wts file.

python3 gen_wts.py -t detect -w best.pt -o best.wts

gen_wts.py -t detect -w best.pt -o best.wtsff

If you need to utilize other models, simply replace "best.pt" in the command

with the name of the respective model file.

5) Execute the command to navigate to the specific directory.

cd ~/third_party_ros2/tensorrtx/yolov5/

6) Run the command, and hit Enter key to open the designated file.

gedit src/config.h

src/config.h

7) Locate the code depicted in the figure below. This parameter represents
the number of categories for target recognition content. Simply adjust the
value according to your specific scenario, as indicated by the position
highlighted in the red box in the figure below. The parameter "kKNumClass"
denotes the total number of calibrated data categories. In this instance, it

is set to 1.

75



=== Page 76 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

config.h

/third_party_ros2/tensorrtx/yolovS/src

Open - irl

1#pragma once
2

4 * These configs are related to tensorrt model, if these are changed,
* please re-compile and re-serialize the tensorrt model.

5
6
rs

8 // For INT8, you need prepare the calibration dataset, please refer to
9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs#int&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

11

12 // These are used to define input/output tensor names,

13 // you can set them to whatever you want.

114 const static char* kInputTensorName = “data";

45 const static char* kOutputTensorName = "prob";

16

17 // Detection model and Segmentation model' number of classes

48 constexpr static int kNumClass =[I}|

19

20 // Classfication model's number of classes

21 constexpr static int kClsNumClass = 1000;

static int kBatchSize = 1;
input width and height must by divisible by 32

static int kInputH = 640;
static int kInputW = 640;

Saving file “/home/ubuntu/third_party_ros2/tensorrtx/y... C/ObjCHeader ~ Tab Width: 8 v Ln 18, Col 36

After modification:

1 pragma once
2

3 /* --------------------------------------------------------
4 * These configs are related to tensorrt model, if these are changed,

5 * please re-compile and re-serialize the tensorrt model.

6 * --[----------------------------------------------------- */

z

8 // For INT8, you need prepare the calibration dataset, please refer to

9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs5#int8&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

pe

12 // These are used to define input/output tensor names,

13 // you can set them to whatever you want.

14 const static char* kInputTensorName = “data”;
15 const static char* kOutputTensorName = "prob";
16

17 // Detection model and Segmentation
18 constexpr static int kNumClass = 67
19

20 // Classfication model's number of classes
21 constexpr static int kClsNumClass = 1000;

10) er of classes

22

23 constexpr static int kBatchSize = 1;

24

25 // Yolo's input width and height must by divisible by 32
26 constexpr static int kInputH = 640;

27 constexpr static int kInputW = 640;

28

29 // Classfication model's input shape

30 constexpr static int kClsInputH = 224;

31 constexpr static int kClsInputW = 224;

The parameters highlighted in the red box below represent the input image
size, with a default height of 640 pixels and a width of 640 pixels. This matches
the size of the images cropped using the previous image capture tool, so you
can leave the settings as default. If changes are needed, adjust them

according to your specific requirements.

76



=== Page 77 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

24
25 // Yolo's input width and height must by divisible by 32
26 constexpr static int kInputH = :

27 constexpr static int kInputW =

28

8) After modification, use short-cut ”

9) Run the command to navigate to the build directory newly built.

cd ~/third_party/tensorrtx/yolov5/ && mkdir build && cd build

~/third_party/tensorrtx/yolovS/ && build && buil

10) Run the below command, and hit Enter to compile the build folder.

cmake ..

11) Execute the command, and hit Enter to compile the configuration file.

make

Notice:

—_

If encountering an error during the compilation of this step, it's necessary to
delete the entire build folder. Afterward, create a new build folder and proceed

with steps 10-11 once again.

ad

The deleted build folder includes data associated with the initial model training.

It's advisable to save this data before deleting the folder.

12) Type the command and then press Enter to copy the generated "best.wts"

file to this directory.

cp ~/third_party_ros2/yolov5/best.wts ./

77



=== Page 78 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

13) Type the command and then press Enter to generate the TensorRT model

engine file.

sudo ./yolov5_det -s best.wts best.engine n

-S best.wts best.engine n

In the command, "best.wts" represents the path to the location of the best.wts
file. Since the file is currently in the same directory, you only need to specify
the file name ".wts" here. "best.engine" is the designated name for the engine
file. Users can define this parameter according to their preference, but it
should be remembered for future reference. The final parameter 'n' signifies
the YOLOv5 model utilized in the previous training (in this case, 9.2.5 model
training). If the model is yolov5s.pt, this command would change to's’; if it's

yolov5n.pt, it would change to 'n’.

Upon successful generation of the engine file, the prompt "Build engine

successfully!" will appear.

Build engine successfully!
10.3 Model Usage

1) Enter the below command:

cp best.engine libmyplugins.so

~/ros2_ws/src/example/example/yolov5_detect/

best.engine Libmyplugins.so ~/ros2 ws/src/example/example/yolov5 detect/§J

2) Run the following command and hit Enter key to terminate app service.

sudo systemctl stop start_app_node.service

stop start_app_node. service

78



=== Page 79 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

3) Click-on al to open the file manager, and direct to the designated

folder.

Recent

| e °

Starred arbage_ tibmyplugin reset.sh traffic_ yolovs_ yolovss. yolovs_trt.
classificati s.so signs_640s_ node.py engine py

Home on_640s_... 7_0.engine

Desktop

Rarumante

yolovSs.
engine

(4) Open With Text Editor Return

Open With Other Application

5) In the first red box, enter the names of the objects you want to recognize
(delete the original content). In the second red box, enter the name of the
trained model in the first parameter field, and the "libmyplugins.so" file in
the second parameter field. Rename it here to "libmyplugins_640.so."

Adjust based on your file names.

3991f _ name_
400 import os
401 import sdk.fps as fps

402 os.system( t )

403 # cla = ['qo'! oe ea Ses Mera ¥

404 classes = [ T

405

406

407

408

409

410

411

412

413 @ YoLovStRT instance

414 # yolovS wrapper = YoLov5TRT('./traffic_ signs 640s 7 0.engine' './lLibmyplugins.so', classes)
41s [| yolov5 wrapper = YoLovSTRT(’ / > tren, Luomyptugt :

416 single_ptcture = Fats

417 if single_picture:

418 frame = cv2.imread( f )
419 boxes, scores, classid = yolovS_wrapper.infer(frame)

420 for box, cls_conf, cls_id in zip(boxes, scores, classid):

421 color = colors(cls_id,

6) Click "Save" in the top right corner to save the changes.

ooo
s
te
fT

7) Enter the following command to start object detection:

python3 ~/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.py
79



=== Page 80 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

~/ros2 ws/src/example/example/yolov5 detect/yolov5S trt.

11. Physical Model Training

@ This section provides a general overview. Please refer to the actual

product names and reference paths mentioned in the document.

Training large datasets on a Jetson Orin Nano is not recommended due to its
slow training speed caused by limitations in I/O port speed and memory. It is
advised to utilize a computer equipped with a dedicated graphics card for
faster training. The training process remains the same, and only the relevant

program running environment needs to be configured.

If the recognition accuracy of the physical sorting gameplay is inadequate and
you require training your own model, you can refer to this section for guidance

on physical model training.

11.1 Preparation

1) Prepare a laptop, or a PC with wireless network card and mouse.
2) Access the robot system desktop using NoMachine.

11.2 Training Instructions

The training steps are akin to those followed for road sign model.
11.2.1 Image Collecting

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.
3) Execute the command to terminate the app service.

sudo systemctl stop start_app_node.service

80



=== Page 81 ===
a IWE) M Oo ES t Shenzhen Hiwonder Technology Co,Ltd

stop start app node.servicell

4) Run the command to initiate the camera service.

Launch peripherals depth camera. Launch. py

5) Open a new terminal window. Navigate to the directory where the

collection tool is stored, then enter to launch the image collection tool.

software/collect_picture && main.pyf

Camera

save number" represents the picture ID, indicating which picture has been

saved. "existing" denotes the total number of saved images.

6) Change the storage path to ‘Ihome/ubuntu/my_data’.

Resolution Width 640° Height 480

Save path /home/ubuntu/my_data PSy

7) Position the target recognition content within the camera's field of view,

81



=== Page 82 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

then click the 'Save (space)' button or press the space bar to capture and
save the current camera image. Upon saving, both the 'save number’ and
‘existing’ counts will increment by 1. These two parameters allow for

monitoring of the saved picture names displayed on the current camera

screen and the total number of pictures stored in the folder.

Camera

Upon selecting the "Save (space)" option, a JPEGImages folder will
automatically be created within the directory "/home/ubuntu/my_data" to

store the images.

Notice:

1. For enhanced model reliability, capture target recognition content from various
distances, rotation angles, and tilt angles.

2. To guarantee recognition stability, it's advisable to increase the number of
training images. It is recommended that each image category comprises at least

200 images for effective model training.

8) After image collecting, click-on ‘Quit’ to close this software.

Resolution width | 640 Height 480 |
Save path /home/ubuntu/my_data Sy

82




=== Page 83 ===
Hiweander Shenzhen Hiwonder Technology Co,Ltd

=
9) Click-on to open the file manager, and navigate to the folder as

pictured, then you can check the saved pictures.

11.2.2 Image Labeling

Note: the input command should be case sensitive, and keywords can be

complemented using Tab key.

1) Start the robot, and access the robot system desktop using NoMachine.

2) Click-on bo to open the command-line terminal.

3) Execute the command to terminate the app service.

sudo systemctl stop start_app_node.service

stop start app node.servicell

4) Execute the following command to start the annotation tool.
python3 ./software/labellmg/labellmg.py
/software/labelImg/labelImg.py

5) Upon launching the image annotation tool, the table below outlines the key

functions
A Select the directory where the
Ctrl+U
Open Dir picture is saved.
‘24 Select the directory where the
Ctrl+R
Change Save Dir calibration data is saved.
WwW Create annotation box
Create RectBox

83



=== Page 84 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

— ,
rae Ctrl+S Save annotation
Save

¢ A Switch to the previous image

Prev Image

D Switch to the next image

“

Next Image

6) Press "Ctrl+U" to designate the image storage directory as

"Ihome/ubuntu/my_data/JPEGImages", then click the "Open" button.

labelimg
Box Labels
4
difficult

Use default label
Open Dir

Change Save Dir
Next Image
Prev Image

w File List

Verify Image

</>

Pascal VOC

7) First, click "Open Dir" to access the folder where the pictures are stored.
Then, select "/home/ubuntu/My_Data/JPEGImage" and click "Open" to

proceed.

84



=== Page 85 ===
| IW) | Oo [Shenzhen Hiwonder Technology Co,Ltd

labelimg
Box Labels

7 B

Open difficult

| Use default label |
Open Dir

7

Change Save Dir

»

Next Image

«

Prev Image

w File List
Verify Image

PascalVOC
¥

8) Click-on ‘Create RectBox’ button to create a annotation box.

labelimg /home/hiwonder/my_data/JPEGimages/Camera_1801.jpg
Box Labels
A a

Open difficult

e

Open Dir

A

Change Save Dir

»>

Next Image

Co

Prev Image

w

Verify Image

Use default label

Pascal VOC

Create RectBox

|| BoxLabels | File List
X: 672; Y: 381

9) Position the mouse cursor appropriately, then hold down the left mouse
button and drag out a rectangular frame to select the training content in the

photo. For instance, let's use an apple as an example.

85



=== Page 86 ===
| IWEM Oo er Shenzhen Hiwonder Technology Co,Ltd

labelimg /home/hiwonder/my_data/JPEGImages/Camera_1801.jpg

Box Labels

Open difficult

e

Open Dir

A

Change Save Dir

ke

Next Image

¢

Prev Image

w

Verify Image

Use default label — |

PascalVOC

| Box Labels File List |
Width: 228, Height: 259 / X: 506; Y: 401

¥
Click & drag to move shape 'banana’

10) After releasing the mouse, enter the category name of this object in the
pop-up dialog box, and then click "OK". For instance, if it's an apple, enter
"apple"; if it's a potato, enter "potato". (Items of the same type must have the

same name.)

Box Labels
7

Open difficult

A

Open Dir

Change Save Dir
be

Next Image

«

Prev Image

w

Verify Image
=]

i

| Use default label |

<> labelimg

Box Labels File List
width: 228, Height: 259 / X:506;¥:401__

Pascal VOC

11) Once you've finished annotating one image, click "Save" to save your

annotations. Then, click "Next Image" to proceed to annotate the next image.

86



=== Page 87 ===
| IWE) | Oo = t Shenzhen Hiwonder Technology Co,Ltd

labelimg /home/hiwonder/my_data/JPEGImages/Camera_1801.jpg

¥, Box Labels
Y a
Open difficult

Open Dir

Change Save Dir

[erm

+

Prev image

w

Verify Image

Use default label

v apple x

PascalVOC
Create RectBox

¥ Box Labels | File List
Click & drag to move shape ‘banana’ X: 27; ¥: 544

Notice:

1. During annotation, you can utilize shortcut keys to expedite the process. For
example, press "D" to switch to the next picture and press "W" to create a label
box.

2. Youcan also press "Ctrl+v" to paste the annotation box from the previous
picture. However, please note that this method is only applicable to
annotations of the same type of image. This is because when the annotation

box is pasted, it will include the name information from the previous picture.

11.2.3 Generate Related Files

1) Click-on bo to open the command-line terminal.

2) Execute the command, and hit Enter.

vim ~/my_data/classes.names

~/my data/cl

3) Press the "i" key to enter editing mode and insert the class name of the
target recognition content. If you need to add multiple class names, ensure

each class name is placed on a separate line.

87



=== Page 88 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

-- INSERT --

Note: The class name entered here must match the naming convention used in the

image annotation software "labellmg.”

4) Having finished input, press ‘Esc’ key, then input ‘:wq’ to save the change

and close the file.

[unix]

5) Run the command to convert the data format.

python3 software/xml2yolo.py --data ~/my_data --yaml

~/my_data/data.yaml

software/xml2yolo.py --data ~/my data --yaml ~/my data/data.yamUfl

This output should be based on the actual storage location of the folder in the
robot system. The output paths may vary between different robots, but the final

data.yaml file generated corresponds to the calibrated dataset.

11.2.4 Start Training

1) Click-on lo to open the command-line terminal.

2) Run the command to navigate to the specific directory.

88



=== Page 89 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

cd ~/third_party_ros2/yolov5/

3) Enter the command and press Enter to train the model.

python3 train.py --img 640 --batch 8 --epochs 300 --data
~/my_data/data.yaml --weights yolov5n.pt

train.py --img 640 --batch 64 --epochs 300 --data ~/my data/data.yaml --weights yolovsn. pti

In the command, "--img" denotes the image size; "--batch" indicates the
number of individual image inputs processed together; "--epochs" represents
the number of training iterations, reflecting the frequency of machine learning
cycles. This value should be determined based on the final desired model
performance. For expedited testing purposes, the number of training iterations
is initially set to 8. However, on more capable computer host systems,

increasing this value can lead to improved training outcomes.

".-data" specifies the path to the dataset folder that has been calibrated.
"weights" denotes the path to the pre-trained model file, which serves as the
starting point for your own training process. It's important to note whether

you're using "yolov5n.pt" or "yolov5s.pt" as input parameters.

Users have the flexibility to adjust these parameters according to their specific
requirements. To enhance the model's reliability, users may consider
increasing the number of training iterations, although it's essential to be aware

that this will also extend the training duration accordingly.

4) When the terminal prints that Enter your choice, enter ‘3’, and hit Enter.

Matplotlib created a temporary config/cache directory at /tmp/matplotlib-1ufysni_ because t
he default path (/home/jetauto/.cache/matplotlib) is not a writable directory; it is highly

recommended to set the MPLCONFIGDIR environment variable to a writable directory, in parti
cular to speed up the import of Matplotlib and to better support multiprocessing.

: (1) Create a W&B account

: (2) Use an existing W&B account

: (3) Don't visualize my results

: Enter your choice: (30 second timeout) |3

If the content displayed in the image below appears, it indicates that the

89



=== Page 90 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

training process is currently underway.

Starting training for 5 epochs...

Epoch gpu_mem box obj cls labels img_size
0% | | 0/35 [00:00<?, ?it/s]
0/4 ©.877G @.1251 0.03143 0.0532 31 640: 100% | | 35

Class Images Labels P R MAP@.5 mAP@.5:.95: ic}
WARNING: NMS time Limit 0.780s exceeded

Class Images Labels P R MAP@.5 mAP@.5:.95: 100
all 53 146 6.175 @.0197 ®©.00285 ©.000911

Epoch gpu_mem box obj cls labels img_size
1/4 0.965G ©.1072 ©.02946 0©.02899 20 640: 100% | | 35
Class Images Labels P R MAP@.5 mAP@.5:.95: 100
all 53 146 0.853 0.0645 0.0137 ©.0033

Once the model training is completed, the path of the generated file will be
printed on the terminal and recorded. This path will be utilized in the

subsequent step of "Generating TensorRT Model Engine’.

Optimizer stripped from runs/train/exp5/weights/Llast.pt, 3.9MB

Optimizer stripped from runs/train/exp5/weights/best.pt, 3.9MB

Results saved to runs/train/exp5

Note: If multiple trainings are conducted, the naming of the "exp5" folder
mentioned here will vary. For instance, it might be adjusted to "exp2", "exp3", and

so forth. The subsequent steps will depend on the folder naming used in this step.

11.2.5 Format Conversion

1) Click-on lo to open the command line terminal.

2) Execute the command, and hit Enter to navigate to the specific directory.

cd third_party/yolov5

~/third party ros2/yolovs/

3) Copy the "best.pt" file generated after model training to the current

directory by entering the following command, then press Enter.

cp ~/third_party/yolov5/runs/train/exp5/weights/best.pt ./

~/third_party/yolovS/runs/train/expS/weights/best.pt ./ a

90



=== Page 91 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

Users can adjust the path of the "best.pt" file in the instructions based on their

specific circumstances.
4) Run the command, and hit Enter to convert pt file into wts file.

python3 gen_wts.py -t detect -w best.pt -o best.wts

gen_wts.py -t detect -w best.pt -o best.wtsff

If you need to utilize other models, simply replace "best.pt" in the command

with the name of the respective model file.

5) Execute the command to navigate to the specific directory.

cd ~/third_party_ros2/tensorrtx/yolov5/

6) Run the command, and hit Enter key to open the designated file.

gedit src/config.h

src/config.h

7) Locate the code depicted in the figure below. This parameter represents
the number of categories for target recognition content. Simply adjust the value
according to your specific scenario, as indicated by the position highlighted in
the red box in the figure below. The parameter "kNumClass" denotes the total

number of calibrated data categories. In this instance, it is set to 1.

91



=== Page 92 ===
| IW) | Oo =) t Shenzhen Hiwonder Technology Co,Ltd

config.h

/third_party_ros2/tensorrtx/yolovS/sre

Open - irl

1#pragma once
2

4 * These configs are related to tensorrt model, if these are changed,
* please re-compile and re-serialize the tensorrt model.

5
6
rs

8 // For INT8, you need prepare the calibration dataset, please refer to
9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs#int&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

11

12 // These are used to define input/output tensor names,

143 // you can set them to whatever you want.

114 const static char* kInputTensorName = “data";

45 const static char* kOutputTensorName = "prob";

16

17 // Detection model and Segmentation model' number of classes

48 constexpr static int kNumClass =[I}|

19

20 // Classfication model's number of classes

21 constexpr static int kClsNumClass = 1000;

static int kBatchSize = 1;
input width and height must by divisible by 32

static int kInputH = 640;
static int kInputW = 640;

Saving file “/home/ubuntu/third_party_ros2/tensorrtx/y... C/ObjCHeader ~ Tab Width: 8 v Ln 18, Col 36

After modification:

1 pragma once
2

3 /* --------------------------------------------------------
4 * These configs are related to tensorrt model, if these are changed,

5 * please re-compile and re-serialize the tensorrt model.

6 * --[----------------------------------------------------- */

z

8 // For INT8, you need prepare the calibration dataset, please refer to

9// https://github.com/wang-xinyu/tensorrtx/tree/master/yolovs5#int8&-quantization
10 #define USE_FP16 // set USE_INT8 or USE_FP16 or USE_FP32

pe

12 // These are used to define input/output tensor names,

13 // you can set them to whatever you want.

14 const static char* kInputTensorName = “data”;
15 const static char* kOutputTensorName = "prob";
16

17 // Detection model and Segmentation
18 constexpr static int kNumClass = 67
19

20 // Classfication model's number of classes

21 constexpr static int kClsNumClass = 1000;

22

23 constexpr static int kBatchSize = 1;

24

25 // Yolo's input width and height must by divisible by 32
26 constexpr static int kInputH = 640;

10) er of classes

27 constexpr static int kInputW = 640;
28

29 // Classfication model's input shape
30 constexpr static int kClsInputH = 224;

31 constexpr static int kClsInputW = 224;

The parameters highlighted in the red box below represent the input image
size, with a default height of 640 pixels and a width of 640 pixels. This matches
the size of the images cropped using the image capture tool. It's recommended
to keep the default settings, but you can adjust them if necessary based on

your specific needs.

92



=== Page 93 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd

24
25 // Yolo's input width and height must by divisible by 32
26 constexpr static int kInputH = 4

27 constexpr static int kInputW =

28

8) After modification, use short-cut ‘ctrl +s’ save the change and close the
file.

9) Run the command to navigate to the build directory newly built.

cd ~/third_party/tensorrtx/yolov5/ && mkdir build && cd build

/third_party/tensorrtx/yolovS/ && build && buil

10) Run the command, and hit Enter to compile the build folder.

cmake ..

11) Execute the command, and hit Enter to compile the configuration file.

make

Notice:

—_

If encountering an error during the compilation of this step, it's necessary to
delete the entire build folder. Afterward, create a new build folder and proceed

with steps 10-11 once again.

ad

The deleted build folder includes data associated with the initial model training.

It's advisable to save this data before deleting the folder.

12) Type the command and then press Enter to copy the generated "best.wts"

file to this directory.

cp ~/third_party_ros2/yolov5/best.wts ./

13) Type the command and then press Enter to generate the TensorRT model

93



=== Page 94 ===
Hiwoander Shenzhen Hiwonder Technology Co,Ltd
engine file.

sudo ./yolov5_det -s best.wts best.engine n

-S best.wts best.engine n

In the command, "best.wts" represents the path to the location of the best.wts
file. Since the file is currently in the same directory, you only need to specify
the file name ".wts" here. "best.engine" is the designated name for the engine
file. Users can define this parameter according to their preference, but it
should be remembered for future reference. The final parameter 'n' signifies
the YOLOv5 model utilized in the previous training (in this case, 9.2.5 model
training). If the model is yolov5s.pt, this command would change to 's'; if it's

yolov5n.pt, it would change to 'n’.

Upon successful generation of the engine file, the prompt "Build engine

successfully!" will appear.

Build engine successfully!

11.3 Model Usage

1) In the command line terminal from the previous section, continue by
entering the command to copy the TensorRT model engine file to the

specified folder. After entering the command, press Enter.

cp best.engine libmyplugins.so

~/ros2_ws/src/example/example/yolov5_detect/

best.engine Libmyplugins.so ~/ros2 ws/src/example/example/yolovs detect/fJ

2) Execute the below command and hit Enter key to terminate the app

service.

sudo systemctl stop start_app_node.service

stop start_app_node. service

94



=== Page 95 ===
| iIiwWeonder Shenzhen Hiwonder Technology Co,Ltd

3) Click-on al to open the file manager and direct to the designated

folder.

Recent

| e °

Starred arbage_ tibmyplugin reset.sh traffic_ yolovs_ yolovss. yolovs_trt.
classificati s.so signs_640s_ node.py engine py

Home on_640s_... 7_0.engine

Desktop

Rarumante

yolovSs.
engine

[4] Open With Text Editor Return

Open With Other Application

5) In the first red box, enter the names of the objects you want to recognize
(delete the original content). In the second red box, enter the name of the
trained model in the first parameter field, and the "libmyplugins.so" file in
the second parameter field. Rename it here to "libmyplugins_640.so."

Adjust based on your file names.

3991f _ name_
400 import os
401 import sdk.fps as fps

402 os.system( t )

403 # cla = ['qo'! oe ea Ses Mera ¥

404 classes = [ T

405

406

407

408

409

410

411

412

413 @ YoLovStRT instance

414 # yolovS wrapper = YoLovSTRT('./traffic signs 640s 7 0.engine', './libmyplugins.so', classes)
41s [| yolov5 wrapper = YoLovSTRT(’ / Sswengtnen, 7 Utbmyptugt :

416 single_ptcture = Fats

417 if single_picture:

418 frame = cv2.imread( f g')
419 boxes, scores, classid = yolovS_wrapper.infer(frame)

420 for box, cls_conf, cls_id in zip(boxes, scores, classid):

421 color = colors(cls_id,

6) Click "Save" in the top right corner to save the changes.

ope, ieageetd save | =
375 rigin_w =i)
380 rigin.w -1)
381 igin_h -1)
382 ‘igin_h -1)
383

7) Enter the following command to start object detection:

python3 ~/ros2_ws/src/example/example/yolov5_detect/yolov5_trt.py
95



=== Page 96 ===
bf IVW/E) im | = t Shenzhen Hiwonder Technology Co,Ltd

12. FAQ

1. If multiple training attempts fail, you can follow these steps:

1) Click ) on the system desktop to open the command line terminal.
Enter the following commands in order to increase the swap memory

space:

2) Return to the model training steps of the corresponding course section and

continue from there.

96


